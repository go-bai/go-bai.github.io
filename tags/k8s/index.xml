<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>K8s on gobai's blog</title><link>/tags/k8s/</link><description>Recent content in K8s on gobai's blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 25 Nov 2024 14:56:03 +0800</lastBuildDate><atom:link href="/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Kube Prometheus Stack 安装部署</title><link>/posts/kube-prometheus-stack/</link><pubDate>Mon, 25 Nov 2024 14:56:03 +0800</pubDate><guid>/posts/kube-prometheus-stack/</guid><description>&lt;h2 id="安装-kube-prometheus-stack">安装 kube-prometheus-stack&lt;/h2>
&lt;p>使用 helm charts 安装 kube-prometheus-stack&lt;/p>
&lt;pre>&lt;code class="language-bash">mkdir -p ~/charts/kube-prometheus-stack
cd ~/charts/kube-prometheus-stack
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
# values.yaml 用来查看默认值
helm show values prometheus-community/kube-prometheus-stack &amp;gt; values.yaml
cat &amp;lt;&amp;lt;EOF &amp;gt; custom-values.yaml
prometheus:
 prometheusSpec:
 additionalScrapeConfigs: []
 service:
 type: NodePort

# grafana service
grafana:
 service:
 type: NodePort

alertmanager:
 enabled: false
EOF
helm upgrade --install --create-namespace --namespace monitoring kube-prometheus-stack prometheus-community/kube-prometheus-stack -f custom-values.yaml
&lt;/code>&lt;/pre>
&lt;h2 id="配置-grafana-dashboard">配置 grafana dashboard&lt;/h2>
&lt;p>导入一个查看 node exporter 的 dashboard&lt;/p></description></item><item><title>Rook Ceph 安装部署</title><link>/posts/rook-ceph/</link><pubDate>Sun, 24 Nov 2024 14:56:03 +0800</pubDate><guid>/posts/rook-ceph/</guid><description>&lt;p>使用 &lt;a href="../rke2/">RKE2 快速搭建 k8s 集群&lt;/a> 创建的集群&lt;/p>
&lt;h2 id="安装-rook-ceph">安装 rook ceph&lt;/h2>
&lt;p>使用 helm charts 安装 rook ceph&lt;/p>
&lt;p>&lt;a href="https://rook.io/docs/rook/latest-release/Helm-Charts/helm-charts/">https://rook.io/docs/rook/latest-release/Helm-Charts/helm-charts/&lt;/a>&lt;/p>
&lt;h3 id="安装-ceph-operator">安装 ceph operator&lt;/h3>
&lt;p>我这里禁用了 cephfs 和 nfs 相关功能&lt;/p>
&lt;pre>&lt;code class="language-bash">mkdir -p ~/charts/rook-ceph/ceph-operator
cd ~/charts/rook-ceph/ceph-operator
helm repo add rook-release https://charts.rook.io/release
# values.yaml 用来查看默认值
helm show values rook-release/rook-ceph &amp;gt; values.yaml
cat &amp;lt;&amp;lt;EOF &amp;gt; custom-values.yaml
logLevel: DEBUG
csi:
 enableCephfsDriver: false
 enableCephfsSnapshotter: false
 enableNFSSnapshotter: false
EOF
helm upgrade --install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph -f custom-values.yaml
&lt;/code>&lt;/pre>
&lt;h3 id="安装-ceph-cluster">安装 ceph cluster&lt;/h3>
&lt;p>添加三个 node 上的三个盘作为 osd&lt;/p></description></item><item><title>CSI 工作原理</title><link>/posts/csi/</link><pubDate>Mon, 04 Nov 2024 22:07:17 +0800</pubDate><guid>/posts/csi/</guid><description>&lt;h2 id="关于-csi">关于 CSI&lt;/h2>
&lt;p>CSI 全称为 &lt;code>Container Storage Interface&lt;/code>, 容器存储接口&lt;/p>
&lt;p>要实现一个第三方的 csi driver 需要实现下面的 gRPC service &lt;a href="https://github.com/container-storage-interface/spec/blob/master/lib/go/csi/csi_grpc.pb.go">csi spec&lt;/a>&lt;/p>
&lt;pre>&lt;code class="language-golang">// 如果 NodeServer 和 ControllerServer 对应服务运行在不同 pod 中, 那么两个服务都要实现 IdentityServer
type IdentityServer interface {
 // 用来获取插件名称
 GetPluginInfo(context.Context, *GetPluginInfoRequest) (*GetPluginInfoResponse, error)
 GetPluginCapabilities(context.Context, *GetPluginCapabilitiesRequest) (*GetPluginCapabilitiesResponse, error)
 Probe(context.Context, *ProbeRequest) (*ProbeResponse, error)
 mustEmbedUnimplementedIdentityServer()
}

type ControllerServer interface {
 // 创建 volume, 如 ceph 创建一个 rbd 或者 hostpath 创建一个目录
 CreateVolume(context.Context, *CreateVolumeRequest) (*CreateVolumeResponse, error)
 // 删除 volume, 如 ceph 删除一个 rbd 或者 hostpath 删除一个目录
 DeleteVolume(context.Context, *DeleteVolumeRequest) (*DeleteVolumeResponse, error)
 // 将 volume attach 到 node 上, 如 rbd 通过 rbd map 命令 attach, 成功后 node 上会多出一个 rbdx 的 block 设备
 ControllerPublishVolume(context.Context, *ControllerPublishVolumeRequest) (*ControllerPublishVolumeResponse, error)
 // 将 volume 从 node 上 detach, 如 rbd 通过 rbd unmap 命令 detach
 ControllerUnpublishVolume(context.Context, *ControllerUnpublishVolumeRequest) (*ControllerUnpublishVolumeResponse, error)
 ValidateVolumeCapabilities(context.Context, *ValidateVolumeCapabilitiesRequest) (*ValidateVolumeCapabilitiesResponse, error)
 // 列出所有 volume
 ListVolumes(context.Context, *ListVolumesRequest) (*ListVolumesResponse, error)
 GetCapacity(context.Context, *GetCapacityRequest) (*GetCapacityResponse, error)
 ControllerGetCapabilities(context.Context, *ControllerGetCapabilitiesRequest) (*ControllerGetCapabilitiesResponse, error)
 CreateSnapshot(context.Context, *CreateSnapshotRequest) (*CreateSnapshotResponse, error)
 DeleteSnapshot(context.Context, *DeleteSnapshotRequest) (*DeleteSnapshotResponse, error)
 ListSnapshots(context.Context, *ListSnapshotsRequest) (*ListSnapshotsResponse, error)
 ControllerExpandVolume(context.Context, *ControllerExpandVolumeRequest) (*ControllerExpandVolumeResponse, error)
 ControllerGetVolume(context.Context, *ControllerGetVolumeRequest) (*ControllerGetVolumeResponse, error)
 ControllerModifyVolume(context.Context, *ControllerModifyVolumeRequest) (*ControllerModifyVolumeResponse, error)
 mustEmbedUnimplementedControllerServer()
}

// 这些会被 kubelet 调用
type NodeServer interface {
 // format (如果没format), mount 到 node 的 global directory
 NodeStageVolume(context.Context, *NodeStageVolumeRequest) (*NodeStageVolumeResponse, error)
 // umount
 NodeUnstageVolume(context.Context, *NodeUnstageVolumeRequest) (*NodeUnstageVolumeResponse, error)
 // mount --bind 到 pod directory
 NodePublishVolume(context.Context, *NodePublishVolumeRequest) (*NodePublishVolumeResponse, error)
 // umount --bind
 NodeUnpublishVolume(context.Context, *NodeUnpublishVolumeRequest) (*NodeUnpublishVolumeResponse, error)
 NodeGetVolumeStats(context.Context, *NodeGetVolumeStatsRequest) (*NodeGetVolumeStatsResponse, error)
 NodeExpandVolume(context.Context, *NodeExpandVolumeRequest) (*NodeExpandVolumeResponse, error)
 NodeGetCapabilities(context.Context, *NodeGetCapabilitiesRequest) (*NodeGetCapabilitiesResponse, error)
 NodeGetInfo(context.Context, *NodeGetInfoRequest) (*NodeGetInfoResponse, error)
 mustEmbedUnimplementedNodeServer()
}
&lt;/code>&lt;/pre>
&lt;h2 id="关于-sidecar-containers">关于 Sidecar Containers&lt;/h2>
&lt;p>&lt;a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html">Sidecar Containers&lt;/a> 是一系列标准容器，用于简化 CSI 插件的开发和部署&lt;/p></description></item><item><title>使用 RKE2 快速搭建 k8s 集群</title><link>/posts/rke2/</link><pubDate>Mon, 01 Jul 2024 21:24:49 +0800</pubDate><guid>/posts/rke2/</guid><description>&lt;p>根据&lt;a href="../creating-a-bridged-network-with-netplan-on-ubuntu-22-04/">创建 bridge 网络&lt;/a>和&lt;a href="../create-vm-with-cloudinit/">创建虚拟机时使用 cloudinit 初始化&lt;/a>创建虚拟机, 并配置静态ip如下&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>主机名&lt;/th>
 &lt;th>配置&lt;/th>
 &lt;th>ip (域名)&lt;/th>
 &lt;th>系统盘 / 数据盘&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>k8s-node01&lt;/td>
 &lt;td>8核16G&lt;/td>
 &lt;td>192.168.1.218 (&lt;code>lb.k8s.lan&lt;/code>)&lt;/td>
 &lt;td>50GB / 100GB*1&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>k8s-node02&lt;/td>
 &lt;td>8核16G&lt;/td>
 &lt;td>192.168.1.219&lt;/td>
 &lt;td>50GB / 100GB*1&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>k8s-node03&lt;/td>
 &lt;td>8核16G&lt;/td>
 &lt;td>192.168.1.220&lt;/td>
 &lt;td>50GB / 100GB*1&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="安装-rke2">安装 RKE2&lt;/h2>
&lt;h3 id="安装第一个-server-节点">安装第一个 server 节点&lt;/h3>
&lt;p>在 k8s-node01 节点执行&lt;/p>
&lt;pre>&lt;code class="language-bash"># 初始化 rke2 配置文件
mkdir -p /etc/rancher/rke2
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/rancher/rke2/config.yaml
tls-san:
 - lb.k8s.lan
write-kubeconfig-mode: &amp;quot;0600&amp;quot;
disable-cloud-controller: true
# 为了节省资源使用的 flannel, 也可以使用 calico
cni: flannel
debug: true
# 指定 kube-scheduler 自定义参数, 会自动覆盖到 /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml
kube-scheduler-arg:
 - v=4
 - bind-address=0.0.0.0
kube-controller-manager-arg:
 - bind-address=0.0.0.0
etcd-expose-metrics: true
EOF

curl -sfL https://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh -
systemctl enable rke2-server.service
systemctl start rke2-server.service
&lt;/code>&lt;/pre>
&lt;h4 id="配置介绍">配置介绍&lt;/h4>
&lt;h5 id="tls-san">&lt;code>tls-san&lt;/code>&lt;/h5>
&lt;p>&lt;code>tls-san&lt;/code> 在 server 的 TLS 证书中增加了多个地址作为 &lt;code>Subject Alternative Name&lt;/code>, 这样 apiserver 就可以通过 &lt;code>lb.k8s.lan&lt;/code> 和 各个 server 节点 ip 访问.&lt;/p></description></item></channel></rss>