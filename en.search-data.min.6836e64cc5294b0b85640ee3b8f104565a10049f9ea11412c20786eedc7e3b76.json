[{"id":0,"href":"/posts/prometheus-basics/","title":"Prometheus 基础","section":"Prometheus","content":" 什么是 Prometheus # Prometheus 是一个开源的 monitoring 和 alerting 系统。\nPrometheus 将 指标 (metrics) 收集并存储为 时序(time series) 数据，指标信息与记录它的 时间 和可选的被称为 标签 (labels)的键值对一块存储。\n什么是 metrics # 时序\n通过问题理解 Prometheus # Prometheus 是如何收集指标数据的 # Prometheus 是如何存储指标数据的 # Prometheus 是如何存储一个指标的时间序列的 # 参考 # Are metrics timestamped at the start of the scrape, or at the end? #11261 "},{"id":1,"href":"/posts/kube-prometheus-stack/","title":"kube-prometheus-stack 安装","section":"Prometheus","content":"kube-prometheus-stack 是 prometheus 的官方 helm charts，包含 prometheus-operator、prometheus、grafana、alertmanager、node-exporter 等组件。\n安装 kube-prometheus-stack # 使用 helm charts 安装 kube-prometheus-stack\nmkdir -p ~/charts/kube-prometheus-stack cd ~/charts/kube-prometheus-stack helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update # values.yaml 用来查看默认值 helm show values prometheus-community/kube-prometheus-stack \u0026gt; values.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; custom-values.yaml prometheus: prometheusSpec: additionalScrapeConfigs: [] podMonitorSelectorNilUsesHelmValues: false ruleSelectorNilUsesHelmValues: false probeSelectorNilUsesHelmValues: false scrapeConfigSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false storageSpec: volumeClaimTemplate: spec: storageClassName: \u0026#34;ceph-block\u0026#34; accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] resources: requests: storage: 50Gi service: type: NodePort ingress: enabled: true ingressClassName: nginx hosts: [\u0026#39;prometheus.lan\u0026#39;] replicas: 1 retention: 10d prometheusOperator: enabled: true # grafana service grafana: service: type: NodePort ingress: enabled: true ingressClassName: nginx hosts: [\u0026#39;grafana.lan\u0026#39;] persistence: enabled: true type: sts storageClassName: \u0026#34;ceph-block\u0026#34; accessModes: - ReadWriteOnce size: 20Gi alertmanager: enabled: true nodeExporter: enabled: true EOF helm upgrade --install --create-namespace --namespace monitoring kube-prometheus-stack prometheus-community/kube-prometheus-stack -f custom-values.yaml 配置说明 # 有一个相关 issue 讨论：servicemonitor not being discovered\nprometheus: prometheusSpec: podMonitorSelectorNilUsesHelmValues: false ruleSelectorNilUsesHelmValues: false probeSelectorNilUsesHelmValues: false scrapeConfigSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false 如果没有配置上面这些，在相关 selector 为空时，就会使用 release: kube-prometheus-stack 作为默认的，自己创建的 ServiceMonitor 等资源如果没有设置此 label 就会被会被自动服务发现。比较坑人!!!\n# kubectl -n monitoring get prometheus -oyaml apiVersion: v1 items: - apiVersion: monitoring.coreos.com/v1 kind: Prometheus name: kube-prometheus-stack-prometheus namespace: monitoring spec: podMonitorNamespaceSelector: {} podMonitorSelector: matchLabels: release: kube-prometheus-stack probeNamespaceSelector: {} probeSelector: matchLabels: release: kube-prometheus-stack ruleNamespaceSelector: {} ruleSelector: matchLabels: release: kube-prometheus-stack scrapeConfigNamespaceSelector: {} scrapeConfigSelector: matchLabels: release: kube-prometheus-stack serviceMonitorNamespaceSelector: {} serviceMonitorSelector: matchLabels: release: kube-prometheus-stack 配置 grafana dashboard # 默认账号密码:\n账号：admin 密码：prom-operator 导入一个查看 node exporter 的 dashboard\nhttps://grafana.com/grafana/dashboards/16098-node-exporter-dashboard-20240520-job/\n"},{"id":2,"href":"/posts/node-exporter/","title":"node-exporter 安装","section":"Prometheus","content":" node-exporter 安装 # 生成账号密码的 bcrypt hash # apt install apache2-utils -y 生成一个账号密码的 bcrypt hash\n-B 强制使用 bcrypt 算法 -C 10 指定 bcrypt 的 cost 值为 10, golang bcrypt 默认 cost 值也为 10 注意修改下面的 username 和 password 为你要设置的账号密码\n# htpasswd -nbBC 10 username password username:$2y$10$poDYDLemE3r95gcQ.h8FdODudFaFZhwZCSX1RTwpI2s8V4Mwm0.lO 关于 bcrypt # 格式为 $2\u0026lt;a/b/x/y\u0026gt;$[cost]$[22 character salt][31 character hash]\n例如\n$2y$10$poDYDLemE3r95gcQ.h8FdODudFaFZhwZCSX1RTwpI2s8V4Mwm0.lO \\__/\\/ \\____________________/\\_____________________________/ Alg Cost Salt Hash 运行 node-exporter # 创建 prometheus 配置文件 /etc/prometheus/web.yml 创建 docker-compose.yml 文件 运行 docker compose PASS=\u0026#39;$2y$10$poDYDLemE3r95gcQ.h8FdODudFaFZhwZCSX1RTwpI2s8V4Mwm0.lO\u0026#39; mkdir -p /etc/prometheus cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/prometheus/web.yml basic_auth_users: # username: password prometheus: ${PASS} EOF cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/prometheus/docker-compose.yml services: node-exporter: image: quay.io/prometheus/node-exporter:latest container_name: node-exporter command: - \u0026#34;--path.rootfs=/host\u0026#34; - \u0026#34;--web.config.file=/etc/prometheus/web.yml\u0026#34; network_mode: \u0026#34;host\u0026#34; pid: host restart: always volumes: - \u0026#39;/:/host:ro,rslave\u0026#39; - /etc/prometheus/web.yml:/etc/prometheus/web.yml EOF docker compose -f /etc/prometheus/docker-compose.yml up -d 在 prometheus 中配置指标收集 # 方式一：直接修改 prometheus 配置 # 修改 kube-prometheus-stack chart 配置并更新，或者直接修改保存配置的 configmap 中的 job 配置, 记得修改 {EDIT_HERE} 为实际值\ncat \u0026lt;\u0026lt;EOF\u0026gt; custom_values.yaml # prometheus service prometheus: prometheusSpec: additionalScrapeConfigs: - job_name: \u0026#39;node-exporter-external\u0026#39; basic_auth: username: {EDIT_HERE} # 这里是明文账号密码 password: {EDIT_HERE} static_configs: - targets: - \u0026#39;{EDIT_HERE}:9100\u0026#39; service: type: NodePort # grafana service grafana: service: type: NodePort EOF helm upgrade --install --create-namespace --namespace monitoring kube-prometheus-stack -f custom-values.yaml prometheus-community/kube-prometheus-stack 方式二：配置 ServiceMonitor 进行自动服务发现 # 把拉取 metrics 时需要的认证信息保存在 secret 中，然后创建 smon(ServiceMonitor)\napiVersion: v1 kind: Secret metadata: name: node-exporter-external namespace: monitoring data: username: {EDIT_HERE} # 这里是 base64 后的账号密码 password: {EDIT_HERE} type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: node-exporter-external namespace: monitoring spec: namespaceSelector: matchNames: - monitoring selector: matchLabels: app: node-exporter-external jobLabel: app endpoints: - port: metrics path: /metrics interval: 5s basicAuth: username: name: node-exporter-external key: username password: name: node-exporter-external key: password 然后创建指向 node-expoter 服务的 endpoint 和同名 service\napiVersion: v1 kind: Endpoints metadata: labels: app: node-exporter-external # 用来被 smon select name: node-exporter-external namespace: monitoring subsets: # 所有 node exporter 地址信息 - addresses: - ip: 192.168.1.100 nodeName: home ports: - name: metrics port: 9100 --- apiVersion: v1 kind: Service metadata: labels: app: node-exporter-external # 用来设置 job 名称 name: node-exporter-external namespace: monitoring spec: ports: - name: metrics port: 9100 targetPort: metrics "},{"id":3,"href":"/posts/tmux/","title":"tmux 使用笔记","section":"其他","content":"tmux 是一个强大的终端复用工具，可以在一个终端窗口中管理多个会话、窗口和窗格。一个令我相见恨晚的工具，下面记录一下安装和常用命令与快捷键。\n安装 # 在 Debian / Ubuntu 使用 apt 安装\napt install tmux 在 MacOS 使用 brew 安装\nbrew install tmux 使用 # 以下是常用的快捷键和命令\n也可以用使用 Ctrl+b ? 查看所有快捷键\n1. session 会话管理 # 功能 命令 创建新 session 并指定名称 tmux new -s \u0026lt;session-name\u0026gt; 列出当前所有 session tmux ls 常用的几个 session 管理快捷键如下\n功能 快捷键 切换多个 session Ctrl+b s 分离当前 session Ctrl+b d 重命名当前 session Ctrl+b $ 2. window 窗口管理 # 功能 快捷键 切换多个窗口 Ctrl+b w 切换到下一个窗口 Ctrl+b n 切换到前一个窗口 Ctrl+b p 创建一个新窗口 Ctrl+b c 给当前窗口命名 Ctrl+b , 3. pane 窗格管理 # 功能 快捷键 划分左右两个窗格 Ctrl+b % 划分上下两个窗格 Ctrl+b \u0026quot; 光标切换到其他窗格 Ctrl+b \u0026lt;arrow key\u0026gt; 调整窗格大小 Ctrl+b Ctrl+\u0026lt;arrow key\u0026gt; 关闭当前窗格 Ctrl+b x 窗格内滚动查看 Ctrl+b [ "},{"id":4,"href":"/posts/openwrt-v2/","title":"OpenWrt v2","section":"Network","content":"最近更新了家中 OpenWrt 的网络, 在宿主机增加一个 USB 网卡连通互联网, 拓扑图如下:\n准备 qcow2 镜像 # 首先下载最新的镜像, 截止目前最新版为23.05.3, 我这里下载的是x86-64的镜像 text\nwget https://mirror-03.infra.openwrt.org/releases/23.05.3/targets/x86/64/openwrt-23.05.3-x86-64-generic-ext4-combined.img.gz gunzip openwrt-23.05.3-x86-64-generic-ext4-combined.img.gz qemu-img convert -f raw openwrt-23.05.3-x86-64-generic-ext4-combined.img -O qcow2 /var/lib/libvirt/images/openwrt.qcow2 配置宿主机桥接网络 # /etc/netplan/ 目录只放 01-all.yaml 配置文件并执行 netplan apply 应用配置。\n因为 USB 网卡重启后名称会变, 所以我这里通过 mac match 作了一个别名\nnetwork: version: 2 renderer: NetworkManager ethernets: enp1s0: dhcp4: false dhcp6: false usb-nic: match: macaddress: \u0026#34;68:da:73:a1:c7:13\u0026#34; dhcp4: false dhcp6: false bridges: br0: dhcp4: false dhcp6: false addresses: - 192.168.1.100/24 routes: - to: default via: 192.168.1.99 nameservers: addresses: - 192.168.1.99 search: - lan interfaces: - enp1s0 parameters: stp: false br1: dhcp4: false dhcp6: false addresses: - 192.168.31.100/24 interfaces: - usb-nic parameters: stp: false 启动 OpenWrt 虚拟机 # 我是用 libvirt 来管理qemu/kvm虚拟机, 如果没安装要先安装\napt install virt-manager qemu bridge-utils -y 我这里将镜像复制到了 /var/lib/libvirt/disks/ 目录下\nqemu-img create -f qcow2 -F qcow2 -b /var/lib/libvirt/images/openwrt.qcow2 /var/lib/libvirt/disks/openwrt.qcow2 1G 使用virt-install运行虚拟机, 这里网卡使用virtio类型并桥接到之前文档里创建的 br0 和 br1 上, 选择virtio是因为性能好, 可以达到 20Gbps\nvirt-install \\ --name openwrt \\ --memory 512 \\ --vcpus 1 \\ --network bridge=br0,model=virtio \\ --network bridge=br1,model=virtio \\ --disk path=/var/lib/libvirt/disks/openwrt.qcow2,bus=virtio \\ --os-type linux \\ --os-variant generic \\ --machine q35 \\ --import \\ --autostart \\ --graphics none \\ --noautoconsole 配置 OpenWrt 网络 # 连接console配置网络\nvirsh console openwrt 修改网络配置文件 /etc/config/network\nconfig interface \u0026#39;loopback\u0026#39; option device \u0026#39;lo\u0026#39; option proto \u0026#39;static\u0026#39; option ipaddr \u0026#39;127.0.0.1\u0026#39; option netmask \u0026#39;255.0.0.0\u0026#39; config device \u0026#39;lan_br\u0026#39; option name \u0026#39;br-lan\u0026#39; option type \u0026#39;bridge\u0026#39; list ports \u0026#39;eth0\u0026#39; list ports \u0026#39;phy0-ap0\u0026#39; config interface \u0026#39;lan\u0026#39; option device \u0026#39;br-lan\u0026#39; option proto \u0026#39;static\u0026#39; option ipaddr \u0026#39;192.168.1.99\u0026#39; option netmask \u0026#39;255.255.255.0\u0026#39; option ipv6 \u0026#39;0\u0026#39; list dns \u0026#39;223.5.5.5\u0026#39; config interface \u0026#39;wan\u0026#39; option device \u0026#39;eth1\u0026#39; option proto \u0026#39;static\u0026#39; option ipaddr \u0026#39;192.168.31.88\u0026#39; option netmask \u0026#39;255.255.255.0\u0026#39; option ipv6 \u0026#39;0\u0026#39; option gateway \u0026#39;192.168.31.1\u0026#39; option type \u0026#39;bridge\u0026#39; 修改之后重启网络\nservice network restart 测试路由和DNS解析是否正常: ping baidu.com, 一切OK再继续下面的\n更换 OpenWrt 软件源 # 大陆码农生存必备技能了, 这里使用的中科大的源, 配置文件位于 /etc/opkg/distfeeds.conf\nsrc/gz openwrt_core http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/targets/x86/64/packages src/gz openwrt_base http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/base src/gz openwrt_luci http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/luci src/gz openwrt_packages http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/packages src/gz openwrt_routing http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/routing src/gz openwrt_telephony http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/telephony 然后更新一下\nopkg update 扩容根分区和文件系统 # 如果觉得默认的100M左右就够用了可以跳过这步\n这个文档里的脚本只支持x86的ext4和squashfs镜像创建的虚机, 自动检测根分区和文件系统, 将空闲空间分给根分区和文件系统\nopenwrt.org/docs/guide-user/advanced/expand_root\n扩容之后就使用起来就不用扣扣搜搜的了\nroot@OpenWrt:~# df -hT / Filesystem Type Size Used Available Use% Mounted on /dev/root ext4 994.8M 56.9M 921.9M 6% / 配置 LAN 口 DHCP 服务 # 修改配置文件 /etc/config/dhcp\nconfig dhcp \u0026#39;lan\u0026#39; option interface \u0026#39;lan\u0026#39; option start \u0026#39;150\u0026#39; option limit \u0026#39;100\u0026#39; option leasetime \u0026#39;12h\u0026#39; option ignore \u0026#39;0\u0026#39; 修改之后重启 service dnsmasq restart, 配置之后有段时间dhcp server没正常运行, 可通过 logread -e dnsmasq 查看服务日志排查.\n正常运行之后dhcp server会监听67端口\nroot@OpenWrt:~# netstat -anp | grep :67 udp 0 0 0.0.0.0:67 0.0.0.0:* 27573/dnsmasq 配置无线网 # 2025-05-10 去掉无线网配置，改为直接使用路由器并且关闭DHCP，AX200 直通进虚拟机后做 AP 性能比较差，吞吐带宽低并且延迟很高\n将 AX200 无线网卡直通到虚拟机 # 直接在 Virtual Machine Manager 中选择 Add Hardware -\u0026gt; PCI Host Device 即可\n$ lspci -D | grep AX200 0000:02:00.0 Network controller: Intel Corporation Wi-Fi 6 AX200 (rev 1a) 0000: 这是 Domain 号, 通常是 0000 02: 这是 Bus 号 00: 这是 Device 号, 在 libvirt XML 中对应 slot 0: 这是 Function 号 也可以通过命令行的方式, 先创建一个 pci-device.xml 文件, 其中 bus slot 和 function 从上面 lspci 结果获取\n\u0026lt;hostdev mode=\u0026#39;subsystem\u0026#39; type=\u0026#39;pci\u0026#39; managed=\u0026#39;yes\u0026#39;\u0026gt; \u0026lt;source\u0026gt; \u0026lt;address domain=\u0026#39;0x0000\u0026#39; bus=\u0026#39;0x02\u0026#39; slot=\u0026#39;0x00\u0026#39; function=\u0026#39;0x0\u0026#39;/\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;/hostdev\u0026gt; 然后热添加到虚拟机中并持久化进配置\nvirsh attach-device openwrt pci-device.xml --live --config 配置 AX200 无线网卡为 AP 模式 # 默认情况下, OpenWrt 的系统(system)菜单下面找不到无线(wireless)选项，需要进行下面的安装\n无线网卡型号是 AX200 的驱动安装:\nopkg update # 针对 intel 网卡 opkg install kmod-iwlwifi opkg install iwlwifi-firmware-ax200 # 需要重启 reboot 修改文件 /etc/config/wireless 设置 AP 模式, AX200型号网卡受iwlwifi驱动限制, 只能在2.4GHz带宽下工作\nconfig wifi-device \u0026#39;radio0\u0026#39; option type \u0026#39;mac80211\u0026#39; option path \u0026#39;pci0000:00/0000:00:01.4/0000:05:00.0\u0026#39; option htmode \u0026#39;HT20\u0026#39; option disable \u0026#39;0\u0026#39; option cell_density \u0026#39;0\u0026#39; option band \u0026#39;2g\u0026#39; option channel \u0026#39;11\u0026#39; option country \u0026#39;CN\u0026#39; config wifi-iface \u0026#39;default_radio0\u0026#39; option device \u0026#39;radio0\u0026#39; option network \u0026#39;lan\u0026#39; option mode \u0026#39;ap\u0026#39; option ssid \u0026#39;OpenWrt\u0026#39; option encryption \u0026#39;psk2\u0026#39; option key \u0026#39;88888888\u0026#39; option disable \u0026#39;0\u0026#39; 重新加载\nwifi 安装OpenClash # 和 OpenWrt 中配置一样\n参考 # OpenWrt in QEMU OpenWrt: Expanding root partition and filesystem OpenWrt: DHCP and DNS configuration /etc/config/dhcp OpenWrt: Clarifying the term \u0026ldquo;Interface\u0026rdquo; OpenClash 安装 "},{"id":5,"href":"/posts/reverse-nodes-in-k-group/","title":"25. K 个一组翻转链表","section":"链表","content":" 题目描述 # 给你链表的头节点 head ，每 k 个节点一组进行翻转，请你返回修改后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。\n示例 1：\n输入： head = [1,2,3,4,5], k = 2\n输出： [2,1,4,3,5]\n示例 2：\n输入： head = [1,2,3,4,5], k = 3\n输出： [3,2,1,4,5]\n提示：\n链表中的节点数目为 n 1 \u0026lt;= k \u0026lt;= n \u0026lt;= 5000 0 \u0026lt;= Node.val \u0026lt;= 1000 进阶： 你可以设计一个只用 O(1) 额外内存空间的算法解决此问题吗？\n思路 # 代码 # TODO "},{"id":6,"href":"/posts/go-delve/","title":"Go Delve","section":"开发工具","content":" 介绍 # Delve 是 Go 编程语言的调试器。\n原理 # TODO\n在 vscode 中使用 # 将 request 设置为 attach 并且 processId 设置为 0, 这样就会每次执行\n在 go 项目根目录下设置 .vscode/launch.json 文件如下:\n\u0026quot;type\u0026quot;: \u0026quot;go\u0026quot;: 配置类型为 go \u0026quot;request\u0026quot;: \u0026quot;attach\u0026quot;: 这里配置为 attach 到一个已经运行的进程 \u0026quot;mode\u0026quot;: \u0026quot;local\u0026quot;: 相当于 attach 到本地进程 \u0026quot;processId\u0026quot;: 0: 相当于没指定 pid, 每次启动 debug 会让选择进程 { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Attach to Process\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;processId\u0026#34;: 0 } ] } 参考 # github.com/golang/vscode-go/docs/debugging "},{"id":7,"href":"/posts/go-testing/","title":"Go Testing","section":"开发工具","content":" 介绍 # testing 包为 Go 包自动化测试提供支持，它与 go test 命令一起使用，该命令可以自动执行下面这种格式的函数。\nfunc TestXxx(*testing.T) 其中函数名中的 Xxx 不能以小写字母开头。\n在函数中，使用 Error, Fail 或相关的函数表示失败。\n要编写测试函数，需要县创建一个以 _test.go 结尾的文件，该文件会被排除在常规包构建之外，但将在运行 go test 命令时被包含在内。\n一般将 _test.go 文件创建在要被测试的函数的同包下面\n测试函数示例 # 比如这里我要验证 interface 与 nil 是否相等\n// test/interface_test/interface_test.go package interface_test import \u0026#34;testing\u0026#34; func TestInterfaceNil(t *testing.T) { var a interface{} var b = (*int)(nil) a = b if a == nil { t.Fatal(\u0026#34;a == nil\u0026#34;) } } go test 参数 # 有一些常用参数，可以通过 go help test 和 go help testflag 查看。\ngo test -run regexp # 指定正则匹配到的测试函数，通过 ^ 标记开头和 $ 标记结尾可以精准匹配。\n$ go test -run ^TestInterfaceNil$ test/interface_test ok test/interface_test 0.002s go test -v # 默认 go test 通过的不会打印详细过程, 加上 -v 显示详细过程\ngo test -v -run ^TestInterfaceNil$ test/interface_test === RUN TestInterfaceNil --- PASS: TestInterfaceNil (0.00s) PASS ok test/interface_test 0.002s go test ./\u0026hellip; # 执行所有包下面的所有测试函数\n$ go test ./... ok test/interface_test 0.002s ? test/lru [no test files] 参考 # pkg.go.dev/testing "},{"id":8,"href":"/posts/go-interface/","title":"Go interface","section":"数据结构","content":" interface 底层结构 # interface{} 底层存储结构如下所示：\n// ----------- runtime/runtime2.go ----------------- type iface struct { tab *itab data unsafe.Pointer } type eface struct { _type *_type data unsafe.Pointer } type itab = abi.ITab type _type = abi.Type // ----------- internal/abi/iface.go ----------------- // The first word of every non-empty interface type contains an *ITab. // It records the underlying concrete type (Type), the interface type it // is implementing (Inter), and some ancillary information. // // allocated in non-garbage-collected memory type ITab struct { Inter *InterfaceType Type *Type Hash uint32 // copy of Type.Hash. Used for type switches. Fun [1]uintptr // variable sized. fun[0]==0 means Type does not implement Inter. } 可以看到 interface{} 底层被存储为两种类型:\niface 表示有方法集的接口类型变量 eface 表示没有方法的空接口类型变量，也就是 interface{} 类型的变量 interface 注意要点 # 判断 interface{} 是否等于 nil # 比如以下函数 returnError 返回 error, 处理返回的 error 时要判断是不是 nil\ntype MyError string func (e *MyError) Error() string { return string(*e) } func bad() bool { return false } func returnError() error { var err *MyError = nil if bad() { badErr := MyError(\u0026#34;bad error\u0026#34;) err = \u0026amp;badErr } return err } 既然是注意点，那就肯定不是简单的 v == nil 的方式比较。\n首先简化一下上述函数，因为一直不会进入 if bad() {}, 所以简化为\nfunc returnError() error { var err *MyError = nil return err } 这里也要注意，上述代码和和**直接 var err *MyError = nil 然后比较 err 是否为 nil 是不一样的。**这种 err是 *MyError 类型，可以直接与 nil 比较并且相等，不是 interface{}\n而 func returnError() error 相当与如下代码，返回的是 interface{}:\nvar ret error var err *MyError = nil ret = err return ret 上述代码中, 相当于把一个类型为 *MyError 但是值为 nil 的变量赋值给了 interface{} 类型的 ret，因为 ret 有实现 error 接口的方法 func Error() string，所以底层使用 iface 存储，其中:\niface.tab.Type 会存储 *MyError 类型 iface.data 为 nil 当 ret 直接与 nil 比较时会先比较类型，*MyError 和 nil 不相等，如果直接比较就容易被坑!!!\n所以：\n当调用一些奇怪函数返回 interface{} 类型变量或者函数接收 interface{} 类型变量并且需要判断是否为 nil 时就要张个心眼。可以使用如下函数判断一个 interface{} 是否为 nil：\nfunc IsNil(v interface{}) bool { if v == nil { return true } switch reflect.TypeOf(v).Kind() { case reflect.Chan, reflect.Func, reflect.Slice, reflect.Map, reflect.Ptr: return reflect.ValueOf(v).IsNil() default: return false } } 参考 # 理解Go interface的两种底层实现:iface和eface Go: Check Nil interface the right way Why is my nil error value not equal to nil? "},{"id":9,"href":"/posts/procfs/","title":"proc filesystem","section":"Storage","content":"procfs 是一个特殊的文件系统，包含一个伪文件系统（启动时动态生成的文件系统），用于通过内核访问进程信息。这个文件系统通常被挂载到 /proc 目录。由于 proc 不是一个真正的文件系统，它也就不占用存储空间，只是占用有限的内存。\n下面以 bash 进程为例查看 /proc/PID/ 下的信息。\n/proc/PID/exe # 指向原始的可执行文件\n$ ls -lh /proc/3553026/exe lrwxrwxrwx 1 root root 0 4月 10 22:09 /proc/3553026/exe -\u0026gt; /usr/bin/bash /proc/PID/fd # 是一个目录, 包含此进程打开的所有文件描述符 (file descriptors)\n$ ls -lhv /proc/3553026/fd total 0 lrwx------ 1 root root 64 4月 10 22:06 0 -\u0026gt; /dev/pts/14 lrwx------ 1 root root 64 4月 10 22:06 1 -\u0026gt; /dev/pts/14 lrwx------ 1 root root 64 4月 10 22:06 2 -\u0026gt; /dev/pts/14 lrwx------ 1 root root 64 4月 10 22:06 255 -\u0026gt; /dev/pts/14 TODO # 参考 # The /proc Filesystem wiki/Procfs "},{"id":10,"href":"/posts/netpoll/","title":"Netpoll","section":"OS","content":"关于 select, poll 和 epoll 系统调用\nselect # poll # epoll # "},{"id":11,"href":"/posts/filebrowser/","title":"Filebrowser 部署","section":"其他","content":" Filebrowser 介绍 # Filebrowser 可以通过 web 管理指定文件目录, 并且支持多用户隔离在不同子目录下.\nFilebrowser 部署在 k8s 中 # 在 k8s 中部署 filebrowser\n装完后默认密码 admin / admin, File Browser Install\n配置文件样例: settings.json\n关于数据持久化，都是持久化在 ceph 块存储中，使用 sc/ceph-block\n数据库存储在 pvc/filebrowser-database-pvc 中，分配 1Gi 文件目录存储在 pvc/filebrowser-data-pvc 中，分配 50Gi apiVersion: v1 kind: Namespace metadata: name: filebrowser --- apiVersion: v1 kind: ConfigMap metadata: name: filebrowser-config namespace: filebrowser data: settings.json: | { \u0026#34;port\u0026#34;: 80, \u0026#34;baseURL\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;log\u0026#34;: \u0026#34;stdout\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;/database/filebrowser.db\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;/srv\u0026#34; } --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: filebrowser-database-pvc namespace: filebrowser spec: accessModes: - ReadWriteOnce storageClassName: ceph-block resources: requests: storage: 1Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: filebrowser-data-pvc namespace: filebrowser spec: accessModes: - ReadWriteOnce storageClassName: ceph-block resources: requests: storage: 50Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: filebrowser namespace: filebrowser labels: app: filebrowser spec: replicas: 1 selector: matchLabels: app: filebrowser template: metadata: labels: app: filebrowser spec: containers: - env: - name: PUID value: \u0026#34;0\u0026#34; - name: PGID value: \u0026#34;0\u0026#34; name: filebrowser image: filebrowser/filebrowser:s6 ports: - containerPort: 80 volumeMounts: - name: filebrowser-config mountPath: /config - name: filebrowser-database mountPath: /database - name: filebrowser-data mountPath: /srv volumes: - name: filebrowser-config configMap: name: filebrowser-config - name: filebrowser-database persistentVolumeClaim: claimName: filebrowser-database-pvc - name: filebrowser-data persistentVolumeClaim: claimName: filebrowser-data-pvc --- apiVersion: v1 kind: Service metadata: name: filebrowser namespace: filebrowser spec: selector: app: filebrowser ports: - port: 80 targetPort: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: filebrowser-ingress namespace: filebrowser annotations: nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;0\u0026#34; spec: rules: - host: file.lan http: paths: - path: / pathType: Prefix backend: service: name: filebrowser port: number: 80 "},{"id":12,"href":"/posts/partitioning-disks/","title":"Linux 磁盘分区","section":"Storage","content":"https://www.baeldung.com/linux/partitioning-disks https://en.wikipedia.org/wiki/Master_boot_record https://www.cnblogs.com/god-of-death/p/18221794\n概览 # 通常安装 Linux 系统的第一步就是磁盘分区。在我们创建任何文件前需要先存在文件系统。\n分区被用来将原始存储空间分割成大块，可以用来隔离文件系统故障。\n磁盘类型 (Disk Types) # 分区表格式 MBR 和 GPT # MBR (Master Boot Record) 和 GPT (GUID Partition Table) 是最广泛使用的分区表，相较于 GPT，MBR 是一个老的标准并且有一些限制。\nBIOS 和 UEFI # 分区工具 # fdisk 和 parted\nTODO\n"},{"id":13,"href":"/posts/inode/","title":"Linux 文件系统之 inode","section":"Storage","content":"TODO\nhttps://www.ruanyifeng.com/blog/2011/12/inode.html\ninode 是什么 # inode 是 Linux 文件系统中的一个重要概念，它是一个文件的元数据，记录了文件的权限、类型、大小、创建时间、修改时间等信息。\ninode 的结构 # inode 的结构如下:\nstruct inode { umode_t i_mode; // 文件类型和权限 uid_t i_uid; // 文件所有者 gid_t i_gid; // 文件所属组 loff_t i_size; // 文件大小 struct timespec i_atime; // 文件访问时间 struct timespec i_ctime; // 文件创建时间 struct timespec i_mtime; // 文件修改时间 } 使用场景 # 通过 inode 查找并删除文件 # # ls -li total 32 459 -rw-r--r-- 1 root root 20070 Mar 13 11:27 \u0026#39;\u0026#39;$\u0026#39;\\033\\033\u0026#39; 526984 drwxr-xr-x 5 root root 4096 Jan 1 17:47 charts 784 drwxr-xr-x 2 root root 4096 Jan 19 20:42 pods 49203 drwx------ 3 root root 4096 Feb 10 2023 snap # find . -inum 459 ./?? # find . -inum 459 -delete # ls -li total 12 526984 drwxr-xr-x 5 root root 4096 Jan 1 17:47 charts 784 drwxr-xr-x 2 root root 4096 Jan 19 20:42 pods 49203 drwx------ 3 root root 4096 Feb 10 2023 snap 参考 # "},{"id":14,"href":"/posts/linux-boot-process-bios/","title":"Linux 启动流程 (BIOS)","section":"OS","content":" 启动流程 # 开机自检(POST) -\u0026gt; 主板固件(BIOS) -\u0026gt; Boot Loader 引导(grub2) vmlinuz运行 -\u0026gt; 内核挂载文件系统 -\u0026gt; 初始化(systemd)\n开机自检 (POST) # 当计算机开机时，BIOS(Basic Input Output System) 首先进行自检 POST(Power-On Self Test)，检查硬件组件（如内存、硬盘、显卡等）是否正常工作\n加载引导程序 # TODO\n其他 # 使用 initial RAM disk (initrd) # initrd 提供了通过 boot loader 程序加载 RAM 磁盘的能力，然后这个 RAM 磁盘可以被挂载为根文件系统，并可以从中运行程序。之后，可以从不同的设备挂载新的文件系统，然后将前一个根目录(来自initrd)移动到一个目录，然后可以卸载。\ninitrd 主要设计为允许系统启动分为两个阶段进行，其中内核携带最少的编译进去的驱动程序集，从 initrd 加载其他模块。\n当时用 initrd，系统通常按照下面顺序启动：\nboot loader(一般是grub) 加载内核 vmlinuz 和 initial RAM disk initrd.img, 具体的文件名要看 /boot/grub/grub.cfg 中的 menu 设置。 TODO\ninitrd 与 microcode 微码加载 # 内核可以在启动的非常早期阶段更新微码，早期加载微码可以在内核启动阶段发现CPU问题之前修复问题。\n微码也会被存储在 initrd 文件中。在启动时，它被读出来并加载到CPU核中。\n微码文件在 initrd image 中存储如下\nlsinitramfs /boot/initrd.img | grep -E \u0026#39;microcode.*bin\u0026#39; kernel/x86/microcode/AuthenticAMD.bin kernel/x86/microcode/GenuineIntel.bin 更新微码步骤一般为：\n在 intel 或 amd 网站下载微码并解压 复制到对应位置 /lib/firmware/{amd-ucode,intel-ucode} 使用 cpio 将微码打包成但文件后输出到 /boot/initrd.img 中, 也可以直接使用 update-initramfs -u -k all 参考 # Using the initial RAM disk (initrd) 23. The Linux Microcode Loader Guide to the Boot Process of a Linux System Boot Process In Linux – Detailed Steps For Beginners "},{"id":15,"href":"/posts/go-memory-escape/","title":"Go 内存逃逸","section":"语言基础","content":" 内存逃逸 # 可以通过 go build -gcflags=-m main.go 分析内存逃逸\n编译阶段不能确定大小的变量以及生命周期超出函数的局部变量数据都会逃逸到堆中。\n1. 指针逃逸 # 函数返回指向局部变量的指针，变量内存不能随着函数结束而回收，只能分配在堆上 函数调用其他寿命更长的函数时，将局部变量指针传递过去，同理。 2. 将变量存储到 interface{} 变量中 # TODO 如果函数参数为 interface{}，编译期间很难确定其参数的类型以及大小，也会发生逃逸。\nhttps://goperf.dev/01-common-patterns/interface-boxing/\n3. 栈空间不足 # 每个 goroutine 都维护着一个自己的栈区，初始大小 2KB，栈结构经过了分段栈到连续栈的发展。\n分配大变量，如大 slice，或者大小不确定的变量，会有可能栈空间不足，然后编译器将其分配在堆上，虽然栈会自动增长，但是也有大小限制(TODO)。\n4. 闭包捕获变量 # 当一个闭包函数引用了外部变量并且会执行后续读写操作，则变量会被逃逸到堆上。\npackage main func Increase() func() int { n := 0 // move to heap return func() int { n++ return n } } func main() { in := Increase() println(in()) // 1 } 5. 变量地址存储在引用类型对象中 # 如将变量地址保存在 切片(slice)、映射(map)、通道(channel)、 接口(interface) 以及 函数(func) 中，那么此变量会逃逸到堆上.\nfunc main() { a := make([]*int, 100) b := int(1) a = append(a, \u0026amp;b) } slice 和 map 的值不是指针类型时不会逃逸\n6. 初始化 channel # channel 一定是跨 goroutine 使用的，直接初始化在堆中\n参考 # Go栈内存管理 7.3 栈空间管理 Contiguous stacks "},{"id":16,"href":"/posts/go-gc/","title":"Go GC 垃圾回收","section":"语言基础","content":" GC 垃圾回收 # 分配在栈上的数据，随着函数调用栈的销毁便释放了自身占用的内存，可以被程序重复利用。\n协程栈也是从堆上分配的，也在 mheap 管理的 span 中，mspan.spanState 会记录该 span 是用作堆内存还是栈内存。\n而分配在堆上的数据，他们占用的内存需要程序主动释放才可以重新使用，否则称为垃圾。\n三色标记原理 # 三色标记法，白色，灰色和黑色\n垃圾回收开始会把所有数据（栈、堆、数据段）都标记为白色 然后把直接追踪(扫描全局数据区和栈区)到的 root 节点标记为灰色，灰色代表基于当前节点展开的追踪还未完成。 基于某个节点的追踪任务完成后标记为黑色，标识有用并且无需基于它再追踪。 没有灰色节点后意味着标记工作结束。此时有用的数据为黑色，垃圾都是白色，在清除阶段回收这些白色的垃圾即可。 混合写屏障 # 通过 混合写屏障 防止GC过程中并发修改对象的问题。\n混合写屏障 继承了插入写屏障的优点，起始时无需 STW 打快照，直接并发扫描垃圾即可 混合写屏障 继承了删除写屏障的优点，赋值器是黑色赋值器，GC期间，任何在栈上创建的新对象，均为黑色。扫描过后就不需要扫描了，这样就消除了插入写屏障最后 STW 的重新扫描栈了。 混合写屏障 扫描栈虽然不用 STW，但是扫描某一个具体的栈的时候，还是要停止这个 goroutine 赋值器的工作（针对一个 goroutine 来说，是暂停扫的，要么全灰，要么全黑，是原子状态切换的） GC 触发时机 # 主动触发：调用 runtime.GC 被动触发：使用系统监控 sysmon，该触发条件由 runtime.forcegcperiod 控制，默认为 2 分钟。当超过时间没有产生任何 GC 时，强制触发 GC。使用步调算法。。。 GC 流程 # 标记设置 Mark Setup (STW) # 打开写入屏障\n需要STW, 所有 goroutine 需要暂停工作并执行, 没有抢占之前这里如果存在 tight loop operation 会消耗很长等待的时间.\n并发标记 Marking Concurrent # 一旦写入屏障打开, 并发标记就开始工作, 标记阶段主要是标记出还在使用的堆内存.\n回收器(collector) 会占用 25% 的可用 CPU 容量, 使用 Goroutine 执行回收工作并且使用和应用 Goroutine 相同的 P 和 M.\n标记流程为:\nTODO 补充三色标记流程\n寻找根节点, 把 全局数据区 和 栈区 的节点标记为 灰色 标记辅助(Mark Assist)可以用来加速标记阶段执行. 通过 runtime.gcAssistAlloc 函数实现, 当某个goroutine分配内存过快时，调度器会强制其执行更多标记工作\n标记终止 Mark Termination (STW) # 关闭写入屏障, 执行各种清理任务并计算下一个回收目标.\n应用程序恢复到全功率.\n并发清除 Sweeping Concurrent # 清除是指回收 reclaim 与堆内存中未被标记为在使用的值关联的内存.\n注意这里的 reclaim 不一定是被操作系统立即回收的, 表现形式就是回收了, 但是进程占用的内存没少.\nTODO: 补充 reclaim 的操作系统相关接口调用\nGC 对应用性能影响 # 窃取 25% CPU 容量 STW 延时 参考 # Go\u0026rsquo;s garbage collector "},{"id":17,"href":"/posts/go-make-and-new/","title":"Go make 与 new 区别","section":"语言基础","content":" make 和 new 的区别 # 都是内存分配函数\n1. 基本用途 # make 仅用于创建 slice、map 和 channel，并且会初始化这些类型的内部数据结构，返回初始化后的值类型 new 可用于任何类型，返回指向该类型零值的指针\n2. 返回值 # make返回初始化后的值类型 new返回指向该类型零值的指针\n3. 内存分配 # make会分配内存并初始化数据结构 new只分配内存，并把内存置零，不做初始化\n"},{"id":18,"href":"/posts/go-gmp/","title":"Go GMP 模型","section":"语言基础","content":" CSP # CSP(Communicating Sequential Processes) 被认为是 Go 在并发编程中成功的关键，论文指出应该重视 input 和 output 原语，尤其是并发编程的代码。\nGMP介绍 # G M P 是 Go 调度器的三个核心组件\nG 对应 goroutine, 属于用户线程或绿色线程 # type g struct { stack stack // goroutine 使用的栈，存储了栈的范围 [lo, hi) m *m // 当前与 g 绑定的 m sched gobuf // goroutine 的运行现场, 存储各种寄存器的值，如 PC、SP等寄存器，M恢复现场时需要用到 } M 对应内核线程 # M 代表一个工作线程或者说系统线程，G需要调度到M上才能执行，和 P 绑定去获取 G 来执行。\n它保存了 M 自身使用的栈信息，当前正在M上执行的G信息，与之绑定的 P 信息。\n// m 代表工作线程，保存了自身使用的栈信息 type m struct { // 记录工作线程（也就是内核线程）使用的栈信息。在执行调度代码时需要使用 // 执行用户 goroutine 代码时，使用用户 goroutine 自己的栈，因此调度时会发生栈的切换 g0 *g // goroutine with scheduling stack/ // 通过 tls 结构体实现 m 与工作线程的绑定 // 这里是线程本地存储 tls [6]uintptr // thread-local storage (for x86 extern register) // 当前工作线程绑定的 p p puintptr // attached p for executing go code (nil if not executing go code) // 工作线程 id thread uintptr // thread handle // 记录所有工作线程的链表 alllink *m // on allm } P 是调度队列，包含缓存信息 # P 取 processor 首字母，为 M 的执行提供上下文，保存 M 执行 G 时的一些资源，例如本地可执行 G 队列、memory cache等。一个M只有绑定P才可以执行goroutine，当M阻塞时，整个P会被传递给其他M，或者说整个P被接管。\n每个 P 都有一个 mcache 用作本地 span 缓存，小对象分配时先从本地 mcache 中获取，没有的话就去 mcentral 获取并设置到 P ，mcentral 中也没有的话就会去 mheap 申请。\n// p 保存 go 运行时所必须的资源 type p struct { lock mutex // 指向绑定的 m，如果 p 是 idle 的话，那这个指针是 nil m muintptr // back-link to associated m (nil if idle) // Queue of runnable goroutines. Accessed without lock. // 本地可运行的队列，不用通过锁即可访问 runqhead uint32 // 队列头 runqtail uint32 // 队列尾 // 使用数组实现的循环队列 runq [256]guintptr // runnext 非空时，代表的是一个 runnable 状态的 G， // 这个 G 被 当前 G 修改为 ready 状态，相比 runq 中的 G 有更高的优先级。 // 如果当前 G 还有剩余的可用时间，那么就应该运行这个 G // 运行之后，该 G 会继承当前 G 的剩余时间 runnext guintptr } 关于 work stealing 机制 # 每个 P 与一个 M 绑定，M 是真正执行 goroutine 的实体，M 从绑定的 P 中的本地队列获取 G 来执行。\n当 M 绑定的 P 本地队列 runq 为空时，M 会从全局队列获取到本地队列来执行 G，当从全局队列中也没获取到可执行的 G 时，M 会从其他 P 的本地队列中偷取一半 G 来执行，被称为 work stealing 机制。\n"},{"id":19,"href":"/posts/go-chan/","title":"Go channel","section":"数据结构","content":" channel 数据结构 # channel 被用来实现 goroutine 间的通信，以通信的方式共享内存。\n如下代码使用 make 函数会在 堆上 分配一个 runtime.hchan 类型的数据结构并初始化，ch 是存在于 f 函数栈帧上的一个指针，指向堆上的 hchan 数据结构。\nfunc f() { ch := make(chan int) ... } channel 分为 无缓冲 和 有缓冲 两种。对于有缓冲的 channel 来说，使用循环数组 buf 来存储数据。\nGolang 图解channel之数据结构\nselect 和 channel 机制 # 通过 gouroutine 和 channel 实现并发中基于通信的内存同步, channel 还可以和 select、cancel、timeout 结合使用\nchannel 分为不带缓冲和带缓冲的，不带缓冲的可以认为是“同步模式”，带缓冲的可以认为是“异步模式”。\nchannel 的数据结构如下，包含一个 循环数组 buf 存储缓存中的数据 和当前读写在数组中的索引，以及缓存大小，还有一个阻塞\ntype hchan struct { qcount uint // chan 中元素数量, 对于无缓冲的为 0, 数据直接从发送方传递给接收方 dataqsiz uint // chan 底层循环数组的大小, 对于无缓冲的为 0 buf unsafe.Pointer // 指向底层循环数组的指针, 只针对有缓冲的 channel elemsize uint16 // chan 中元素大小 closed uint32 // chan 是否关闭的标志 timer *timer // timer feeding this chan elemtype *_type // chan 中元素类型 sendx uint // 下一个要发送元素在循环数组中的索引 recvx uint // 下一个要接收元素在循环数组中的索引 recvq waitq // 阻塞的尝试从此 channel 接收数据的 goroutine, sudog 双向链表 sendq waitq // 阻塞的尝试向此 channel 发送数据的 goroutine, sudog 双向链表 lock mutex // 保护 hchan 所有字段的锁 } type waitq struct { first *sudog last *sudog } 阻塞模式和非阻塞模式：由 select 是否包含 default 确定\n非阻塞模式在获取不到数据或者写入不了时会直接返回\n阻塞模式下会调用 gopark 函数挂起 goroutine，那么问题来了，阻塞模式 select 有很多 channel 时，挂起的 goroutine 信息会被写入到所有 channel 的对应 recvq 和 sendq 中？。\n1. 阻塞模式下(select无default / 直接从channel读写) # 阻塞模式下发送方写入数据到channel时没有使用 buf 循环数组，直接写入到接收方的接收变量地址中\n1.1 无缓冲 channel\n读：如果 sendq 中没有等待发送的 sudog，则阻塞，并将当前 goroutine 和 接收数据的变量地址填入到 sudog 中加入阻塞的读队列 recvq，当有要发送时\n写：如果\n1.2 有缓冲 channel\n2. 非阻塞模式下(select有default) # 2.1 无缓冲 channel\n2.2 有缓冲 channel\n"},{"id":20,"href":"/posts/linux-iptables/","title":"Linux iptables","section":"Network","content":" 持续更新\u0026hellip;\niptables 介绍 # 四表五链\n五链 # 为什么称为 链，因为每个 链 会有很多规则串在一起，每个经过的报文都要将这条链上的规则匹配一遍，如果有符合条件的规则，则执行规则对应的动作。\n每个链会包含多个表的规则，如果包含对应的表，则表之间的执行顺序为：\n# 具体每个表的作用看后面的 \u0026#34;四表\u0026#34; 介绍, 不是每个 \u0026#34;链\u0026#34; 都能包含全部的四个表 raw -\u0026gt; mangle -\u0026gt; nat -\u0026gt; filter PREROUTING 链 # 数据包抵达系统内核空间时，由 PREROUTING 链负责\nINPUT 链 # 进入内核空间后，如果检测到目的地址是本机，则由 INPUT 链负责\nFORWARD 链 # 数据包如果不是要到本机，只是经过本机路由，就由 FORWARD 链负责\nOUTPUT 链 # 数据包如果从本机出去，就由 OUTPUT 链负责\nPOSTROUTING 链 # 从内核空间出到网卡硬件设备之前做处理\n数据包如果要离开本机，或者路由后，还有个 POSTROUTING 链负责\n四表 # 具有相同功能的规则的集合叫做 表，iptables 定义了 四种表。\n最常用的是 filter 表和 nat 表\nfilter 表 # 过滤数据包\nfilter 表中的规则可以被 INPUT, FORWARD 和 OUTPUT 三个链使用\nnat 表 # 网络地址转换\nnat 表中的规则可以被 PREROUTING, INPUT, OUTPUT 和 POSTROUTING 四个链使用\nmangle 表 # TODO\nraw 表 # TODO\n参考 # iptables详解（1）：iptables概念 "},{"id":21,"href":"/posts/k8s-informer/","title":"k8s informer 介绍","section":"Kubernetes","content":"informer 提供一个保持更新的 k8s 资源的本地缓存。\ninformer # "},{"id":22,"href":"/posts/flannel/","title":"深入了解 Kubernetes CNI 网络插件 Flannel","section":"Kubernetes","content":" 关于 # flannel 是由 CoreOS 开发的一个简单易用的容器网络插件\n网络是 k8s 中至关重要的一部分, 这里以简单的 flannel 为例做深入分析\n工作原理 # 以下介绍在 chart 方式部署的 flannel\nflanneld 进程以 daemonset/kube-flannel-ds 方式运行在所有 node 上, 负责从提前配置好的网络池中分配子网租约 (subnet lease) 给 node.\nflanneld 使用 k8s api 或者 etcd 存储网络配置、分配的子网和任何补充数据(如 node 的 public ip), 在 k8s 中使用一般不会单独提供 etcd 去存储这些数据.\n网络配置存储在 configmap 中, kube-flannel ns 下的 cm/kube-flannel-cfg 中 分配的子网存储在 PodCIDR 中 几个名词解释:\nsubnet: 对应 node.spec.podCIDR。 backend: 负责 node 之间 pod 通讯的后端。 flanneld 进程通过监听 node 资源来生成 subnet event, 然后在对应 backend 的 handleSubnetEvents 方法中处理逻辑，对于 vxlan backend 主要是按顺序设置 arp, fdb 和 route 来实现pod跨节点通讯。\nflannel 支持多种 pod 之间数据的转发后端(backend), 一旦设置 backend 就不应该在运行中更改, 推荐使用 VXLAN\nnode 子网信息从哪里来 # 我对于这个问题之前困惑了挺久，在 flannel 代码中也没找到相关代码。\n后来发现子网信息是从 node.Spec.podCIDR 中获取的。\n当 kube-controller-manager 设置了 allocate-node-cidrs 和 cluster-cidr 参数时，kube-controller-manager 会为每个 node 确定 podCIDR。flanneld 刚启动时，在 RegisterNetwork（调用 kubeSubnetManager.AcquireLease）中获取当前 node 的 spec.podCIDR，并把需要的一些信息写入到 node 的 annotation。再把子网信息写入到 /run/flannel/subnet.env，由 flannel CNI 读取，用于分配 pod ip。\n数据转发流程 # TODO\n推荐的 backend # VXLAN (Virtual eXtensible LAN) 虚拟可扩展局域网 # 关于 VXLAN 简单介绍:\nVXLAN 协议是一个隧道协议, 用来解决 VLAN ID 在 IEEE 802.1q 中限制只能有 4096(12bit) 个的问题. 在 VXLAN 中, VXLAN 标识符(VNI)的大小扩展至 16777216(24bit).\nVXLAN 由 IETF RFC7348 描述, 并且被很多厂商实现(如linux kernel vxlan module)了, 该协议使用单个目的端口(一般是4789)运行在 UDP 之上.\nVXLAN 采用 MAC in UDP 的封装方式.\nhost-gw # 使用host-gw创建通过远程机器IP到子网的IP路由, 需要在运行 flannel 的主机之间建立直接的 layer2 连接\nHost-gw提供了良好的性能, 很少依赖, 并且易于设置\nWireGuard # 使用内核内的WireGuard封装和加密报文.\nUDP # 如果你的网络和内核不支持使用VXLAN或host-gw, 则仅在调试时使用UDP.\n支持 Network Policy 的安装方式 # 从 v0.25.5 开始, 可以和 flannel 在同一个 pod 中一起部署 kube-network-policies 来提供 network policy controller.\n通过 chart 安装 flannel # 当前 chart 已经支持部署 kube-network-policies, 设置 netpol.enabled=true 即可\nmkdir -p ~/charts/flannel/flannel cd ~/charts/flannel/flannel helm repo add flannel https://flannel-io.github.io/flannel/ # values.yaml 用来查看默认值 helm show values flannel/flannel \u0026gt; values.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; custom-values.yaml podCidr: \u0026#34;10.42.0.0/16\u0026#34; flannel: image: repository: docker.io/flannel/flannel tag: v0.26.2 image_cni: repository: docker.io/flannel/flannel-cni-plugin tag: v1.6.0-flannel1 args: - \u0026#34;--ip-masq\u0026#34; - \u0026#34;--kube-subnet-mgr\u0026#34; backend: \u0026#34;vxlan\u0026#34; backendPort: 4789 mtu: 1450 vni: 4096 netpol: enabled: true args: - \u0026#34;--hostname-override=\\$(MY_NODE_NAME)\u0026#34; - \u0026#34;--v=2\u0026#34; image: repository: registry.k8s.io/networking/kube-network-policies tag: v0.4.0 EOF kubectl create ns kube-flannel kubectl label --overwrite ns kube-flannel pod-security.kubernetes.io/enforce=privileged helm upgrade --install --namespace kube-flannel flannel flannel/flannel -f custom-values.yaml TODO\n"},{"id":23,"href":"/posts/linux-bridge/","title":"Linux Bridge","section":"Network","content":"关于 Linux Bridge FDB\n"},{"id":24,"href":"/posts/kube-scheduler/","title":"Kube Scheduler","section":"Kubernetes","content":"RKE2 自定义调度器配置\n创建调度器配置文件 NodeResourcesFit 是一个调度插件, 检查节点是否拥有 Pod 请求的所有资源, 得分可以使用以下三种策略之一: LeastAllocated (默认)、MostAllocated 和 RequestedToCapacityRatio\n实现了多个扩展点: preFilter、filter、preScore、score\n我这里自定义使用 MostAllocated 策略, 优选分配比率较高的节点\n# /etc/rancher/rke2/kube-scheduler-config.yaml apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig profiles: - schedulerName: default-scheduler pluginConfig: - name: NodeResourcesFit args: scoringStrategy: type: MostAllocated resources: - name: cpu weight: 1 - name: memory weight: 1 修改 rke2 配置文件 修改 /etc/rancher/rke2/config.yaml\nkube-scheduler-arg: + - config=/etc/rancher/rke2/kube-scheduler-config.yaml 重启 rke2-server 会重新生成 kube-scheduler 的 static pod manifest 文件 /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml\n会挂载 /etc/rancher/rke2/kube-scheduler-config.yaml 文件到 pod 中\n"},{"id":25,"href":"/posts/cri/","title":"CRI 工作原理","section":"Kubernetes","content":" 关于 CRI # CRI 全称为 Container Runtime Interface (容器运行时接口), 是 kubelet 与 容器运行时进行通讯的主要协议。\n是 k8s 根据 OCI runtime-spec\ncri-api 主要定义了六个接口:\nstaging/src/k8s.io/cri-api/ ├── pkg │ ├── apis │ │ ├── runtime │ │ │ └── v1 │ │ │ ├── api.pb.go {RuntimeServiceClient RuntimeServiceServer ImageServiceClient ImageServiceServer} │ │ │ ├── api.proto │ │ │ └── constants.go │ │ ├── services.go {RuntimeService ImageManagerService} "},{"id":26,"href":"/posts/cni/","title":"CNI 工作原理","section":"Kubernetes","content":" 关于 CNI # CNI 全称 Container Network Interface, 容器网络接口, cni 插件是可执行文件, 一般位于 /opt/cni/bin/ 目录\n在 k8s 中, kubelet 调用 cri 创建 sandbox 时(RunPodSandbox)会先去创建 network namespace, 然后创建 pause 和 其他容器并将容器加入到同一个 network namespace 中\ncni spec 文档: https://www.cni.dev/docs/spec/\n有如下环境变量参数:\nCNI_COMMAND: 对应操作 ADD, DEL, CHECK, or VERSION. CNI_CONTAINERID: 容器 id CNI_NETNS: 如 /var/run/netns/[nsname] CNI_IFNAME: 要在容器中创建的接口名称, 一般容器中都是 eth0 CNI_ARGS: 额外的 kv 参数, 如 FOO=BAR;ABC=123 CNI_PATH: 搜索 cni plugin 可执行文件的目录 插件分析 # bridge # 主要是 cmdAdd 和 cmdDel 两个函数, 对应 CNI spec 中的 ADD 和 DEL 两个主要操作\ncmdAdd # TODO: 分析代码\nsetupBridge 确保机器上存储对应的 bridge setupVeth 在对应的 netns 下创建 veth 执行 ipam.ExecAdd(n.IPAM.Type, args.StdinData) 获取 ip 地址 执行 ipam.ConfigureIface(args.IfName, result) 将 ip 地址设知道对应的 veth 上 cni 测试工具 cnitool # https://www.cni.dev/docs/cnitool/\n创建 netns testing # $ ip netns add testing $ ip netns list testing 创建 bridge-static # cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/cni/net.d/999-bridge-static.conf { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.4.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;bridge-static\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;br0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;192.168.1.99\u0026#34; } ], \u0026#34;addresses\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;192.168.1.67/24\u0026#34; } ] } } EOF 将 bridge-static 添加至 testing netns # $ CNI_PATH=/opt/cni/bin cnitool add bridge-static /var/run/netns/testing { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.4.0\u0026#34;, \u0026#34;interfaces\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;br0\u0026#34;, \u0026#34;mac\u0026#34;: \u0026#34;b2:0c:ce:e1:37:1e\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;veth56b82c3a\u0026#34;, \u0026#34;mac\u0026#34;: \u0026#34;c6:44:a7:57:57:2d\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;mac\u0026#34;: \u0026#34;1e:d1:07:6b:a2:6a\u0026#34;, \u0026#34;sandbox\u0026#34;: \u0026#34;/var/run/netns/testing\u0026#34; } ], \u0026#34;ips\u0026#34;: [ { \u0026#34;version\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;interface\u0026#34;: 2, \u0026#34;address\u0026#34;: \u0026#34;192.168.1.67/24\u0026#34; } ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;192.168.1.99\u0026#34; } ], \u0026#34;dns\u0026#34;: {} } 检查 netns 里是否成功配置网络 # $ ip -n testing addr 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0@if18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 1e:d1:07:6b:a2:6a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.1.67/24 brd 192.168.1.255 scope global eth0 valid_lft forever preferred_lft forever $ ip -n testing route default via 192.168.1.99 dev eth0 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.67 $ ip netns exec testing ping -c 1 192.168.1.99 PING 192.168.1.99 (192.168.1.99) 56(84) bytes of data. 64 bytes from 192.168.1.99: icmp_seq=1 ttl=64 time=0.725 ms --- 192.168.1.99 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.725/0.725/0.725/0.000 ms "},{"id":27,"href":"/posts/csi/","title":"CSI 工作原理","section":"Kubernetes","content":" 关于 CSI # CSI 全称为 Container Storage Interface, 容器存储接口\n要实现一个第三方的 csi driver 需要实现下面的 gRPC service csi spec\n// 如果 NodeServer 和 ControllerServer 对应服务运行在不同 pod 中, 那么两个服务都要实现 IdentityServer type IdentityServer interface { // 用来获取插件名称 GetPluginInfo(context.Context, *GetPluginInfoRequest) (*GetPluginInfoResponse, error) GetPluginCapabilities(context.Context, *GetPluginCapabilitiesRequest) (*GetPluginCapabilitiesResponse, error) Probe(context.Context, *ProbeRequest) (*ProbeResponse, error) mustEmbedUnimplementedIdentityServer() } type ControllerServer interface { // 创建 volume, 如 ceph 创建一个 rbd 或者 hostpath 创建一个目录 CreateVolume(context.Context, *CreateVolumeRequest) (*CreateVolumeResponse, error) // 删除 volume, 如 ceph 删除一个 rbd 或者 hostpath 删除一个目录 DeleteVolume(context.Context, *DeleteVolumeRequest) (*DeleteVolumeResponse, error) // 将 volume attach 到 node 上, 如 rbd 通过 rbd map 命令 attach, 成功后 node 上会多出一个 rbdx 的 block 设备 ControllerPublishVolume(context.Context, *ControllerPublishVolumeRequest) (*ControllerPublishVolumeResponse, error) // 将 volume 从 node 上 detach, 如 rbd 通过 rbd unmap 命令 detach ControllerUnpublishVolume(context.Context, *ControllerUnpublishVolumeRequest) (*ControllerUnpublishVolumeResponse, error) ValidateVolumeCapabilities(context.Context, *ValidateVolumeCapabilitiesRequest) (*ValidateVolumeCapabilitiesResponse, error) // 列出所有 volume ListVolumes(context.Context, *ListVolumesRequest) (*ListVolumesResponse, error) GetCapacity(context.Context, *GetCapacityRequest) (*GetCapacityResponse, error) ControllerGetCapabilities(context.Context, *ControllerGetCapabilitiesRequest) (*ControllerGetCapabilitiesResponse, error) CreateSnapshot(context.Context, *CreateSnapshotRequest) (*CreateSnapshotResponse, error) DeleteSnapshot(context.Context, *DeleteSnapshotRequest) (*DeleteSnapshotResponse, error) ListSnapshots(context.Context, *ListSnapshotsRequest) (*ListSnapshotsResponse, error) ControllerExpandVolume(context.Context, *ControllerExpandVolumeRequest) (*ControllerExpandVolumeResponse, error) ControllerGetVolume(context.Context, *ControllerGetVolumeRequest) (*ControllerGetVolumeResponse, error) ControllerModifyVolume(context.Context, *ControllerModifyVolumeRequest) (*ControllerModifyVolumeResponse, error) mustEmbedUnimplementedControllerServer() } // 这些会被 kubelet 调用 type NodeServer interface { // format (如果没format), mount 到 node 的 global directory NodeStageVolume(context.Context, *NodeStageVolumeRequest) (*NodeStageVolumeResponse, error) // umount NodeUnstageVolume(context.Context, *NodeUnstageVolumeRequest) (*NodeUnstageVolumeResponse, error) // mount --bind 到 pod directory NodePublishVolume(context.Context, *NodePublishVolumeRequest) (*NodePublishVolumeResponse, error) // umount --bind NodeUnpublishVolume(context.Context, *NodeUnpublishVolumeRequest) (*NodeUnpublishVolumeResponse, error) NodeGetVolumeStats(context.Context, *NodeGetVolumeStatsRequest) (*NodeGetVolumeStatsResponse, error) NodeExpandVolume(context.Context, *NodeExpandVolumeRequest) (*NodeExpandVolumeResponse, error) NodeGetCapabilities(context.Context, *NodeGetCapabilitiesRequest) (*NodeGetCapabilitiesResponse, error) NodeGetInfo(context.Context, *NodeGetInfoRequest) (*NodeGetInfoResponse, error) mustEmbedUnimplementedNodeServer() } 关于 Sidecar Containers # Sidecar Containers 是一系列标准容器，用于简化 CSI 插件的开发和部署\n它们都有共同的逻辑，watch k8s API，调用第三方 csi driver 执行操作，最后对应的更新 k8s API\n这些容器一般作为sidecar和第三方 csi driver 一起部署在同一个 pod 中, 通过 unix socket 通信\n容器 仓库 文档 node-driver-registrar kubernetes-csi/node-driver-registrar link external-provisioner kubernetes-csi/external-provisioner link external-attacher kubernetes-csi/external-attacher link external-snapshotter kubernetes-csi/external-snapshotter link external-resizer kubernetes-csi/external-resizer link livenessprobe kubernetes-csi/livenessprobe link node-driver-registrar # 从 CSI endpoint 拉取 driver 信息(使用 NodeGetInfo), 然后通过 kubelet plugin registration mechanism 注册到对应节点的 kubelet 中\n表现形式为\n/var/lib/kubelet/plugins/csi-hostpath/csi.sock /var/lib/kubelet/plugins_registry/kubevirt.io.hostpath-provisioner-reg.sock external-provisioner # watch PersistentVolumeClaim 对象, 如果一个 pvc 引用了一个 StorageClass 并且 StorageClass 的 provisioner 字段 和从 CSI endpoint 调用 GetPluginInfo 获取到的一致，则执行下面逻辑\n创建 pvc 事件调用 CSI endpoint 执行 CreateVolume, 成功创建 volume 后就会创建代表这个 volume 的 PersistentVolume 对象 删除 pvc 事件调用 CSI endpoint 执行 DeleteVolume, 成功删除 volume 后也会删除代表这个 volume 的 PersistentVolume 对象 创建 pv 时会设置 pv.spec.claimRef 字段, 指向对应的 pvc, 随后 pvcontroller 会监听到 pv 的 claimRef 字段被设置然后将 pvc 和 pv 绑定(都变成bound状态).\n关于 sc.volumeBindingMode # 枚举类型, 有 WaitForFirstConsumer 和 Immediate 两种\nImmediate: pvc 创建后立即 provision 并且 bound, 这个是默认模式 WaitForFirstConsumer: 只有使用此 pvc 的 pod 被调度之后才会去 provision 并且 bound 调度 pod 后会在 pvc 上增加一个注解 volume.kubernetes.io/selected-node={scheduleResult.SuggestedHost} 通过 pvc 是否包含此注解并不为空来判断是否 provision WaitForFirstConsumer 一般适用于:\n本地盘, 防止卷和pod没创建在同一个节点上 不同 node 对应可用区不同, 需要知道被调度到的 node 对应可用区之后在对应可用区创建存储卷 external-attacher # watch VolumeAttachment 对象, 如果 attacher 字段和从 CSI endpoint 调用 GetPluginInfo 获取到的一致, 则触发调用 CSI endpoint 执行 Controller[Publish|Unpublish]Volume\n一般块存储才会需要 attach/detach 操作, 比如 ceph 的 rbd\nVolumeAttachment 对象是由 ADController(AttachDetach Controller) 创建, ADController 会不断的检查每一个 pod 对应的 pv 和这个 pod 所调度到的宿主机之间的挂载情况(node.status.volumesAttached), 针对没有挂载的 pv 创建的 VolumeAttachment 中存储以下三个信息\n关于 VolumeAttachment # // TODO\nVolumeAttachment 对象记录 pv 和 node 的挂载关系, 是由 ADController(AttachDetach Controller) 创建和删除\nattacher: csi driver 名称 nodeName: volume应该attach到的主机名称 source.persistentVolumeName: 要attach的pv的名称 external-snapshotter # TODO\nexternal-resizer # TODO\nlivenessprobe # TODO\ncsi demo # kubernetes-csi/csi-driver-host-path # kubevirt/hostpath-provisioner 是官方提供的 demo\nkubevirt/hostpath-provisioner # kubernetes-csi/csi-driver-host-path 是 kubevirt 基于 kubernetes-csi/csi-driver-host-path 开发的, 改动不多, 适合学习和使用\ncsi 测试工具 csc # csc 是 Container Storage Client\nidentity # identity service 相关的\nGetPluginInfo # $ csc -e /var/lib/kubelet/plugins/csi-hostpath/csi.sock identity plugin-info \u0026#34;kubevirt.io.hostpath-provisioner\u0026#34;\t\u0026#34;latest\u0026#34; node # node service 相关的\nNodeGetInfo # node-driver-registrar 向 kubelet 注册 CSI plugin 时会调用\n$ csc -e /var/lib/kubelet/plugins/csi-hostpath/csi.sock node get-info test\t0\t\u0026amp;csi.Topology{Segments:map[string]string{\u0026#34;topology.hostpath.csi/node\u0026#34;:\u0026#34;test\u0026#34;}, XXX_NoUnkeyedLiteral:struct {}{}, XXX_unrecognized:[]uint8(nil), XXX_sizecache:0} controller # controller service 相关的\nCreateVolume # external-provisioner 监听到有 pvc 创建时会调用\n$ csc -e /var/lib/kubelet/plugins/csi-hostpath/csi.sock controller create-volume --params storagePool=local --cap MULTI_NODE_MULTI_WRITER,mount,xfs,uid=500,gid=500 pvc-466a771a-a8c7-473e-bca6-780f7663a6cd \u0026#34;pvc-466a771a-a8c7-473e-bca6-780f7663a6cd\u0026#34;\t105226698752\t\u0026#34;storagePool\u0026#34;=\u0026#34;local\u0026#34; ListVolumes # 可以看到刚才创建的\n$ csc -e /var/lib/kubelet/plugins/csi-hostpath/csi.sock controller list-volumes \u0026#34;pvc-466a771a-a8c7-473e-bca6-780f7663a6cd\u0026#34;\t105226698752 NodePublishVolume # kubelet 针对 好像不会调用这个 ?\n$ csc -e /var/lib/kubelet/plugins/csi-hostpath/csi.sock node publish xxx DeleteVolume # external-provisioner 监听到有 pvc 被删除时会调用\n$ csc -e /var/lib/kubelet/plugins/csi-hostpath/csi.sock controller delete-volume pvc-466a771a-a8c7-473e-bca6-780f7663a6cd pvc-466a771a-a8c7-473e-bca6-780f7663a6cd 其他 # PV Controller 作用 # 负责协调 PV 和 PVC 状态, 负责根据规则绑定 PV 和 PVC\nAD Controller 作用 # AD Controller 全称 AttachDetach Controller, 主要负责\n创建和删除 VolumeAttachment 对象 更新 node.status.volumesAttached attachdetach controller 的 reconciler 中调用 csi attacher, 负责创建和删除 VolumeAttachment 对象并等待 attach/detach 成功, 最后更新 node.status.VolumesAttached\n在 attachdetach controller 的 reconciler 中\n// /pkg/controller/volume/attachdetach/reconciler.go func (rc *reconciler) reconcile(ctx context.Context) { for _, attachedVolume := range rc.actualStateOfWorld.GetAttachedVolumes() { // 会调用 Detach err = rc.attacherDetacher.DetachVolume(logger, attachedVolume.AttachedVolume, verifySafeToDetach, rc.actualStateOfWorld) } rc.attachDesiredVolumes(logger) // Update Node Status err := rc.nodeStatusUpdater.UpdateNodeStatuses(logger) } func (rc *reconciler) attachDesiredVolumes(logger klog.Logger) { for _, volumeToAttach := range rc.desiredStateOfWorld.GetVolumesToAttach() { // 会调用 Attach err := rc.attacherDetacher.AttachVolume(logger, volumeToAttach.VolumeToAttach, rc.actualStateOfWorld) } } 创建和删除 VolumeAttachment 对象, 等待 external-attacher 监听到后调用 CSI endpoint 执行实际的 attach/detach 操作\n// /pkg/volume/csi/csi_attacher.go func (c *csiAttacher) Attach(spec *volume.Spec, nodeName types.NodeName) (string, error) { // 创建 VolumeAttachment 对象 _, err = c.k8s.StorageV1().VolumeAttachments().Create(context.TODO(), attachment, metav1.CreateOptions{}) // Attach and detach functionality is exclusive to the CSI plugin that runs in the AttachDetachController, // and has access to a VolumeAttachment lister that can be polled for the current status. if err := c.waitForVolumeAttachmentWithLister(spec, pvSrc.VolumeHandle, attachID, c.watchTimeout); err != nil { return \u0026#34;\u0026#34;, err } return \u0026#34;\u0026#34;, nil } func (c *csiAttacher) Detach(volumeName string, nodeName types.NodeName) error { // 删除 VolumeAttachment 对象 if err := c.k8s.StorageV1().VolumeAttachments().Delete(context.TODO(), attachID, metav1.DeleteOptions{}); err != nil { } // Attach and detach functionality is exclusive to the CSI plugin that runs in the AttachDetachController, // and has access to a VolumeAttachment lister that can be polled for the current status. return c.waitForVolumeDetachmentWithLister(volID, attachID, c.watchTimeout) } kubelet VolumeManager 作用 # 对于持久卷来说, VolumeManager 负责使用 CSI client 调用 CSI plugin 对 volume 进行 mount/unmount 操作\nvolume manager 的 reconciler 会先确认该被 unmount 的 volume 被 unmount 掉, 然后确认该被 mount 的 volume 被 mount.\n根据 node.Status.VolumesAttached 中是否有对应 volume 来判断是否被 attach 成功\nVolumeAttachment 的创建、更新和删除 # pod 被调度后，AD Controller 会创建 VolumeAttachment 对象，external-attacher 监听到后会执行实际的 attach 操作，操作成功后会更新 node.Status.VolumesAttached。\npod 被删除后，如果确认该 volume 不再被该节点上的任何 pod 使用（通过检查 node.Status.VolumesInUse），AD Controller 会删除对应的 VolumeAttachment 对象，external-attacher 监听到后会执行实际的 detach 操作，操作成功后会从 node.Status.VolumesAttached 中移除该记录。\n参考 # How to write a Container Storage Interface (CSI) plugin "},{"id":28,"href":"/posts/builing-multi-platform-container-images-guide/","title":"构建多平台容器镜像","section":"Kubernetes","content":" 构建多平台容器镜像\nDocker buildx 插件/子命令 # buildx 是 Docker 的一个 CLI 插件，用于扩展来自于 Moby BuildKit 项目的构建功能。\n注意：buildx 需要 Docker 19.03 或更高版本。\nBuildKit # BuildKit是一个build引擎，它接收一个配置文件（Dockerfile），并转化成一个制品（容器镜像或其他制品）。相较与传统的build具有多阶段并发构建、更好的layer缓存支持等优点，Dockerfile中的RUN指令会被runc执行。\nDocker Engine 从 23.0.0 版本开始默认在Linux上使用Buildx和BuildKit为builder。\nBuilder: a BuildKit daemon # Builders介绍 一个 builder 是一个 BuildKit 守护进程，BuildKit是build引擎，它解决Dockerfile中的构建步骤，以生成容器镜像或其他制品。\nBuild drivers # Build 驱动有多种，例如 docker、docker-container、kubernetes、remote 等。\ndocker 使用捆绑在Docker守护进程中的BuildKit库。默认的Builder使用的该驱动。 docker-container 使用Docker创建一个专用的BuildKit容器。 kubernetes 在Kubernetes集群中创建BuildKit pods。 remote 直接连接到手动管理的BuildKit守护进程。 Build Drivers Comparison Feature docker docker-container kubernetes remote Automatically load image ✅ Cache export ✓* ✅ ✅ ✅ Tarball output ✅ ✅ ✅ Multi-arch images ✅ ✅ ✅ BuildKit configuration ✅ Managed externally * The docker driver doesn't support all cache export options 默认的 Builder 实例 # docker engine 会自动创建一个默认的 builder 实例，例如 default。默认的驱动是 docker，不支持多平台构建。\n# docker buildx ls NAME/NODE DRIVER/ENDPOINT STATUS BUILDKIT PLATFORMS default* docker \\_ default \\_ default running v0.16.0 linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386 创建 Builder 实例 # 下面创建一个 docker-container driver 的 builder 实例。\ndocker buildx create \\ --name container-builder \\ --driver docker-container \\ --platform linux/amd64,linux/arm64 \\ --bootstrap \\ --use 参数介绍:\n--name 指定实例名称 --driver 指定驱动，默认是 docker，docker不支持多平台构建，这里使用支持多平台构建的 docker-container --platform 指定支持的平台 --bootstrap 启动实例 --use 指定使用该实例 创建后会发现多了一个容器 buildx_buildkit_container-builder0, 使用的镜像是 moby/buildkit:buildx-stable-1。\n# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6be2b567eaa3 moby/buildkit:buildx-stable-1 \u0026#34;buildkitd --allow-i…\u0026#34; 31 minutes ago Up 31 minutes buildx_buildkit_container-builder0 可以指定builder实例容器使用的容器镜像，--driver-opt image=name\n可以指定buildkitd配置文件路径，--buildkitd-config /path/to/buildkitd.toml\nbuildkitd 可以配置镜像仓库的insecure等配置，参考buildkitd config，demo如下\n# buildkitd.toml [registry.\u0026#34;registry.example.com\u0026#34;] insecure = true http = true 使用 buildx 构建多平台镜像 # Buildx + Dockerfile 构建多平台镜像有三种方式\n在内核中使用QEMU仿真支持, 利用的内核的 binfmt_misc 机制提供运行多平台二进制文件的能力 docker run --privileged --rm tonistiigi/binfmt --install all 注册所有平台的二进制文件格式到内核中 上面的命令会将所有平台的二进制文件格式注册到内核中，例如 linux/arm64、linux/amd64 等, 可以在 /proc/sys/fs/binfmt_misc/qemu-* 看到注册的二进制文件格式和对应的解释器路径 使用相同的构建器实例在多个不同平台节点上构建 使用Dockerfile中的一个阶段来交叉编译到不同的架构，需要语言支持交叉编译(Go语言等)或者平台无关(Java语言和前端静态文件等) 下面是使用 BuildKit 构建多平台镜像时，Dockerfile 中可以使用的ARGs, 参考Automatic platform args in the global scope\nBUILDPLATFORM 是构建时平台的标识符，例如 linux/amd64 BUILDOS 是构建时操作系统的标识符，例如 linux BUILDARCH 是构建时架构的标识符，例如 amd64 TARGETPLATFORM 是目标平台的标识符，例如 linux/arm64 TARGETOS 是目标操作系统的标识符，例如 linux TARGETARCH 是目标架构的标识符，例如 arm64 使用第一种多平台镜像构建方式是最简单的（如果构建使用的节点支持），下面这个实例使用第三种构建多平台镜像方式，在 Dockerfile 中使用一个阶段来交叉编译到不同的架构。\n# Dockerfile # 第一个阶段, 使用构建时所在平台的镜像环境中交叉编译, 最终复制到目标平台对应的镜像中使用 FROM --platform=$BUILDPLATFORM golang:1.23.1-alpine AS builder ARG TARGETOS ARG TARGETARCH ENV GO111MODULE=on \\ CGO_ENABLED=0 WORKDIR /build RUN apk --no-cache add tzdata COPY . . # 交叉编译 # docker buildx build --platform linux/amd64,linux/arm64 ... 等同于在 linux/amd64 平台下执行 # 1. GOOS=linux GOARCH=amd64 go build -ldflags \u0026#34;-s -w\u0026#34; -o main # 2. GOOS=linux GOARCH=arm64 go build -ldflags \u0026#34;-s -w\u0026#34; -o main RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -ldflags \u0026#34;-s -w\u0026#34; -o main # 第二个阶段, 使用目标平台的镜像做为运行时镜像 # FROM 默认 --platform=$TARGETPLATFORM FROM scratch COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo COPY --from=builder /usr/share/zoneinfo/Asia/Shanghai /etc/localtime COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ COPY --from=builder /build/main / ENTRYPOINT [\u0026#34;/main\u0026#34;] 构建镜像 # docker buildx build --platform linux/amd64,linux/arm64 -t registry.example.com/my-image:latest . 推送镜像 # docker buildx build --platform linux/amd64,linux/arm64 -t registry.example.com/my-image:latest --push . 参考 # (docker docs) Multi-platform builds "},{"id":29,"href":"/posts/failed-to-reserve-sandbox-name/","title":"failed to reserve sandbox name","section":"问题记录","content":" 问题描述 # 当 k8s 集群时间发生回退时, 会出现 failed to reserve sandbox name 错误.\n... Aug 31 13:58:38 kind-control-plane kubelet[554]: E0831 13:58:38.610262 554 log.go:32] \u0026#34;RunPodSandbox from runtime service failed\u0026#34; err=\u0026#34;r pc error: code = Unknown desc = failed to reserve sandbox name \\\u0026#34;kube-scheduler-kind-control-plane_kube-system_3ead35239782468d5c21d9cb3933d bb2_0\\\u0026#34;: name \\\u0026#34;kube-scheduler-kind-control-plane_kube-system_3ead35239782468d5c21d9cb3933dbb2_0\\\u0026#34; is reserved for \\\u0026#34;e193fd73214c4db18ca31ed c6aa2591445dba26d77b6df9d8956a9228031689e\\\u0026#34;\u0026#34; ... 解决办法 # 将 \u0026lt;sandbox-id\u0026gt; 对应的容器删掉\ncrictl stopp \u0026lt;sandbox-id\u0026gt; crictl rmp \u0026lt;sandbox-id\u0026gt; 相关 issue # https://github.com/kubernetes/kubernetes/issues/126514 https://github.com/containerd/containerd/issues/9459 相关 PR # https://github.com/kubernetes/kubernetes/pull/130551 "},{"id":30,"href":"/posts/k8s-namespace/","title":"k8s namespaces","section":"Kubernetes","content":" 一个 namespace 将全局系统资源封装在一个抽象中，使得 namespace 内的进程看起来像是拥有该全局资源的独立实例。 对全局资源的更改只对属于同一 namespace 的其他进程可见。\nLinux 上可用的 namespace 有:\nnamespace 类型 隔离内容 cgroup Cgroup 根目录 ipc System V IPC, POSIX 消息队列 network 网络设备、协议栈、端口等 mount 挂载点 pid 进程 ID time 启动时间和单调时钟 user 用户和组 ID uts 主机名和 NIS 域名 "},{"id":31,"href":"/posts/k8s-cgroup/","title":"k8s cgroups","section":"Kubernetes","content":"目前主要用 cgroup v2, 下面记录 k8s 如何通过 cgroup v2 管理 cpu 和 memory 资源\nk8s 使用 cgroup 对容器进行资源管理 # kubelet 启动时会创建不同 QOS 级别的 root cgroup Guaranteed: /sys/fs/cgroup/kubepods.slice/ Burstable: /sys/fs/cgroup/kubepods.slice/kubepods-burstable.slice BestEffort: /sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice kubelet 通过 CRI 调用 container runtime 创建 sandbox 和 container 这里有个注意点：根据 pod QOS 类型不同, cgroup 创建的目录也不一样, 但是 container runtime 是不知道 kubelet 定义的 QOS 级别的, 所以 kubelet 通过 CRI 调用 container runtime 时会携带 CgroupParent 来指定对应 QOS 对应的 cgroup parent.\n最终 container runtime 调用 runc 创建 sandbox 和 container, 并且会设置对应的 cgroup 和设置 cpu.max 与 memory.max, 将 sandbox 和 container 启动进程的 PID 添加进 cgroup.procs\n进程添加进 cgroup.procs 后, cgroup 会将进程创建的子进程也自动添加到 cgroup.procs 中, 并且在进程结束后也会自动从 cgroup.procs 清除.\n查找进程所在 cgroup 根路径 # 进程 ID 可以通过 crictl ps 与 crictl inspect \u0026lt;container-id\u0026gt; | grep pid 获取\n如进程 ID 为 1227962\n# cat /proc/1227962/cgroup 0::/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podf4bb00c6_a390_4762_860a_643decfd755c.slice/cri-containerd-d2055ddb438f205ccf867146abbf35cc8e3814ebb50eb759e7bff5d5940fb378.scope 这里的 0:: 表示统一的 cgroup v2 层级, 查找 cgroup v2 挂载点:\n# mount | grep cgroup cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime) 所以进程所在 cgroup 根路径如下:\n/sys/fs/cgroup/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podf4bb00c6_a390_4762_860a_643decfd755c.slice/cri-containerd-d2055ddb438f205ccf867146abbf35cc8e3814ebb50eb759e7bff5d5940fb378.scope\n# cat /sys/fs/cgroup/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podf4bb00c6_a390_4762_860a_643decfd755c.slice/cri-containerd-d2055ddb438f205ccf867146abbf35cc8e3814ebb50eb759e7bff5d5940fb378.scope/cpu.max 200000 100000 # cat /sys/fs/cgroup/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podf4bb00c6_a390_4762_860a_643decfd755c.slice/cri-containerd-d2055ddb438f205ccf867146abbf35cc8e3814ebb50eb759e7bff5d5940fb378.scope/memory.max 4294967296 可以看出:\ncpu: 2核 memory: 4Gi "},{"id":32,"href":"/posts/libvirt/","title":"Libvirt 使用笔记","section":"libvirt","content":" virsh 命令 # virsh domifaddr 查看虚拟机网卡ip # # virsh domifaddr k3s-node01 --source lease Name MAC address Protocol Address ------------------------------------------------------------------------------- 通过arp获取网卡ip\n# virsh domifaddr k3s-node01 --source arp Name MAC address Protocol Address ------------------------------------------------------------------------------- vnet18 52:54:00:0e:08:02 ipv4 192.168.1.248/0 通过qemu guest agent获取网卡ip\n# virsh domifaddr k3s-node01 --source agent Name MAC address Protocol Address ------------------------------------------------------------------------------- lo 00:00:00:00:00:00 ipv4 127.0.0.1/8 enp1s0 52:54:00:0e:08:02 ipv4 192.168.1.248/24 flannel.1 92:61:59:0c:29:90 ipv4 10.42.0.0/32 cni0 42:fb:8d:3e:c1:a4 ipv4 10.42.0.1/24 vethde547696 da:18:a8:b2:ed:f0 N/A N/A vethe1841f6e ce:79:fc:e1:1e:0b N/A N/A veth464995dc 82:a9:3b:a6:b5:49 N/A N/A veth2370e2ac 4a:c8:32:5c:fb:34 N/A N/A "},{"id":33,"href":"/posts/rook-ceph/","title":"安装 Rook Ceph","section":"Ceph","content":"使用 RKE2 快速搭建 k8s 集群 创建的集群\n安装 rook ceph # 使用 helm charts 安装 rook ceph\nhttps://rook.io/docs/rook/latest-release/Helm-Charts/helm-charts/\n安装 ceph operator # 我这里禁用了 cephfs 和 nfs 相关功能\nmkdir -p ~/charts/rook-ceph/ceph-operator cd ~/charts/rook-ceph/ceph-operator helm repo add rook-release https://charts.rook.io/release # values.yaml 用来查看默认值 helm show values rook-release/rook-ceph \u0026gt; values.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; custom-values.yaml logLevel: DEBUG csi: enableCephfsDriver: false enableCephfsSnapshotter: false enableNFSSnapshotter: false EOF helm upgrade --install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph -f custom-values.yaml 安装 ceph cluster # 添加三个 node 上的三个盘作为 osd\nmkdir -p ~/charts/rook-ceph/ceph-cluster cd ~/charts/rook-ceph/ceph-cluster helm repo add rook-release https://charts.rook.io/release # values.yaml 用来查看默认值 helm show values rook-release/rook-ceph-cluster \u0026gt; values.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; custom-values.yaml toolbox: enabled: true cephClusterSpec: dashboard: # 禁用 SSL, 一般运行在 nginx 后面 ssl: false storage: useAllNodes: false useAllDevices: false nodes: - name: \u0026#34;k8s-node01\u0026#34; devices: - name: \u0026#34;/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-1\u0026#34; - name: \u0026#34;k8s-node02\u0026#34; devices: - name: \u0026#34;/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-1\u0026#34; - name: \u0026#34;k8s-node03\u0026#34; devices: - name: \u0026#34;/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-1\u0026#34; ingress: dashboard: host: name: ceph.lan ingressClassName: nginx cephFileSystems: [] cephBlockPoolsVolumeSnapshotClass: enabled: true EOF helm upgrade --install --create-namespace --namespace rook-ceph rook-ceph-cluster rook-release/rook-ceph-cluster -f custom-values.yaml 查看集群状态 # 进入 toolbox 容器\ntoolbox=$(kubectl -n rook-ceph get pods -l app=rook-ceph-tools -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl -n rook-ceph exec -it $toolbox -- bash 执行 ceph status 查看集群状态, 看到 HEALTH_OK 表示集群健康\n# 查看集群状态 $ ceph -s cluster: id: 2f7e89df-f919-4eab-9fb2-82273c8da466 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c (age 13m) mgr: a(active, since 12m), standbys: b osd: 3 osds: 3 up (since 11m), 3 in (since 12m) rgw: 1 daemon active (1 hosts, 1 zones) data: pools: 10 pools, 121 pgs objects: 248 objects, 586 KiB usage: 217 MiB used, 1.5 TiB / 1.5 TiB avail pgs: 121 active+clean io: client: 85 B/s rd, 170 B/s wr, 0 op/s rd, 0 op/s wr 其他常用 ceph 命令\n# 查看 OSD 状态 ceph osd status ceph osd df ceph osd utilization ceph osd pool stats ceph osd tree # 查看 Ceph 容量 ceph df # 查看 Rados 状态 rados df # 查看 PG 状态 ceph pg stat 访问 ceph dashboard # 登陆地址获取, 使用 https 访问, 可以通过 nodeport 访问或者直接和集群容器网络打通, 我这里直接在局域网网关上配了静态路由(类似 host-gw 模式), pod 网络被转发到第一个 node 节点上\nkubectl -n rook-ceph get svc rook-ceph-mgr-dashboard admin 用户名密码获取\nkubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 连接 s3 服务 # 获取地址访问\nkubectl -n rook-ceph get svc rook-ceph-rgw-ceph-objectstore 获取 ak/sk\ntoolbox=$(kubectl -n rook-ceph get pods -l app=rook-ceph-tools -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) ak=$(kubectl -n rook-ceph exec -it $toolbox -- bash -c \u0026#34;radosgw-admin user info --uid rgw-admin-ops-user | jq -r \u0026#39;.keys[0].access_key\u0026#39;\u0026#34;) sk=$(kubectl -n rook-ceph exec -it $toolbox -- bash -c \u0026#34;radosgw-admin user info --uid rgw-admin-ops-user | jq -r \u0026#39;.keys[0].secret_key\u0026#39;\u0026#34;) echo $ak echo $sk 其他问题 # 时间同步问题 # clock skew detected on mon.b, mon.c\n如果 k8s node 时间不同步, 会导致 ceph 集群状态异常, 可以通过以下命令同步时间\n$ ceph -s cluster: id: 2f7e89df-f919-4eab-9fb2-82273c8da466 health: HEALTH_WARN 2 mgr modules have recently crashed 所有节点设置时间同步\n安装 chrony: apt install chrony -y 编辑 /etc/chrony/chrony.conf, 添加内容 pool 192.168.1.99 iburst, 这是我局域网的 ntp server 重启 chrony: systemctl restart chrony 最近有 crash 问题 # 2 mgr modules have recently crashed\n$ ceph -s cluster: id: 2f7e89df-f919-4eab-9fb2-82273c8da466 health: HEALTH_WARN 2 mgr modules have recently crashed 查看并清理 crash 信息\n# 查看 crash 列表 $ ceph crash ls ID ENTITY NEW 2024-11-24T06:08:46.350971Z_23380e67-87a5-492e-bf5c-fd10fd90eb8c mgr.a * 2024-11-24T06:45:19.829015Z_66d90350-1476-4a52-9ffc-2a6248884f1d mgr.a * # 查看 crash 详情 $ ceph crash info 2024-11-24T06:08:46.350971Z_23380e67-87a5-492e-bf5c-fd10fd90eb8c { \u0026#34;backtrace\u0026#34;: [ \u0026#34; File \\\u0026#34;/usr/share/ceph/mgr/nfs/module.py\\\u0026#34;, line 189, in cluster_ls\\n return available_clusters(self)\u0026#34;, \u0026#34; File \\\u0026#34;/usr/share/ceph/mgr/nfs/utils.py\\\u0026#34;, line 70, in available_clusters\\n completion = mgr.describe_service(service_type=\u0026#39;nfs\u0026#39;)\u0026#34;, \u0026#34; File \\\u0026#34;/usr/share/ceph/mgr/orchestrator/_interface.py\\\u0026#34;, line 1664, in inner\\n completion = self._oremote(method_name, args, kwargs)\u0026#34;, \u0026#34; File \\\u0026#34;/usr/share/ceph/mgr/orchestrator/_interface.py\\\u0026#34;, line 1731, in _oremote\\n raise NoOrchestrator()\u0026#34;, \u0026#34;orchestrator._interface.NoOrchestrator: No orchestrator configured (try `ceph orch set backend`)\u0026#34; ], \u0026#34;ceph_version\u0026#34;: \u0026#34;18.2.4\u0026#34;, \u0026#34;crash_id\u0026#34;: \u0026#34;2024-11-24T06:08:46.350971Z_23380e67-87a5-492e-bf5c-fd10fd90eb8c\u0026#34;, \u0026#34;entity_name\u0026#34;: \u0026#34;mgr.a\u0026#34;, \u0026#34;mgr_module\u0026#34;: \u0026#34;nfs\u0026#34;, \u0026#34;mgr_module_caller\u0026#34;: \u0026#34;ActivePyModule::dispatch_remote cluster_ls\u0026#34;, \u0026#34;mgr_python_exception\u0026#34;: \u0026#34;NoOrchestrator\u0026#34;, \u0026#34;os_id\u0026#34;: \u0026#34;centos\u0026#34;, \u0026#34;os_name\u0026#34;: \u0026#34;CentOS Stream\u0026#34;, \u0026#34;os_version\u0026#34;: \u0026#34;9\u0026#34;, \u0026#34;os_version_id\u0026#34;: \u0026#34;9\u0026#34;, \u0026#34;process_name\u0026#34;: \u0026#34;ceph-mgr\u0026#34;, \u0026#34;stack_sig\u0026#34;: \u0026#34;922e03f28672a048b4c876242e1e5b1c28a51719b3a09938b8f19b8435ffacbb\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-11-24T06:08:46.350971Z\u0026#34;, \u0026#34;utsname_hostname\u0026#34;: \u0026#34;rook-ceph-mgr-a-d959864d7-4cckg\u0026#34;, \u0026#34;utsname_machine\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;utsname_release\u0026#34;: \u0026#34;5.15.0-125-generic\u0026#34;, \u0026#34;utsname_sysname\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;utsname_version\u0026#34;: \u0026#34;#135-Ubuntu SMP Fri Sep 27 13:53:58 UTC 2024\u0026#34; } # 清理一天之前的 crash 信息 $ ceph crash prune 1 如果重装 rook ceph 集群 # https://rook.io/docs/rook/latest/Storage-Configuration/ceph-teardown/#delete-the-data-on-hosts\n需要清理 /var/lib/rook 目录并擦除osd盘的文件系统, 如下 sdb 是 osd 盘\nrm -rf /var/lib/rook wipefs /dev/sdb -a "},{"id":34,"href":"/posts/rke2/","title":"RKE2 安装 k8s 集群","section":"Kubernetes","content":"根据创建 bridge 网络和创建虚拟机时使用 cloudinit 初始化创建虚拟机, 并配置静态ip如下\n主机名 配置 ip (域名) 系统盘 / 数据盘 k8s-node01 8核16G 192.168.1.218 (lb.k8s.lan) 50GB / 100GB*1 k8s-node02 8核16G 192.168.1.219 50GB / 100GB*1 k8s-node03 8核16G 192.168.1.220 50GB / 100GB*1 安装 RKE2 # 安装第一个 server 节点 # 在 k8s-node01 节点执行\n# 初始化 rke2 配置文件 mkdir -p /etc/rancher/rke2 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/rancher/rke2/config.yaml tls-san: - lb.k8s.lan write-kubeconfig-mode: \u0026#34;0600\u0026#34; disable-cloud-controller: true # cni 单独部署, 如无特殊需求, 这里也可以直接指定 flannel 或 calico cni: none debug: true # 指定 kube-scheduler 自定义参数, 会自动覆盖到 /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml kube-scheduler-arg: - v=4 - bind-address=0.0.0.0 kube-controller-manager-arg: - bind-address=0.0.0.0 etcd-expose-metrics: true EOF curl -sfL https://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh - systemctl enable rke2-server.service systemctl start rke2-server.service 配置介绍 # tls-san # tls-san 在 server 的 TLS 证书中增加了多个地址作为 Subject Alternative Name, 这样就可以通过 lb.k8s.lan 和 各个 server 节点 ip 访问 apiserver 服务.\netcd-expose-metrics # 默认为 false, rke2 会使用 k3s 代码 pkg/etcd/etcd.go 中的 func (e *ETCD) cluster(ctx context.Context, reset bool, options executor.InitialOptions) error 生成 /var/lib/rancher/rke2/server/db/etcd/config 文件存储 etcd 启动需要的参数, 其中就包含 listen-metrics-urls: http://127.0.0.1:2381,http://192.168.1.218:2381, 如果只监听了 loopback 地址, 那么 prometheus 抓不到对应的 metrics 数据, 如下是代码部分\n// cluster calls the executor to start etcd running with the provided configuration. func (e *ETCD) cluster(ctx context.Context, reset bool, options executor.InitialOptions) error { ctx, e.cancel = context.WithCancel(ctx) return executor.ETCD(ctx, executor.ETCDConfig{ Name: e.name, InitialOptions: options, ForceNewCluster: reset, ListenClientURLs: e.listenClientURLs(reset), ListenMetricsURLs: e.listenMetricsURLs(reset), // 这里指定 metrics 监听的端口 ListenPeerURLs: e.listenPeerURLs(reset), AdvertiseClientURLs: e.advertiseClientURLs(reset), DataDir: dbDir(e.config), ServerTrust: executor.ServerTrust{ CertFile: e.config.Runtime.ServerETCDCert, KeyFile: e.config.Runtime.ServerETCDKey, ClientCertAuth: true, TrustedCAFile: e.config.Runtime.ETCDServerCA, }, PeerTrust: executor.PeerTrust{ CertFile: e.config.Runtime.PeerServerClientETCDCert, KeyFile: e.config.Runtime.PeerServerClientETCDKey, ClientCertAuth: true, TrustedCAFile: e.config.Runtime.ETCDPeerCA, }, SnapshotCount: 10000, ElectionTimeout: 5000, HeartbeatInterval: 500, Logger: \u0026#34;zap\u0026#34;, LogOutputs: []string{\u0026#34;stderr\u0026#34;}, ExperimentalInitialCorruptCheck: true, ListenClientHTTPURLs: e.listenClientHTTPURLs(), }, e.config.ExtraEtcdArgs) } // listenMetricsURLs returns a list of URLs to bind to for metrics connections. func (e *ETCD) listenMetricsURLs(reset bool) string { metricsURLs := fmt.Sprintf(\u0026#34;http://%s:2381\u0026#34;, e.config.Loopback(true)) if !reset \u0026amp;\u0026amp; e.config.EtcdExposeMetrics { // 如果设置为 true 则增加监听主机 host 地址 metricsURLs += \u0026#34;,\u0026#34; + fmt.Sprintf(\u0026#34;http://%s\u0026#34;, net.JoinHostPort(e.address, \u0026#34;2381\u0026#34;)) } return metricsURLs } 生成 etcd 配置文件之后, etcd 的 static pod manifest 中的启动命令就是 etcd --config-file=/var/lib/rancher/rke2/server/db/etcd/config, 配置文件通过 hostPath 方式挂载.\n安装 cni # 查看 《深入了解 Kubernetes CNI 网络插件 Flannel》 安装 flannel cni\n安装其他 server 节点 # 初始化 rke2 配置文件, 需要修改 /etc/rancher/rke2/config.yaml 中的 token\n# 从第一个 server 节点的 /var/lib/rancher/rke2/server/node-token 获取 token=\u0026lt;edit-me\u0026gt; mkdir -p /etc/rancher/rke2 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/rancher/rke2/config.yaml server: https://lb.k8s.lan:9345 token: $token tls-san: - lb.k8s.lan write-kubeconfig-mode: \u0026#34;0600\u0026#34; disable-cloud-controller: true # cni 单独部署, 如无特殊需求, 这里也可以直接指定 flannel 或 calico cni: none debug: true # 指定 kube-scheduler 自定义参数, 会自动覆盖到 /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml kube-scheduler-arg: - v=4 - bind-address=0.0.0.0 etcd-expose-metrics: true EOF 安装\ncurl -sfL https://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh - systemctl enable rke2-server.service systemctl start rke2-server.service 配置节点 # server 和 worker 节点都需要执行\n# kubectl ctr crictl... CONFIG=\u0026#34;PATH=\\$PATH:/var/lib/rancher/rke2/bin/\u0026#34; grep \u0026#34;$CONFIG\u0026#34; ~/.bashrc || echo \u0026#34;$CONFIG\u0026#34; \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc # command auto completiom CONFIG=\u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; grep \u0026#34;$CONFIG\u0026#34; ~/.bashrc || echo \u0026#34;$CONFIG\u0026#34; \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc # KUBECONFIG ENV CONFIG=\u0026#34;export KUBECONFIG=/etc/rancher/rke2/rke2.yaml\u0026#34; grep \u0026#34;$CONFIG\u0026#34; ~/.bashrc || echo \u0026#34;$CONFIG\u0026#34; \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc # CRI_CONFIG_FILE CONFIG=\u0026#34;export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml\u0026#34; grep \u0026#34;$CONFIG\u0026#34; ~/.bashrc || echo \u0026#34;$CONFIG\u0026#34; \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc # alias ctr=\u0026#34;ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io\u0026#34; CONFIG=\u0026#34;alias ctr=\\\u0026#34;ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io\\\u0026#34;\u0026#34; grep \u0026#34;$CONFIG\u0026#34; ~/.bashrc || echo \u0026#34;$CONFIG\u0026#34; \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc # install helm HELM_LATEST_VERSION=v3.15.2 wget https://get.helm.sh/helm-${HELM_LATEST_VERSION}-linux-amd64.tar.gz tar -zxvf helm-${HELM_LATEST_VERSION}-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm rm -f helm-${HELM_LATEST_VERSION}-linux-amd64.tar.gz \u0026amp;\u0026amp; rm -rf linux-amd64/ worker 节点的 kubeconfig /etc/rancher/rke2/rke2.yaml 需要从 server 节点上拷贝, 无需修改\nRKE2架构 # RKE2 Server 和 Agent 有利用 k3s 的 agent\n进程生命周期 # rke2进程使用systemd守护运行, rke2生成containerd进程和kubelet进程, 然后apiserver controller-manager scheduler etcd kube-proxy以static pod的形式被kubelet启动\ncontainerd进程退出时rke2也会重启, kubelet进程退出时rke2会再拉起一个kubelet进程\n# ps -e --forest 899 ? 01:32:51 rke2 1101 ? 01:58:12 \\_ containerd 1123 ? 05:23:44 \\_ kubelet 1227 ? 00:02:15 containerd-shim 1344 ? 00:00:00 \\_ pause 1500 ? 05:12:21 \\_ etcd 1228 ? 00:02:22 containerd-shim 1353 ? 00:00:00 \\_ pause 2516 ? 06:26:44 \\_ kube-controller 1229 ? 00:02:16 containerd-shim 1342 ? 00:00:00 \\_ pause 2614 ? 00:44:00 \\_ cloud-controlle 1267 ? 00:02:18 containerd-shim 1363 ? 00:00:00 \\_ pause 1452 ? 00:08:46 \\_ kube-proxy 1920 ? 00:00:00 \\_ timeout \u0026lt;defunct\u0026gt; 1283 ? 00:02:19 containerd-shim 1341 ? 00:00:00 \\_ pause 1541 ? 00:51:47 \\_ kube-scheduler 1801 ? 00:20:15 containerd-shim 1821 ? 00:00:00 \\_ pause 1852 ? 15:16:04 \\_ kube-apiserver 一些常用目录/文件 # 目录/文件 说明 /etc/rancher/rke2/config.yaml rke2配置文件 /var/lib/rancher/rke2/agent/pod-manifests static pod 文件, rke2 启动时根据配置文件自动生成 /var/lib/rancher/rke2/agent/etc/containerd/config.toml containerd配置文件 /var/lib/rancher/rke2/agent/containerd/containerd.log containerd日志 /var/lib/rancher/rke2/agent/logs/kubelet.log kubelet日志 /var/lib/rancher/rke2/server/db/etcd/config etcd配置文件 /var/lib/rancher/rke2/server/manifests 生成的 coredns 等 helm chart 文件 连接 etcd # ETCD_CONTAINER_ID=$(crictl ps --label=io.kubernetes.container.name=etcd --quiet) ETCD_CA_CERT=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt ETCD_CLIENT_CERT=/var/lib/rancher/rke2/server/tls/etcd/server-client.crt ETCD_CLIENT_KEY=/var/lib/rancher/rke2/server/tls/etcd/server-client.key 查看 etcd 集群状态 # $ crictl exec -it $ETCD_CONTAINER_ID etcdctl --cacert $ETCD_CA_CERT --cert $ETCD_CLIENT_CERT --key $ETCD_CLIENT_KEY endpoint status --cluster --write-out=table +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.1.219:2379 | a6bc98228859ce05 | 3.5.13 | 5.5 MB | false | false | 3 | 14026 | 14026 | | | https://192.168.1.220:2379 | b3d0ba8f8abb8a75 | 3.5.13 | 5.4 MB | false | false | 3 | 14026 | 14026 | | | https://192.168.1.218:2379 | d61af8cc4ec4d5b1 | 3.5.13 | 8.5 MB | true | false | 3 | 14026 | 14026 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 参考 # [RKE2 docs] quickstart [RKE2 docs] CLI 工具 "},{"id":35,"href":"/posts/create-vm-with-cloudinit/","title":"创建虚拟机时使用 cloudinit 初始化","section":"libvirt","content":" cloudinit 介绍 # 用于在新建的虚拟机中进行时间设置、密码设置、扩展根文件系统所在分区、设置主机名、运行脚本、安装软件包等初始化设置\n宿主机配置脚本 # 此脚本会用来在 /var/lib/libvirt/disks/${VM}/cloudinit 目录生成 cloudinit iso 镜像\ncat \u0026lt;\u0026lt;EOFALL \u0026gt; /usr/bin/gen-cloudinit-iso #!/bin/bash set -eux CLOUD_INIT_DIR=\u0026#34;/var/lib/libvirt/disks/\\${VM}/cloudinit\u0026#34; FILENAME=\u0026#34;\\${CLOUD_INIT_DIR}/init.iso\u0026#34; mkdir -p \\${CLOUD_INIT_DIR} cat \u0026lt;\u0026lt;EOF \u0026gt; \\${CLOUD_INIT_DIR}/meta-data instance-id: \\${VM} local-hostname: \\${VM} EOF # 更多配置参照 https://cloudinit.readthedocs.io/en/latest/explanation/format.html cat \u0026lt;\u0026lt;EOF \u0026gt; \\${CLOUD_INIT_DIR}/user-data #cloud-config EOF # 参考 kubevirt /pkg/cloud-init/cloud-init.go:defaultIsoFunc xorrisofs -output \\$FILENAME -volid cidata -joliet -rock -partition_cyl_align on \\${CLOUD_INIT_DIR}/user-data \\${CLOUD_INIT_DIR}/meta-data EOFALL chmod +x /usr/bin/gen-cloudinit-iso cloud-init user-data 中指定账户密码(可选) # #cloud-config ssh_pwauth: True chpasswd: list: | root:password expire: False cloud-init user-data 中禁用网络配置 # #cloud-config network: config: disabled 创建虚拟机时使用 cloudinit # 创建虚拟机之前创建 cloudinit iso, 并通过 cdrom 挂载\nfor vm in \u0026#34;k8s-node01\u0026#34; \u0026#34;k8s-node02\u0026#34; \u0026#34;k8s-node03\u0026#34;; do export VM=${vm} # 生成 cloudinit iso gen-cloudinit-iso # prepare sysdisk and datadisk qemu-img create -f qcow2 -F qcow2 -b /var/lib/libvirt/images/ubuntu.qcow2 /var/lib/libvirt/disks/${VM}/sysdisk.qcow2 50G qemu-img create -f qcow2 /var/lib/libvirt/disks/${VM}/datadisk01.qcow2 100G virt-install \\ --name ${VM} \\ --memory 16384 \\ --vcpus 8 \\ --disk /var/lib/libvirt/disks/${VM}/sysdisk.qcow2,device=disk,bus=scsi \\ --disk /var/lib/libvirt/disks/${VM}/datadisk01.qcow2,device=disk,bus=scsi \\ --disk /var/lib/libvirt/disks/${VM}/cloudinit/init.iso,device=cdrom,bus=scsi \\ --network bridge=br0 \\ --import \\ --os-variant ubuntu22.10 \\ --noautoconsole done 参考 # https://cloudinit.readthedocs.io/en/latest/explanation/format.html https://github.com/kubevirt/kubevirt/blob/main/pkg/cloud-init/cloud-init.go "},{"id":36,"href":"/posts/controller-runtime/","title":"Controller Runtime","section":"Kubernetes","content":" controller-runtime是在client-go/tools/cache和client-go/util/workqueue的基础上实现的, 了解client-go/tools/cache和client-go/util/workqueue对理解controller-runtime很有帮助\ninformer 的工作机制是什么 # Reflector 从 kube-apiserver 中获取资源对象, 更新到 DeltaFIFO 中 Informer 从 DeltaFIFO 中获取资源对象, 更新到 local cache 中, 然后执行注册的 EventHandler 自定义控制器会注册 AddFunc, UpdateFunc, DeleteFunc 等事件处理器, 这些事件会添加对象到 WorkQueue 中, 然后从 WorkQueue 中获取对象, 触发 reconcile\n同一个 crd object 会不会同时被 reconcile # 这个全靠Queue数据结构设计的精妙, 保证了正在执行的reconcile不会处理相同的object\n向queue中增加object之前会检查是否有次object存在于queue中，如果不存在则加入dirty set，如果也不存在于processing set才会加入queue中，当processing中的处理完成之后（调用Done），会将object从processing set种移除，如果次object在处理过程中加入到了dirty set，则将object再次加入到queue中 https://www.cnblogs.com/daniel-hutao/p/18010835/k8s_clientgo_workqueue\n有几种队列，Queue，DelayingQueue，RateLimitingQueue\nreconcile 时会读到旧数据吗，如何解决 # 因为读写分离，更新是直接更新 kube-apiserver，读是从 indexer(local cache) 中，所以读到的有可能是陈旧的数据。\nMy cache might be stale if I read from a cache! How should I deal with that?\n在更新或patch status之后，通过wait.Pool(100ms, 2s, func()(bool, error))校验cache中的本object数据直至更新\nhttps://github.com/kubernetes-sigs/controller-runtime/blob/main/FAQ.md#q-my-cache-might-be-stale-if-i-read-from-a-cache-how-should-i-deal-with-that\nhttps://github.com/kubernetes/test-infra/blob/8f0f19a905a20ed6f76386e5e11343d4bc2446a7/prow/plank/reconciler.go#L516-L520\n"},{"id":37,"href":"/posts/kubevirt-sidecar/","title":"Kubevirt Hook Sidecar","section":"KubeVirt","content":" 简介 # 背景 # 在kubevirt中, 通过vmi的spec没办法涵盖所有的libvirt domain xml元素, 所以有了hook sidecar功能来允许我们在define domain之前自定义domainSpecXML\n功能介绍 # 在kubevirt中, Hook Sidecar容器是sidecar container(和main application container跑在同一个pod中)用来在vm初始化完成前执行一些自定义操作.\nsidecar container与main container(compute)通过gRPC通讯, 有两种主要的sidecar hooks\nOnDefineDomain: 这个hook帮助自定义libvirt的XML, 并通过gRPC协议返回最新的XML以创建vm PreCloudInitIso: 这个hook帮助定义cloud-init配置, 它运行并返回最新的cloud-init data Shutdown: 这个是v1alpha3版本才支持的 使用hook sidecar功能需要在kv.spec.configuration.developerConfiguration.featureGates中开启Sidecar功能\n源码分析 # kubevirt-boot-sidecar 介绍 # 以下以kubevirt-boot-sidecar为例讲述sidecar的工作流程, 这个sidecar支持修改引导设备顺序(boot)和开启交互式引导菜单(bootmenu)\nkubevirt-boot-sidecar只实现了OnDefineDomain, 下面也是主要串一下OnDefineDomain相关的\nsidecar工作流程 # virt-launcher刚启动时收集所有sidecar信息 // cmd/virt-launcher/virt-launcher.go func main() { hookSidecars := pflag.Uint(\u0026#34;hook-sidecars\u0026#34;, 0, \u0026#34;Number of requested hook sidecars, virt-launcher will wait for all of them to become available\u0026#34;) // 收集所有sidecar的信息 err := hookManager.Collect(*hookSidecars, *qemuTimeout) // 启动 cmd server, 这里面有 SyncVirtualMachine 方法, 具体的实现在 func (l *LibvirtDomainManager) SyncVMI // virt-handler在初始化完虚拟机硬盘等之后会通过 SyncVirtualMachine 调用SyncVMI函数开始创建domain // SyncVMI将vmi spec转换为domainSpec, 然后调用hooksManager.OnDefineDomain执行所有的sidecar的OnDefineDomain方法 // 最终用OnDefineDomain编辑后的domainSpec创建domain cmdServerDone := startCmdServer(cmdclient.UninitializedSocketOnGuest(), domainManager, stopChan, options) } // pkg/hooks/manager.go // numberOfRequestedHookSidecars为vmi注解 hooks.kubevirt.io/hookSidecars 的数组长度, 在virt-controller生成pod manifest的逻辑中计算得出 func (m *hookManager) Collect(numberOfRequestedHookSidecars uint, timeout time.Duration) error { // callbacksPerHookPoint callbacksPerHookPoint, err := m.collectSideCarSockets(numberOfRequestedHookSidecars, timeout) m.CallbacksPerHookPoint = callbacksPerHookPoint } // pkg/hooks/manager.go func (m *hookManager) collectSideCarSockets(numberOfRequestedHookSidecars uint, timeout time.Duration) (map[string][]*callBackClient, error) { callbacksPerHookPoint := make(map[string][]*callBackClient) processedSockets := make(map[string]bool) timeoutCh := time.After(timeout) for uint(len(processedSockets)) \u0026lt; numberOfRequestedHookSidecars { sockets, err := os.ReadDir(m.hookSocketSharedDirectory) // 遍历 /var/run/kubevirt-hooks/ 目录下的 unix socket 文件 for _, socket := range sockets { select { case \u0026lt;-timeoutCh: return nil, fmt.Errorf(\u0026#34;Failed to collect all expected sidecar hook sockets within given timeout\u0026#34;) default: if _, processed := processedSockets[socket.Name()]; processed { continue } // 连接 sock 文件对应的 sidecar server 的 Info 函数获取 server 实现了哪些 hook(onDefineDomain或preCloudInitIso) callBackClient, notReady, err := processSideCarSocket(filepath.Join(m.hookSocketSharedDirectory, socket.Name())) if notReady { log.Log.Info(\u0026#34;Sidecar server might not be ready yet, retrying in the next iteration\u0026#34;) continue } else if err != nil { return nil, err } // callbacksPerHookPoint[onDefineDomain|preCloudInitIso][]*callBackClient{} // 聚合出 onDefineDomain:[\u0026#34;aaaa.sock\u0026#34;,\u0026#34;bbbb.sock\u0026#34;] for _, subscribedHookPoint := range callBackClient.subscribedHookPoints { callbacksPerHookPoint[subscribedHookPoint.GetName()] = append(callbacksPerHookPoint[subscribedHookPoint.GetName()], callBackClient) } processedSockets[socket.Name()] = true } } time.Sleep(time.Second) } // {\u0026#34;onDefineDomain\u0026#34;:[{\u0026#34;SocketPath\u0026#34;:\u0026#34;/var/run/kubevirt-hooks/shim-xxxx.sock\u0026#34;, \u0026#34;Version\u0026#34;:\u0026#34;v1alpha3\u0026#34;, \u0026#34;subscribedHookPoints\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;onDefineDomain\u0026#34;, \u0026#34;priority\u0026#34;: 0}]}]} return callbacksPerHookPoint, nil } virt-launcher启动之后, virt-handler会执行一些本地盘等相关初始化配置后通过gRPC调用virt-launcher的SyncVirtualMachine方法开始创建domain SyncVMI Convert_v1_VirtualMachineInstance_To_api_Domain 将 vmi 转换为 domainSpec lookupOrCreateVirDomain 先LookupDomainByName, 如果已存在则直接退出 preStartHook hooksManager := hooks.GetManager() // 执行所有的 PreCloudInitIso sidecar cloudInitData, err = hooksManager.PreCloudInitIso(vmi, cloudInitData) setDomainSpecWithHooks // pkg/virt-launcher/virtwarp/util/libvirt-helper.go func SetDomainSpecStrWithHooks(virConn cli.Connection, vmi *v1.VirtualMachineInstance, wantedSpec *api.DomainSpec) (cli.VirDomain, error) { hooksManager := getHookManager() // 执行所有的 OnDefineDomain sidecar domainSpec, err := hooksManager.OnDefineDomain(wantedSpec, vmi) // 调用 virConn.DomainDefineXML 创建 domain return SetDomainSpecStr(virConn, vmi, domainSpec) } // /pkg/hooks/manager.go func (m *hookManager) OnDefineDomain(domainSpec *virtwrapApi.DomainSpec, vmi *v1.VirtualMachineInstance) (string, error) { domainSpecXML, err := xml.MarshalIndent(domainSpec, \u0026#34;\u0026#34;, \u0026#34;\\t\u0026#34;) callbacks, found := m.CallbacksPerHookPoint[hooksInfo.OnDefineDomainHookPointName] if !found { return string(domainSpecXML), nil } vmiJSON, err := json.Marshal(vmi) for _, callback := range callbacks { // 执行所有的sidecar OnDefineDomain函数, 一次次编辑domainSpecXML domainSpecXML, err = m.onDefineDomainCallback(callback, domainSpecXML, vmiJSON) } return string(domainSpecXML), nil } // /pkg/hooks/manager.go func (m *hookManager) onDefineDomainCallback(callback *callBackClient, domainSpecXML, vmiJSON []byte) ([]byte, error) { // dial /var/run/kubevirt-hooks/shim-xxxx.sock conn, err := grpcutil.DialSocketWithTimeout(callback.SocketPath, 1) switch callback.Version { case hooksV1alpha3.Version: client := hooksV1alpha3.NewCallbacksClient(conn) // 调用sidecar server 的 OnDefineDomain 方法 result, err := client.OnDefineDomain(ctx, \u0026amp;hooksV1alpha3.OnDefineDomainParams{ DomainXML: domainSpecXML, Vmi: vmiJSON, }) domainSpecXML = result.GetDomainXML() } return domainSpecXML, nil } 会发现上面主要是sidecar client视角, 没有介绍sidecar server在哪实现的, 最新的解决方案是搭配sidecar-shim, 下面开始介绍\nsidecar-shim介绍 # 为了简化sidecar的开发, kubevirt提供了sidecar-shim镜像完成和主容器的通信, 我们只需要开发一个程序接收vmi和domain两个参数, 然后编译成名为onDefineDomain的可执行程序放到sidecar-shim镜像的/usr/bin/目录即可, sidecar-shim在执行时会执行我们开发的可执行程序.\n// /cmd/sidecars/sidecar_shim.go func runOnDefineDomain(vmiJSON []byte, domainXML []byte) ([]byte, error) { // 检查是否存在 onDefineDomainBin 可执行程序 if _, err := exec.LookPath(onDefineDomainBin); err != nil { return nil, fmt.Errorf(\u0026#34;Failed in finding %s in $PATH due %v\u0026#34;, onDefineDomainBin, err) } vmiSpec := virtv1.VirtualMachineInstance{} if err := json.Unmarshal(vmiJSON, \u0026amp;vmiSpec); err != nil { return nil, fmt.Errorf(\u0026#34;Failed to unmarshal given VMI spec: %s due %v\u0026#34;, vmiJSON, err) } args := append([]string{}, \u0026#34;--vmi\u0026#34;, string(vmiJSON), \u0026#34;--domain\u0026#34;, string(domainXML)) command := exec.Command(onDefineDomainBin, args...) // 只有将开发的可执行程序错误日志写入到stderr中才会在hook-sidecar-x容器日志中打印出来 // stdout只用来输出新的domainSpecXML, 如果程序exit code非0, 则不会打印stdout中的内容 if reader, err := command.StderrPipe(); err != nil { log.Log.Reason(err).Infof(\u0026#34;Could not pipe stderr\u0026#34;) } else { go logStderr(reader, \u0026#34;onDefineDomain\u0026#34;) } // command.Output()返回的error信息只有exit code return command.Output() } virt-launcher pod内所有容器共享 /var/run/kubevirt-hooks目录, sidecar-shim在/var/run/kubevirt-hooks目录下创建sock文件实现Info和OnDefineDomain方法然后监听gRPC远程调用, 然后主容器会连接/var/run/kubevirt-hooks目录下的sock文件调用函数\n使用 kubevirt-boot-sidecar # 只需在vmi的模版中增加两个注解\nhooks.kubevirt.io/hookSidecars会被virt-controller读取并生成virt-launcher 的 pod manifest时增加一个hook-sidecar-x的容器 os.vm.kubevirt.io/boot会被ghcr.io/go-bai/kubevirt-boot-sidecar镜像中的/usr/bin/onDefineDomain程序读取并用来设置在domainSpecXML中返回给virt-launcher, 最终用来define domain apiVersion: kubevirt.io/v1 kind: VirtualMachine spec: template: metadata: annotations: + hooks.kubevirt.io/hookSidecars: \u0026#39;[{\u0026#34;args\u0026#34;: [\u0026#34;--version\u0026#34;, \u0026#34;v1alpha3\u0026#34;],\u0026#34;image\u0026#34;: \u0026#34;ghcr.io/go-bai/kubevirt-boot-sidecar:v1.2.0\u0026#34;}]\u0026#39; + os.vm.kubevirt.io/boot: \u0026#39;{\u0026#34;boot\u0026#34;:[{\u0026#34;dev\u0026#34;:\u0026#34;hd\u0026#34;},{\u0026#34;dev\u0026#34;:\u0026#34;cdrom\u0026#34;}]}\u0026#39; 注意点 # OnDefineDomain可能会被调用超过一次, 所以要保证函数幂等 参考 # [offical user guide] hook-sidecar "},{"id":38,"href":"/posts/wireless-to-wired-network/","title":"无线转有线网络","section":"Network","content":" 通过无线网卡连接网络A(192.168.31.0/24), 无线网卡相当于WAN口，通过有线网卡接入网络B(192.168.1.0/24), 有线网卡相当于LAN口\n准备一个ubuntu虚拟机router # # 准备qcow2基础镜像 wget https://down.idc.wiki/Image/realServer-Template/current/qcow2/ubuntu22.qcow2 -O /var/lib/libvirt/images/ubuntu.qcow2 # 创建虚拟机以基础镜像为backing file的增量盘 qemu-img create -f qcow2 -F qcow2 -b /var/lib/libvirt/images/ubuntu.qcow2 /var/lib/libvirt/disks/router.qcow2 20G # 创建并启动虚拟机 virt-install --name router --memory 512 --vcpus 1 --disk /var/lib/libvirt/disks/router.qcow2,bus=sata --import --os-variant ubuntu22.10 --network bridge=br0 --noautoconsole # 设置自动启动 virsh autostart router 配置网络 # 将无线网卡透传进虚拟机 # 打开 virt-manager -\u0026gt; 双击 router domain -\u0026gt; 点击 Show virtual hardware details -\u0026gt; 点击 Add Hardware -\u0026gt; 点击 PCI Host Device -\u0026gt; 选择 Intel Corporation Wi-Fi 6 AX200 -\u0026gt; 点击 Finish\nvirsh console router进虚拟机里检查发现已存在一个有线网卡enp1s0和无线网卡wlp6s0\n使用netplan配置网卡 # network: version: 2 renderer: NetworkManager ethernets: enp1s0: dhcp4: false dhcp6: false addresses: [192.168.1.110/24] wifis: wlp6s0: dhcp4: no access-points: \u0026#34;wifi名称\u0026#34;: password: \u0026#34;wifi密码\u0026#34; addresses: [192.168.31.88/24] nameservers: addresses: [223.5.5.5, 114.114.114.114] routes: - to: default via: 192.168.31.1 配置NAT网络 # 开启ipv4 forward\nvim /etc/sysctl.conf - #net.ipv4.ip_forward=1 + net.ipv4.ip_forward=1 sysctl -p 设置SNAT和伪装\niptables -t nat -A POSTROUTING -s \u0026#39;192.168.1.0/24\u0026#39; -o wlp6s0 -j MASQUERADE # 持久化规则 DEBIAN_FRONTEND=noninteractive apt install iptables-persistent -y iptables-save -c \u0026gt; /etc/iptables/rules.v4 查看无线网卡信息 # 安装 # apt install wireless-tools -y 查看无线网卡频率 # 有些wifi配置2.4G和5G网络合并显示, 所以如果信号不好时会连上2.4GHz的频段, 使用iwlist命令查看无线网卡的频率\n# iwlist wlp6s0 freq wlp6s0 32 channels in total; available frequencies : Channel 01 : 2.412 GHz Channel 02 : 2.417 GHz Channel 03 : 2.422 GHz Channel 04 : 2.427 GHz Channel 05 : 2.432 GHz Channel 06 : 2.437 GHz Channel 07 : 2.442 GHz Channel 08 : 2.447 GHz Channel 09 : 2.452 GHz Channel 10 : 2.457 GHz Channel 11 : 2.462 GHz Channel 12 : 2.467 GHz Channel 13 : 2.472 GHz Channel 36 : 5.18 GHz Channel 40 : 5.2 GHz Channel 44 : 5.22 GHz Channel 48 : 5.24 GHz Channel 52 : 5.26 GHz Channel 56 : 5.28 GHz Channel 60 : 5.3 GHz Channel 64 : 5.32 GHz Channel 100 : 5.5 GHz Channel 104 : 5.52 GHz Channel 108 : 5.54 GHz Channel 112 : 5.56 GHz Channel 116 : 5.58 GHz Channel 120 : 5.6 GHz Channel 124 : 5.62 GHz Channel 128 : 5.64 GHz Channel 132 : 5.66 GHz Channel 136 : 5.68 GHz Channel 140 : 5.7 GHz Current Frequency:5.18 GHz (Channel 36) 参考 # Problem with my iptables configuration on reboot "},{"id":39,"href":"/posts/rclone/","title":"Rclone 使用笔记","section":"其他","content":" 使用rclone和alist提供的webdav接口将阿里云盘mount到Ubuntu 22.04的目录上, 实现像访问本地文件一样访问阿里云盘内的文件\n下载安装rclone # curl https://rclone.org/install.sh | bash 配置rclone config # # rclone config No remotes found, make a new one? n) New remote s) Set configuration password q) Quit config n/s/q\u0026gt; n Enter name for new remote. name\u0026gt; alist Option Storage. Type of storage to configure. Choose a number from below, or type in your own value. 1 / 1Fichier \\ (fichier) ... 51 / WebDAV \\ (webdav) ... Storage\u0026gt; 51 Option url. URL of http host to connect to. E.g. https://example.com. Enter a value. url\u0026gt; http://alist.home.lan/dav/ Option vendor. Name of the WebDAV site/service/software you are using. Choose a number from below, or type in your own value. Press Enter to leave empty. 1 / Fastmail Files \\ (fastmail) 2 / Nextcloud \\ (nextcloud) 3 / Owncloud \\ (owncloud) 4 / Sharepoint Online, authenticated by Microsoft account \\ (sharepoint) 5 / Sharepoint with NTLM authentication, usually self-hosted or on-premises \\ (sharepoint-ntlm) 6 / rclone WebDAV server to serve a remote over HTTP via the WebDAV protocol \\ (rclone) 7 / Other site/service or software \\ (other) vendor\u0026gt; 7 Option user. User name. In case NTLM authentication is used, the username should be in the format \u0026#39;Domain\\User\u0026#39;. Enter a value. Press Enter to leave empty. user\u0026gt; admin Option pass. Password. Choose an alternative below. Press Enter for the default (n). y) Yes, type in my own password g) Generate random password n) No, leave this optional password blank (default) y/g/n\u0026gt; y Enter the password: password: Confirm the password: password: Option bearer_token. Bearer token instead of user/pass (e.g. a Macaroon). Enter a value. Press Enter to leave empty. bearer_token\u0026gt; Edit advanced config? y) Yes n) No (default) y/n\u0026gt; n Configuration complete. Options: - type: webdav - url: http://alist.home.lan/dav/ - vendor: other - user: admin - pass: *** ENCRYPTED *** Keep this \u0026#34;alist\u0026#34; remote? y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d\u0026gt; y Current remotes: Name Type ==== ==== alist webdav e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q\u0026gt; q 查看有哪些remote # # rclone listremotes alist: 查看某个remote下的目录或文件 # # rclone lsd alist:/ali/video/诛仙 -1 2024-04-02 12:27:23 -1 诛仙.第1季.2022-2023.4K.全26集 -1 2024-04-02 12:27:24 -1 诛仙.第2季.2024 # rclone ls alist:/ali/video/诛仙/诛仙.第2季.2024 959919343 第27集-Zhu.Xian-2024-03-29-4k-HEVC-H265.AAC-WEB-DL.mkv 995844543 第28集-Zhu.Xian-2024-03-29-4K-HEVC-H265.AAC-WEB-DL.mkv 1012470661 第29集-Zhu.Xian-2024-03-29-4K-HEVC-H265.AAC-WEB-DL.mkv 将alist mount 到目录 # sudo mkdir -p /mnt/alist sudo chmod 777 /mnt/alist # --header \u0026#34;Referer:https://www.aliyundrive.com/\u0026#34; 是必须要有的 rclone mount --cache-dir=/tmp --vfs-cache-mode=writes --header \u0026#34;Referer:https://www.aliyundrive.com/\u0026#34; alist: /mnt/alist/ 配置开机自动挂载\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/mount-mnt-alist.service [Unit] Description=Mount alist After=network.target [Service] Type=simple # 因为我的桌面用户是gobai, 所以需要指定gobai, mount之后/mnt/alist目录的owner会变成执行用户 User=gobai # --cache-dir 指定 cache目录 # --dir-cache-time=10s 指定对目录的cache时间, 因为我主要是用来读取远端云盘的文件, 希望可以尽快刷新目录看到新的文件 ExecStart=rclone mount --cache-dir=/tmp/rclone --dir-cache-time=10s --vfs-cache-mode=full --fs-cache-expire-duration=2m --header \u0026#34;Referer:https://www.aliyundrive.com/\u0026#34; alist: /mnt/alist/ ExecStop=umout /mnt/alist Restart=on-failure RestartSec=15 [Install] WantedBy=default.target EOF systemctl start mount-mnt-alist systemctl enable mount-mnt-alist 参考 # https://rclone.org/downloads/ https://alist.nn.ci/guide/webdav.html "},{"id":40,"href":"/posts/multi-bootable-usb/","title":"Multi-Bootable USB","section":"OS","content":" 从一个USB设备(U盘)启动多个操作系统, 并且U盘还能继续存储其他普通文件\n下载并解压ventoy # wget https://github.com/ventoy/Ventoy/releases/download/v1.0.97/ventoy-1.0.97-linux.tar.gz tar -xvzf ventoy-1.0.97-linux.tar.gz cd ventoy-1.0.97/ Ventoy2Disk.sh用来安装ventor到U盘\n./Ventoy2Disk.sh -h ********************************************** Ventoy: 1.0.97 x86_64 longpanda admin@ventoy.net https://www.ventoy.net ********************************************** Usage: Ventoy2Disk.sh CMD [ OPTION ] /dev/sdX CMD: -i install Ventoy to sdX (fails if disk already installed with Ventoy) -I force install Ventoy to sdX (no matter if installed or not) -u update Ventoy in sdX -l list Ventoy information in sdX OPTION: (optional) -r SIZE_MB preserve some space at the bottom of the disk (only for install) -s/-S enable/disable secure boot support (default is enabled) -g use GPT partition style, default is MBR (only for install) -L Label of the 1st exfat partition (default is Ventoy) -n try non-destructive installation (only for install) 安装ventoy # # ./Ventoy2Disk.sh -I /dev/sdb ********************************************** Ventoy: 1.0.97 x86_64 longpanda admin@ventoy.net https://www.ventoy.net ********************************************** Disk : /dev/sdb Model: Kingston DataTraveler 3.0 (scsi) Size : 115 GB Style: MBR Attention: You will install Ventoy to /dev/sdb. All the data on the disk /dev/sdb will be lost!!! Continue? (y/n) y All the data on the disk /dev/sdb will be lost!!! Double-check. Continue? (y/n) y Create partitions on /dev/sdb by parted in MBR style ... Done Wait for partitions ... partition exist OK create efi fat fs /dev/sdb2 ... mkfs.fat 4.2 (2021-01-31) success Wait for partitions ... /dev/sdb1 exist OK /dev/sdb2 exist OK partition exist OK Format partition 1 /dev/sdb1 ... mkexfatfs 1.3.0 Creating... done. Flushing... done. File system created successfully. mkexfatfs success writing data to disk ... sync data ... esp partition processing ... Install Ventoy to /dev/sdb successfully finished. 查看最终的U盘分区, sdb1分区是存放iso镜像的\n# fdisk -l /dev/sdb Disk /dev/sdb: 115.5 GiB, 124017180672 bytes, 242221056 sectors Disk model: DataTraveler 3.0 Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x7eda2ae4 Device Boot Start End Sectors Size Id Type /dev/sdb1 * 2048 242155519 242153472 115.5G 7 HPFS/NTFS/exFAT /dev/sdb2 242155520 242221055 65536 32M ef EFI (FAT-12/16/32) 拷贝iso镜像到U盘 # 为了充分利用U盘的空间, 我这里会在U盘里既存iso镜像(在image目录)也存储普通文件(在data目录)\n但是如果data下文件特别多，搜索过程就会非常慢，这里就需要指定搜索路径来解决了\n# 如果操作系统没有自动将sdb1 mount到目录需要先手动mount, 否则跳过就好 mkdir -p /mnt/Ventoy mount /dev/sdb1 /mnt/Ventoy # 创建文件夹 cd /mnt/Ventoy mkdir -p image mkdir -p data mkdir -p ventoy # 编辑 ventoy/ventoy.json 文件 cat \u0026lt;\u0026lt;EOF \u0026gt; ventoy/ventoy.json { \u0026#34;control\u0026#34;: [ { \u0026#34;VTOY_DEFAULT_SEARCH_ROOT\u0026#34;: \u0026#34;/image\u0026#34; } ] } EOF 最后拷贝就直接 cp xx.iso /mnt/Ventoy/image/ 就行了\n参考 # Ventoy Global Control Plugin Ventoy Search Configuration Ventoy Github "},{"id":41,"href":"/posts/shell-script/","title":"Shell Script","section":"其他","content":" 最近写的shell脚本比较多，记录一些常用命令, 相当于记录一个索引, 以后用时可以快速回忆起来.\n#!/bin/bash # #!/bin/bash被称为shebang line, 指定执行此脚本文件时使用/bin/bash做为shell解释器程序\n很多主流操作系统默认的shell解释器也是bash\n# echo $SHELL /bin/bash set # set命令用来修改shell环境的运行参数, 完整的可定制的官方手册\n下面是我常用的几个, 可以合并为如下内容写在脚本开头:\n#!/bin/bash set -uxe set -o pipefail set -u # 执行脚本时, 如果遇到不存在的变量, Bash默认会忽略, set -u可以让脚本读到不存在变量时报错\nset -x # 命令执行前会先打印出来, 行首以+表示, 在调试脚本时非常有帮助\nset -e # 执行脚本时, Bash遇到错误默认会继续执行, set -e使得脚本只要发生错误, 就中止执行\nset -o pipefail # set -e有一个例外情况, 就是不适用于管道命令, 比如下面的不会退出\n#!/bin/bash set -e foo | echo a echo bar 执行的结果为:\na set.sh: line 4: foo: command not found bar set -o pipefail可以解决这个问题, 只要一个子命令失败, 整个管道命令就失败, 脚本就会终止执行\n#!/bin/bash set -eo pipefail foo | echo a echo bar 执行的结果为:\na set.sh: line 4: foo: command not found 单引号''和双引号\u0026quot;\u0026quot; # 双引号会将$var解析成本身的值, 单引号不会解析\n# var=1 # echo \u0026#34;$var\u0026#34; 1 # echo \u0026#39;$var\u0026#39; $var \u0026lt;\u0026lt; here document # 一般使用Here Document作为标准输入喂给kubectl apply -f -或者重定向到文件里.\n#!/bin/bash # 标识符或限定符IDENT一般使用EOF表示 COMMAND \u0026lt;\u0026lt;IDENT this is ... IDENT cat \u0026lt;\u0026lt;EOF写入到文件 # cat一般用来查看文件内容, cat \u0026lt;\u0026lt;EOF可以用来将多行内容打印到标准输出重定向写入到文件里, 这里限定符使用EOF.\ncat \u0026lt;\u0026lt;EOF \u0026gt; doc.md # this is ... EOF kubectl apply -f - \u0026lt;\u0026lt;EOF # 使用kubectl直接不创建文件去apply一个yaml\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: macvlan-conf-2 EOF sed (stream editor) # sed全名stream editor, 会流式的一行一行编辑文件, sed手册\n下面的修改都是打印到标准输出, 加上-i参数sed -i 'xxx' filename就可以直接更新到文件了\ns命令替换字符串 # 结合上面的here document一起演示, 这样就不用再单独创建文件了\n将镜像taglatest改为v1.1.1 # sed \u0026#39;s/latest/v1.1.1/g\u0026#39; - \u0026lt;\u0026lt;EOF ecr.gobai.top/example:latest EOF s表示替换命令, /latest/表示匹配latest, /v1.1.1/表示将匹配到的替换为v1.1.1, /g表示每一行中匹配到的全部替换, 没有g只会替换每一行中的第一个.\n只替换行中匹配到的某一个 # # 替换第1个 sed \u0026#39;s/latest/v1.1.1/1\u0026#39; - \u0026lt;\u0026lt;EOF ecr.gobai.top/example:latest latest latest EOF # 替换第2个 sed \u0026#39;s/latest/v1.1.1/2\u0026#39; - \u0026lt;\u0026lt;EOF ecr.gobai.top/example:latest latest latest EOF # 替换第2个和之后的 sed \u0026#39;s/latest/v1.1.1/2g\u0026#39; - \u0026lt;\u0026lt;EOF ecr.gobai.top/example:latest latest latest EOF 只替换部分行字符串 # # 只替换第2行 sed \u0026#39;2s/latest/v1.1.1/g\u0026#39; - \u0026lt;\u0026lt;EOF ecr.gobai.top/example:latest ecr.gobai.top/example:latest ecr.gobai.top/example:latest EOF # 只替换2-3行 sed \u0026#39;2,3s/latest/v1.1.1/g\u0026#39; - \u0026lt;\u0026lt;EOF ecr.gobai.top/example:latest ecr.gobai.top/example:latest ecr.gobai.top/example:latest EOF 更复杂的涉及很多正则的场景我一般直接丢给ChatGPT去写, 知道sed可以完成这些任务就可以了!!!\n一些sed中常用的正则的语法 # 大部分都是通用的, 在其他需要正则匹配的地方也可以使用, 如在grep -e或grep -E中也可以使用\nhttps://www.gnu.org/software/sed/manual/html_node/Regular-Expressions.html\n字符 含义 单个普通字符匹配自身 * 匹配*前面正则表达式的零个或多个匹配项 \\+ 类似*, 但是匹配至少一个 \\? 类似*, 但是匹配零个或一个 \\{i\\} 类似*, 但是匹配i个 \\{i,\\} 类似*, 匹配至少i个 \\(regexp\\) 将内部的regexp作为一个整体分组 . 匹配任意单个字符, 包括换行 ^ 匹配开始处的null字符串 $ 匹配结尾处的null字符串 [list] 匹配方括号中的字符列表中的任意一个 [^list] 匹配非方括号中的字符列表中的任意一个 regexp1|regexp2 匹配regexp1或regexp2 regexp1regexp2 匹配regexp1和regexp2的连接结果 \\digit 匹配正则表达式中第digit个圆括号\\(...\\)子表达式 \\n 匹配换行符 \\char 匹配char字符, char可以是$ * . [ \\ 或者 ^ 一些注意点:\n虽然.可以匹配换行符, 但是有的命令是一行一行处理, 所以正则匹配不到换行, 如sed命令 圆括号匹配 # 圆括号括起来的正则表达式所匹配的字符串可以当成变量来使用, 通过\\1或\\2来引用\n将VERSION后面的版本替换 # V=\u0026#34;1.1.1\u0026#34; sed \u0026#34;s/\\(^VERSION:\\s*\\)[0-9.]\\+/\\1$V/\u0026#34; - \u0026lt;\u0026lt;EOF VERSION: 0.0.1 EOF 圆括号()需要转义\\(\\), 并且因为有变量$V, 单引号需要改为双引号, ^代表行的开始, VERSION:匹配文本字符串, \\s*匹配0或多个空白字符, [0-9.]\\+匹配一个或多个数字或., \\(^VERSION:\\s*\\)匹配到了版本号前面的内容作为变量1, \\1$V代表将匹配到的所有内容替换为版本好前面的内容+新的版本号$V\necho命令 # echo -n # -n do not output the trailing newline 在结尾处不自动添加换行符\necho -e和echo -E # -e enable interpretation of backslash escapes # 激活转义字符 -E disable interpretation of backslash escapes (default) grep命令 # grep应该是特别常见的命令了, grep支持从标准输入stdin作为参数, 如echo 'abc' | grep abc, 也可以从命令行输入参数, 如grep abc demo.md\ngrep -E # 正则匹配\n# str=$(echo \u0026#39;1. aaa\\n2. bbb\u0026#39;) # echo $str | grep -E \u0026#39;a|b\u0026#39; 1. aaa 2. bbb grep -i # 忽略大小写\n# str=$(echo \u0026#39;1. aaa\\n2. bbb\u0026#39;) # echo $str | grep -i -E \u0026#39;A|B\u0026#39; 1. aaa 2. bbb grep -v # -v, --invert-match # 反向匹配, 选择不匹配的行 Invert the sense of matching, to select non-matching lines. 获取不包含a的行\n# str=$(echo \u0026#39;1. aaa\\n2. bbb\u0026#39;) # echo $str | grep -v a 2. bbb grep -n # -n, --line-number # 打印匹配到的行在输入中的行号 Prefix each line of output with the 1-based line number within its input file. 打印匹配到的行的行号\n# echo $str | grep -n -E \u0026#39;a|b\u0026#39; 1:1. aaa 2:2. bbb grep -r # -r, --recursive # 递归读取目录下的文件 Read all files under each directory, recursively, following symbolic links only if they are on the command line. Note that if no file operand is given, B\u0026lt;grep\u0026gt; searches the working directory. This is equivalent to the -d recurse option. 在当前目录递归遍历匹配字符串并打印行号 # grep -rn \u0026#34;string\u0026#34; . xargs命令 # TODO\nawk命令 # awk也是依次处理文件的每一行, 适合处理每一行格式相同的数据\n基本用法\n# 格式 awk 动作 文件名 # 示例 awk \u0026#39;{print $0}\u0026#39; - \u0026lt;\u0026lt;EOF abc 123 666 bcd 234 777 EOF 大括号{}内部是处理当前行的动作, $0代表当前行, 最终效果就是原样打印所有行. awk会根据空格或制表符将每一行分成若干字段, 通过$1 $2 $3代表每一个字段, 也可以通过awk -F ':' '{print $1}' xxx手动指定每一列之间的分隔符为:\nldd命令 # ldd可以查看一个可执行文件依赖哪些动态链接库\n离线有动态链接库的程序 # 查看jq命令依赖哪些动态链接库,\n# ldd $(which jq) linux-vdso.so.1 (0x00007fff12b9f000) libjq.so.1 =\u0026gt; /lib/x86_64-linux-gnu/libjq.so.1 (0x00007fc42d080000) libc.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc42ce00000) libm.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc42cd19000) libonig.so.5 =\u0026gt; /lib/x86_64-linux-gnu/libonig.so.5 (0x00007fc42cc86000) /lib64/ld-linux-x86-64.so.2 (0x00007fc42d0e9000) 如果要离线一个脚本, 只需要将=\u0026gt;后面的文件复制一份打包即可, 这时awk命令就能派上用场了\nAPP_NAME=\u0026#34;jq\u0026#34; mkdir ${APP_NAME}_archive \u0026amp;\u0026amp; cd ${APP_NAME}_archive mkdir libs ldd $(which ${APP_NAME}) | awk \u0026#39;{print $3}\u0026#39; | xargs -i cp -L {} libs 这样只需要再把可执行文件也离线, 就可以离线安装运行了\n不过还有最后一步, 这些lib文件不适合直接都放入/lib/目录下?, 因为有可能这些lib目录下有和当前程序冲突的版本, 所以直接把程序依赖的lib放在一个目录下然后启动时设置LD_LIBRARY_PATH目录让程序去找正确版本的lib库是更稳妥的.\ncp $(which ${APP_NAME}) . cat \u0026lt;\u0026lt;EOF \u0026gt; app_${APP_NAME}.sh #!/bin/bash INSTALL_DIR=\u0026#34;/opt/app_archives\u0026#34; APP_NAME=\u0026#34;${APP_NAME}\u0026#34; export LD_LIBRARY_PATH=\u0026#34;\\${INSTALL_DIR}/\\${APP_NAME}_archive/libs\u0026#34; \\${INSTALL_DIR}/\\${APP_NAME}_archive/\\${APP_NAME} \u0026#34;\\$@\u0026#34; EOF chmod +x app_${APP_NAME}.sh 最终的文件如下\n# tree . . ├── app_jq.sh ├── jq └── libs ├── libc.so.6 ├── libjq.so.1 ├── libm.so.6 └── libonig.so.5 1 directory, 6 files 安装时, 只需要将${APP_NAME}_archive目录放在/opt/app_archives目录下, 然后创建一个如下的软链即可\nln -nsf /opt/app_archives/\\${APP_NAME}_archive/app_\\${APP_NAME}.sh /usr/bin/\\${APP_NAME} mc(minio client) # 设置alias # mc alias set {NAME} http://minio.lan:9000 {USER} {PASSWORD} 设置匿名用户对某bucket权限 # 权限有download, upload 和 public(download+upload)\n设置匿名用户可以下载某个bucket下的文件\n# mc anonymous set download minio/app Access permission for `minio/app` is set to `download` 查看匿名用户对某bucket的权限\n# mc anonymous get minio/app Access permission for `minio/app` is `download` jq命令 # jq用来处理json数据还是超级强大的\nselect过滤数组 # 下面的命令用到了jq中的管道操作|和过滤操作select\n# echo \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;compute\u0026#34;,\u0026#34;image\u0026#34;:\u0026#34;c-image\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;log\u0026#34;}]\u0026#39; | jq \u0026#39;.[] | select(.name == \u0026#34;compute\u0026#34;) | .image\u0026#39; \u0026#34;c-image\u0026#34; 有时不想要字符串两边的双引号, 这时就需要用到上面用到的sed命令了, 思路是正则匹配整个字符串, 然后双引号中间的使用圆括号匹配作为变量\\1, 最后将整个字符串替换为\\1即可\n# str=$(echo \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;compute\u0026#34;,\u0026#34;image\u0026#34;:\u0026#34;c-image\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;log\u0026#34;}]\u0026#39; | jq \u0026#39;.[] | select(.name == \u0026#34;compute\u0026#34;) | .image\u0026#39;) # echo $str | sed \u0026#39;s/^\u0026#34;\\(.*\\)\u0026#34;$/\\1/\u0026#39; c-image tar命令 # 压缩文件 # tar -czvf name.tar.gz name 使用pigz加速压缩 # 如果没安装pigz需要先安装apt install pigz -y\ntar -I pigz -czvf name.tar.gz name 解压文件 # tar -xzvf name.tar.gz docker命令 # 导出镜像到压缩包 # docker save -o all.tar image-a:1.0.0 image-b:1.0.0 image-c:1.0.0 从压缩包导入镜像 # docker load \u0026lt; all.tar TODO # 看一下 the art of command line 然后融入进这片文档 增加 shell 常用的控制循环和条件判断语法 参考 # awk the art of command line "},{"id":42,"href":"/posts/sqlx-vs-xorm/","title":"sqlx vs xorm","section":"三方库","content":" 初始化演示环境 # 使用docker部署 # 部署的当前时间最新版本postgres:16.2\ndocker run -d --name pgsql \\ -e POSTGRES_USER=admin \\ -e POSTGRES_PASSWORD=admin \\ -e POSTGRES_DB=testdb \\ -p 15432:5432 \\ postgres:16.2 使用psql连接 # docker exec -it pgsql psql -U admin -d testdb 初始化user,vps和host表 # CREATE TABLE \u0026#34;user\u0026#34; ( id bigserial PRIMARY KEY, username VARCHAR(255) NOT NULL, password VARCHAR(255) NOT NULL ); CREATE TABLE \u0026#34;host\u0026#34; ( id bigserial PRIMARY KEY, hostname VARCHAR(255) NOT NULL ); CREATE TABLE \u0026#34;vps\u0026#34; ( id bigserial PRIMARY KEY, user_id bigint NOT NULL, host_id bigint NOT NULL, name VARCHAR(255) NOT NULL, sys_disk jsonb NOT NULL DEFAULT \u0026#39;{}\u0026#39; ); 查看创建出的表 # testdb=# \\z Access privileges Schema | Name | Type | Access privileges | Column privileges | Policies --------+-------------+----------+-------------------+-------------------+---------- public | host | table | | | public | host_id_seq | sequence | | | public | user | table | | | public | user_id_seq | sequence | | | public | vps | table | | | public | vps_id_seq | sequence | | | (6 rows) testdb=# \\d user Table \u0026#34;public.user\u0026#34; Column | Type | Collation | Nullable | Default ----------+------------------------+-----------+----------+---------------------------------- id | bigint | | not null | nextval(\u0026#39;user_id_seq\u0026#39;::regclass) username | character varying(255) | | not null | password | character varying(255) | | not null | Indexes: \u0026#34;user_pkey\u0026#34; PRIMARY KEY, btree (id) testdb=# \\d host Table \u0026#34;public.host\u0026#34; Column | Type | Collation | Nullable | Default ----------+------------------------+-----------+----------+---------------------------------- id | bigint | | not null | nextval(\u0026#39;host_id_seq\u0026#39;::regclass) hostname | character varying(255) | | not null | Indexes: \u0026#34;host_pkey\u0026#34; PRIMARY KEY, btree (id) testdb=# \\d vps Table \u0026#34;public.vps\u0026#34; Column | Type | Collation | Nullable | Default ----------+------------------------+-----------+----------+--------------------------------- id | bigint | | not null | nextval(\u0026#39;vps_id_seq\u0026#39;::regclass) user_id | bigint | | not null | host_id | bigint | | not null | name | character varying(255) | | not null | sys_disk | jsonb | | not null | \u0026#39;{}\u0026#39;::jsonb Indexes: \u0026#34;vps_pkey\u0026#34; PRIMARY KEY, btree (id) 初始化数据 # -- 插入用户数据 INSERT INTO \u0026#34;user\u0026#34; (username, password) VALUES (\u0026#39;user1\u0026#39;, \u0026#39;password1\u0026#39;), (\u0026#39;user2\u0026#39;, \u0026#39;password2\u0026#39;); -- 插入主机数据 INSERT INTO \u0026#34;host\u0026#34; (hostname) VALUES (\u0026#39;host1\u0026#39;), (\u0026#39;host2\u0026#39;); -- user1 在 host1 上创建一个 VPS INSERT INTO \u0026#34;vps\u0026#34; (user_id, host_id, name, sys_disk) VALUES ((SELECT id FROM \u0026#34;user\u0026#34; WHERE username = \u0026#39;user1\u0026#39;), (SELECT id FROM host WHERE hostname = \u0026#39;host1\u0026#39;), \u0026#39;vps_user1_host1\u0026#39;, \u0026#39;{\u0026#34;disk_size\u0026#34;: 50}\u0026#39;); -- user2 在 host1 上创建一个 VPS INSERT INTO \u0026#34;vps\u0026#34; (user_id, host_id, name, sys_disk) VALUES ((SELECT id FROM \u0026#34;user\u0026#34; WHERE username = \u0026#39;user2\u0026#39;), (SELECT id FROM host WHERE hostname = \u0026#39;host1\u0026#39;), \u0026#39;vps_user2_host1\u0026#39;, \u0026#39;{\u0026#34;disk_size\u0026#34;: 60}\u0026#39;); -- user1 在 host2 上创建一个 VPS INSERT INTO \u0026#34;vps\u0026#34; (user_id, host_id, name, sys_disk) VALUES ((SELECT id FROM \u0026#34;user\u0026#34; WHERE username = \u0026#39;user1\u0026#39;), (SELECT id FROM host WHERE hostname = \u0026#39;host2\u0026#39;), \u0026#39;vps_user1_host2\u0026#39;, \u0026#39;{\u0026#34;disk_size\u0026#34;: 70}\u0026#39;); -- user2 在 host2 上创建一个 VPS INSERT INTO \u0026#34;vps\u0026#34; (user_id, host_id, name, sys_disk) VALUES ((SELECT id FROM \u0026#34;user\u0026#34; WHERE username = \u0026#39;user2\u0026#39;), (SELECT id FROM host WHERE hostname = \u0026#39;host2\u0026#39;), \u0026#39;vps_user2_host2\u0026#39;, \u0026#39;{\u0026#34;disk_size\u0026#34;: 80}\u0026#39;); 关于 database/sql # database/sql提供了操作SQL数据库的通用接口, 需要结合database driver同时使用, 这里是一些驱动列表\n使用用例可以看官方wiki\n关于 sqlx # 关于 xorm # 使用场景对比 # 前情提要 # 使用\n场景一: upsert/inster/update # 插入结构体时, nil值类型字段会被怎么处理?\n如何指定只插入某些字段\n场景二: get/select # 分页查询并获取总行数SelectAndCount # 场景三: 获取插入行的id # 场景四: 连表查询 # 场景五: sql日志和trace # 场景六: # 一些注意点 # xorm # 对比总结 # 经过使用sqlx来应对日常开发对我来说也是比较顺手的, 可以意识到一些使用xorm注意不到的点, 比如\n"},{"id":43,"href":"/posts/openwrt/","title":"OpenWrt","section":"Network","content":"很久没折腾OpenWrt了, 囊中羞涩, 没有其他合适的设备, 这次是在KVM虚机中运行使用(ALL IN BOOM!)\n先亮个当前的穷人版家庭网络拓扑图\n准备qcow2镜像 # 首先下载最新的镜像, 截止目前最新版为23.05.3, 我这里下载的是x86-64的镜像\nwget https://mirror-03.infra.openwrt.org/releases/23.05.3/targets/x86/64/openwrt-23.05.3-x86-64-generic-ext4-combined.img.gz # 解压 gunzip openwrt-23.05.3-x86-64-generic-ext4-combined.img.gz # 这里因为我要作为KVM虚拟机的镜像, 所以转换为qcow2格式. 如果是在物理机上部署, 可以直接直接刷到U盘上. qemu-img convert -f raw openwrt-23.05.3-x86-64-generic-ext4-combined.img -O qcow2 /var/lib/libvirt/images/openwrt.qcow2 运行虚机 # 我是用libvirt来管理qemu/kvm虚拟机, 如果没安装要先安装\napt install virt-manager qemu bridge-utils -y 我这里将镜像复制到了/var/lib/libvirt/disks/目录下\nqemu-img create -f qcow2 -F qcow2 -b /var/lib/libvirt/images/openwrt.qcow2 /var/lib/libvirt/disks/openwrt.qcow2 1G 使用virt-install运行虚拟机, 这里网卡使用virtio类型并桥接到之前文档里创建的br0上, 选择virtio是因为性能最好, 可以达到10Gbps以上\n# 运行, 这里网络指定的之前文章中创建的网桥网络br0 virt-install \\ --name openwrt \\ --memory 256 \\ --vcpus 1 \\ --network bridge=br0,model=virtio \\ --disk path=/var/lib/libvirt/disks/openwrt.qcow2,bus=ide \\ --import \\ --autostart \\ --osinfo detect=on,require=off \\ --noautoconsole 配置网络 # 连接console配置网络\nvirsh console openwrt 修改网络配置文件 /etc/config/network 只修改了lan配置和删除了br-lan网桥, 其他都是默认的, 因为我的使用场景比较简单, 虚拟机只有一个网卡, 直接使用eth0作为lan口连接器的物理网卡设备 具体修改了lan配置的ipaddr, 增加gateway和dns\nconfig interface \u0026#39;loopback\u0026#39; option device \u0026#39;lo\u0026#39; option proto \u0026#39;static\u0026#39; option ipaddr \u0026#39;127.0.0.1\u0026#39; option netmask \u0026#39;255.0.0.0\u0026#39; config globals \u0026#39;globals\u0026#39; option ula_prefix \u0026#39;fd5d:6ea2:93e4::/48\u0026#39; # config interface is not the configuration of a physical interface, but rather the specification of a connector to some network. config interface \u0026#39;lan\u0026#39; # device is usually not the name of something configured with config interface, but the name of a physical interface. option device \u0026#39;eth0\u0026#39; option proto \u0026#39;static\u0026#39; option ipaddr \u0026#39;192.168.1.99\u0026#39; option netmask \u0026#39;255.255.255.0\u0026#39; option gateway \u0026#39;192.168.1.1\u0026#39; list dns \u0026#39;223.5.5.5\u0026#39; 修改之后重启网络\nservice network restart 测试路由和DNS解析是否正常: ping baidu.com, 一切OK再继续下面的\n换源 # 大陆码农生存必备技能了, 这里使用的中科大的源, 配置文件位于 /etc/opkg/distfeeds.conf\nsrc/gz openwrt_core http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/targets/x86/64/packages src/gz openwrt_base http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/base src/gz openwrt_luci http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/luci src/gz openwrt_packages http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/packages src/gz openwrt_routing http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/routing src/gz openwrt_telephony http://mirrors.ustc.edu.cn/openwrt/releases/23.05.3/packages/x86_64/telephony 然后更新一下\nopkg update 扩容根分区和文件系统 # 如果觉得默认的100M左右就够用了可以跳过这步\n这个文档里的脚本只支持x86的ext4和squashfs镜像创建的虚机, 自动检测根分区和文件系统, 将空闲空间分给根分区和文件系统\nhttps://openwrt.org/docs/guide-user/advanced/expand_root\n扩容之后就使用起来就不用扣扣搜搜的了\nroot@OpenWrt:~# df -hT / Filesystem Type Size Used Available Use% Mounted on /dev/root ext4 994.8M 56.9M 921.9M 6% / 安装OpenClash # 主要是跟着官方wiki执行, 部分Github文件下载使用文件代理加速下载服务 https://mirror.ghproxy.com/\n先卸载dnsmasq, 否则会和dnsmasq-full冲突, openclash依赖dnsmasq-full\n# 如果提示dhcp配置文件(/etc/config/dhcp)没修改, 可以手动删了, 将新的(/etc/config/dhcp-opkg)覆盖过去 opkg remove dnsmasq \u0026amp;\u0026amp; opkg install dnsmasq-full 下载安装安装包和各依赖\n# 下载安装包 cd /tmp \u0026amp;\u0026amp; wget https://mirror.ghproxy.com/https://github.com/vernesong/OpenClash/releases/download/v0.46.011-beta/luci-app-openclash_0.46.011-beta_all.ipk # 安装所有依赖 opkg install coreutils-nohup bash iptables dnsmasq-full curl ca-certificates ipset ip-full iptables-mod-tproxy iptables-mod-extra libcap libcap-bin ruby ruby-yaml kmod-tun kmod-inet-diag unzip luci-compat luci luci-base # 安装 opkg install /tmp/luci-app-openclash_0.46.011-beta_all.ipk 下载clash内核\n# 下载 wget https://mirror.ghproxy.com/https://github.com/vernesong/OpenClash/releases/download/Clash/clash-linux-amd64.tar.gz # 解压 tar -zxvf clash-linux-amd64.tar.gz # 放置到执行目录下 mv clash /etc/openclash/core/clash # 如果没有可执行权限设置执行权限 chmod +x /etc/openclash/core/clash 打开登录 OpenWrt web界面配置 # 默认密码为空, 第一次登录后可以修改一下\n打开Services下的OpenClash进行配置即可.\n此处省略1w字\u0026hellip;\n验证OpenWrt的DHCP服务没问题后, 关闭主路由器的DHCP服务.\n关于为什么关闭主路由的DHCP功能: 因为我的主路由不支持设置DHCP服务器的默认网关, 所以只有设置静态ip并手动填写网关和DNS为OpenWrt的ip才能魔法上网. 不如直接不使用主路由的DHCP服务, 然后开启OpenWrt lan口的DHCP服务.\n至此大功告成, 连接wifi之后即可魔法上网, (手动撒花).\n分割线\n其他小修改 # 修改dhcp分配ip范围 # 修改配置文件 /etc/config/dhcp\nconfig dhcp \u0026#39;lan\u0026#39; option interface \u0026#39;lan\u0026#39; option start \u0026#39;150\u0026#39; option limit \u0026#39;100\u0026#39; option leasetime \u0026#39;12h\u0026#39; 修改之后重启 service dnsmasq restart, 配置之后有段时间dhcp server没正常运行, 可通过 logread -e dnsmasq 查看服务日志排查.\n正常运行之后dhcp server会监听67端口\nroot@OpenWrt:~# netstat -anp | grep :67 udp 0 0 0.0.0.0:67 0.0.0.0:* 27573/dnsmasq 修改br0的网关和dns # 为了小主机三层网络也路由到代理网关, 这里修改之前br0的netplan配置后执行netplan apply生效\nnetwork: bridges: br0: dhcp4: false dhcp6: false addresses: - 192.168.1.100/24 routes: - to: default - via: 192.168.1.1 + via: 192.168.1.99 nameservers: addresses: - - 192.168.1.1 - - 223.5.5.5 + - 192.168.1.99 + search: + - local interfaces: - enp1s0 parameters: stp: false 支持流量优先走主网关 # 因为 192.168.1.1 的网关是使用的我的流量卡的5G流量, 所以是做为备用网关, 主网关是 无线转有线网络 配置的 192.168.1.110\n下面脚本实现优先使用主网关, 主网关不能访问互联网时再切换到备用网关\n下面脚本放在 /root/gateway-switch.sh\n#!/bin/bash PRIMARY_GATEWAY=\u0026#34;192.168.1.110\u0026#34; SECONDARY_GATEWAY=\u0026#34;192.168.1.1\u0026#34; EXTERNAL_IP=\u0026#34;223.6.6.6\u0026#34; echo_with_time() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; } test_gateway() { local gateway=$1 # Add a temporary route ip r add $EXTERNAL_IP via $gateway # Test connectivity ping -c 2 -W 1 $EXTERNAL_IP \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 local result=$? # Remove the temporary route ip r del $EXTERNAL_IP via $gateway return $result } while true; do if test_gateway $PRIMARY_GATEWAY; then echo_with_time \u0026#34;Internet is accessible via primary gateway $PRIMARY_GATEWAY. Setting it as default.\u0026#34; ip r replace default via $PRIMARY_GATEWAY else echo_with_time \u0026#34;Internet is not accessible via primary gateway $PRIMARY_GATEWAY. Using secondary gateway $SECONDARY_GATEWAY.\u0026#34; ip r replace default via $SECONDARY_GATEWAY fi sleep 2s done 设置crontab # openwrt的crontab不支持@reboot, 这里使用flock实现类似效果\n# 设置crontab echo \u0026#34;* * * * * flock -n /root/gateway-switch.lock /root/gateway-switch.sh \u0026gt;\u0026gt; /root/gateway-switch.log 2\u0026gt;\u0026amp;1\u0026#34; \u0026gt;\u0026gt; /etc/crontabs/root # 运行cron并设置开机自启 /etc/init.d/cron start /etc/init.d/cron enable 参考 # OpenWrt in QEMU OpenWrt: Expanding root partition and filesystem OpenWrt: DHCP and DNS configuration /etc/config/dhcp OpenWrt: Clarifying the term \u0026ldquo;Interface\u0026rdquo; OpenClash 安装 "},{"id":44,"href":"/posts/dhclient/","title":"dhclient 问题","section":"Network","content":"在机器上使用netplan+NetworkManager配置bridged network之后\n最近经常电脑用着用着就不能联网了，发现enp1s0总是偶尔冒出一个ipv4地址，并且路由表会多出一个从enp1s0出去的default路由。后来看journalctl日志发现是dhclient搞的事情(学艺不精, 没第一时间联系起来)。\n下面是部分日志：\n➜ ~ journalctl -n 1000000 | grep \u0026#39;192.168.1.22\\|enp1s0\u0026#39; ... 10月 09 20:14:25 gobai-SER dhclient[107299]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x4745a8ce) 10月 09 20:14:26 gobai-SER dhclient[73666]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x2cfc74b3) 10月 09 20:14:26 gobai-SER dhclient[157839]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x453b8549) 10月 09 20:14:28 gobai-SER dhclient[170251]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x334a15e8) 10月 09 20:14:28 gobai-SER dhclient[237127]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x7fd24947) 10月 09 20:14:32 gobai-SER avahi-autoipd(enp1s0)[307826]: Found user \u0026#39;avahi-autoipd\u0026#39; (UID 110) and group \u0026#39;avahi-autoipd\u0026#39; (GID 119). 10月 09 20:14:32 gobai-SER avahi-autoipd(enp1s0)[307826]: Successfully called chroot(). 10月 09 20:14:32 gobai-SER avahi-autoipd(enp1s0)[307826]: Successfully dropped root privileges. 10月 09 20:14:32 gobai-SER avahi-autoipd(enp1s0)[307826]: Starting with address 169.254.4.220 10月 09 20:14:32 gobai-SER avahi-autoipd(enp1s0)[307826]: Got SIGTERM, quitting. 10月 09 20:14:32 gobai-SER dhclient[170251]: DHCPDISCOVER on enp1s0 to 255.255.255.255 port 67 interval 3 (xid=0x1f69d35f) 10月 09 20:14:32 gobai-SER dhclient[170251]: DHCPOFFER of 192.168.1.22 from 192.168.1.1 10月 09 20:14:32 gobai-SER dhclient[170251]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x5fd3691f) 10月 09 20:14:32 gobai-SER dhclient[170251]: DHCPACK of 192.168.1.22 from 192.168.1.1 (xid=0x1f69d35f) 10月 09 20:14:32 gobai-SER avahi-daemon[588]: Joining mDNS multicast group on interface enp1s0.IPv4 with address 192.168.1.22. 10月 09 20:14:32 gobai-SER avahi-daemon[588]: New relevant interface enp1s0.IPv4 for mDNS. 10月 09 20:14:32 gobai-SER avahi-daemon[588]: Registering new address record for 192.168.1.22 on enp1s0.IPv4. 10月 09 20:14:32 gobai-SER systemd-resolved[237121]: enp1s0: Bus client set search domain list to: home 10月 09 20:14:32 gobai-SER dhclient[157839]: DHCPDISCOVER on enp1s0 to 255.255.255.255 port 67 interval 3 (xid=0x41cc913f) 10月 09 20:14:32 gobai-SER systemd-resolved[237121]: enp1s0: Bus client set DNS server list to: 192.168.1.1, 223.5.5.5 10月 09 20:14:32 gobai-SER dhclient[157839]: DHCPOFFER of 192.168.1.22 from 192.168.1.1 10月 09 20:14:32 gobai-SER dhclient[157839]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x3f91cc41) 10月 09 20:14:32 gobai-SER dhclient[157839]: DHCPACK of 192.168.1.22 from 192.168.1.1 (xid=0x41cc913f) 10月 09 20:14:32 gobai-SER dhclient[170251]: bound to 192.168.1.22 -- renewal in 32921 seconds. 10月 09 20:14:32 gobai-SER dhclient[157839]: bound to 192.168.1.22 -- renewal in 36989 seconds. 10月 09 20:14:35 gobai-SER avahi-daemon[588]: Withdrawing address record for 192.168.1.22 on enp1s0. 10月 09 20:14:35 gobai-SER avahi-daemon[588]: Leaving mDNS multicast group on interface enp1s0.IPv4 with address 192.168.1.22. 10月 09 20:14:35 gobai-SER avahi-daemon[588]: Interface enp1s0.IPv4 no longer relevant for mDNS. 10月 09 20:14:35 gobai-SER avahi-autoipd(enp1s0)[307982]: Found user \u0026#39;avahi-autoipd\u0026#39; (UID 110) and group \u0026#39;avahi-autoipd\u0026#39; (GID 119). 10月 09 20:14:35 gobai-SER avahi-autoipd(enp1s0)[307982]: Successfully called chroot(). 10月 09 20:14:35 gobai-SER avahi-autoipd(enp1s0)[307982]: Successfully dropped root privileges. 10月 09 20:14:35 gobai-SER avahi-autoipd(enp1s0)[307982]: Starting with address 169.254.4.220 10月 09 20:14:35 gobai-SER avahi-autoipd(enp1s0)[307982]: Got SIGTERM, quitting. 10月 09 20:14:36 gobai-SER dhclient[73666]: DHCPDISCOVER on enp1s0 to 255.255.255.255 port 67 interval 3 (xid=0x50a89e0e) 10月 09 20:14:36 gobai-SER dhclient[73666]: DHCPOFFER of 192.168.1.22 from 192.168.1.1 10月 09 20:14:36 gobai-SER dhclient[73666]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0xe9ea850) 10月 09 20:14:36 gobai-SER dhclient[73666]: DHCPACK of 192.168.1.22 from 192.168.1.1 (xid=0x50a89e0e) 10月 09 20:14:36 gobai-SER avahi-daemon[588]: Joining mDNS multicast group on interface enp1s0.IPv4 with address 192.168.1.22. 10月 09 20:14:36 gobai-SER avahi-daemon[588]: New relevant interface enp1s0.IPv4 for mDNS. 10月 09 20:14:36 gobai-SER avahi-daemon[588]: Registering new address record for 192.168.1.22 on enp1s0.IPv4. 10月 09 20:14:36 gobai-SER systemd-resolved[237121]: enp1s0: Bus client set search domain list to: home 10月 09 20:14:36 gobai-SER systemd-resolved[237121]: enp1s0: Bus client set DNS server list to: 192.168.1.1, 223.5.5.5 10月 09 20:14:36 gobai-SER dhclient[73666]: bound to 192.168.1.22 -- renewal in 34351 seconds. 10月 09 20:14:36 gobai-SER dhclient[107299]: DHCPDISCOVER on enp1s0 to 255.255.255.255 port 67 interval 3 (xid=0x27725347) 10月 09 20:14:36 gobai-SER dhclient[107299]: DHCPOFFER of 192.168.1.22 from 192.168.1.1 10月 09 20:14:36 gobai-SER dhclient[107299]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x47537227) 10月 09 20:14:36 gobai-SER dhclient[107299]: DHCPACK of 192.168.1.22 from 192.168.1.1 (xid=0x27725347) 10月 09 20:14:36 gobai-SER dhclient[107299]: bound to 192.168.1.22 -- renewal in 40122 seconds. 10月 09 20:14:36 gobai-SER avahi-daemon[588]: Withdrawing address record for 192.168.1.22 on enp1s0. 10月 09 20:14:36 gobai-SER avahi-daemon[588]: Leaving mDNS multicast group on interface enp1s0.IPv4 with address 192.168.1.22. 10月 09 20:14:36 gobai-SER avahi-daemon[588]: Interface enp1s0.IPv4 no longer relevant for mDNS. 10月 09 20:14:36 gobai-SER avahi-autoipd(enp1s0)[308110]: Found user \u0026#39;avahi-autoipd\u0026#39; (UID 110) and group \u0026#39;avahi-autoipd\u0026#39; (GID 119). 10月 09 20:14:36 gobai-SER avahi-autoipd(enp1s0)[308110]: Successfully called chroot(). 10月 09 20:14:36 gobai-SER avahi-autoipd(enp1s0)[308110]: Successfully dropped root privileges. 10月 09 20:14:36 gobai-SER avahi-autoipd(enp1s0)[308110]: Starting with address 169.254.4.220 10月 09 20:14:42 gobai-SER avahi-autoipd(enp1s0)[308110]: Callout BIND, address 169.254.4.220 on interface enp1s0 10月 09 20:14:42 gobai-SER avahi-daemon[588]: Joining mDNS multicast group on interface enp1s0.IPv4 with address 169.254.4.220. 10月 09 20:14:42 gobai-SER avahi-daemon[588]: New relevant interface enp1s0.IPv4 for mDNS. 10月 09 20:14:42 gobai-SER avahi-daemon[588]: Registering new address record for 169.254.4.220 on enp1s0.IPv4. 10月 09 20:14:46 gobai-SER avahi-autoipd(enp1s0)[308110]: Successfully claimed IP address 169.254.4.220 10月 09 20:14:46 gobai-SER avahi-autoipd(enp1s0)[308110]: Got SIGTERM, quitting. 10月 09 20:14:46 gobai-SER avahi-autoipd(enp1s0)[308110]: Callout STOP, address 169.254.4.220 on interface enp1s0 10月 09 20:14:46 gobai-SER avahi-daemon[588]: Withdrawing address record for 169.254.4.220 on enp1s0. 10月 09 20:14:46 gobai-SER avahi-daemon[588]: Leaving mDNS multicast group on interface enp1s0.IPv4 with address 169.254.4.220. 10月 09 20:14:46 gobai-SER avahi-daemon[588]: Interface enp1s0.IPv4 no longer relevant for mDNS. 10月 09 20:14:46 gobai-SER dhclient[237127]: DHCPDISCOVER on enp1s0 to 255.255.255.255 port 67 interval 3 (xid=0x389e944d) 10月 09 20:14:46 gobai-SER dhclient[237127]: DHCPOFFER of 192.168.1.22 from 192.168.1.1 10月 09 20:14:46 gobai-SER dhclient[237127]: DHCPREQUEST for 192.168.1.22 on enp1s0 to 255.255.255.255 port 67 (xid=0x4d949e38) 10月 09 20:14:46 gobai-SER dhclient[237127]: DHCPACK of 192.168.1.22 from 192.168.1.1 (xid=0x389e944d) 10月 09 20:14:46 gobai-SER avahi-daemon[588]: Joining mDNS multicast group on interface enp1s0.IPv4 with address 192.168.1.22. 10月 09 20:14:46 gobai-SER avahi-daemon[588]: New relevant interface enp1s0.IPv4 for mDNS. 10月 09 20:14:46 gobai-SER avahi-daemon[588]: Registering new address record for 192.168.1.22 on enp1s0.IPv4. 10月 09 20:14:47 gobai-SER systemd-resolved[237121]: enp1s0: Bus client set search domain list to: home 10月 09 20:14:47 gobai-SER systemd-resolved[237121]: enp1s0: Bus client set DNS server list to: 192.168.1.1, 223.5.5.5 10月 09 20:14:47 gobai-SER dhclient[237127]: bound to 192.168.1.22 -- renewal in 40782 seconds. ➜ ~ ps -aux | grep dhclient root 73666 0.0 0.0 101232 6228 ? Ssl 9月28 0:15 dhclient root 107299 0.0 0.0 101232 6228 ? Ssl 10月04 0:09 dhclient root 157839 0.0 0.0 101232 6112 ? Ssl 10月06 0:06 dhclient root 170251 0.0 0.0 101232 6184 ? Ssl 10月06 0:08 dhclient root 237127 0.0 0.0 101232 6012 ? Ssl 10月07 0:06 dhclient gobai 322905 0.0 0.0 12308 2816 pts/5 S+ 21:49 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox dhclient 对比上面五个进程(DHCP Client)和日志发现，五个进程都干了同样的事：\nDHCP DISCOVER 将该报文放入目的端口67(DHCP Server)和源端口68(DHCP Client)的UDP报文段 该UDP报文段放置在一个具有广播IP目的地址(255.255.255.255)和源IP地址0.0.0.0的IP数据报中, 因为此时enp1s0还没有ip地址 IP数据报又被放置在以太网帧中, 该以太网帧目的MAC地址为FF:FF:FF:FF:FF:FF使该帧将广播到与路由器/交换机连接的所有设备, 该帧的源MAC地址是enp1s0的MAC地址 DHCP OFFER DHCP Server用来响应DHCP DISCOVER报文，此报文携带了分给enp1s0的地址192.168.1.22和DHCP Server的地址192.168.1.1。 DHCP REQUEST 发送广播的DHCP REQUEST报文来回应服务器的DHCP OFFER报文。表示要给enp1s0申请ip 192.168.1.22 DHCP ACK 服务器对客户端的DHCP REQUEST报文的确认响应报文，客户端收到此报文后，才真正获得了IP地址和相关的配置信息。 出现5个dhclient进程的原因就是之前测试执行dhclient命令后没有停止运行它\n将上面5个进程全kill后就一切正常了，后面有时间再详细读一下dhclient的manual\n参考 # 计算机网络 自顶向下方法 第七版 Understanding the Basic Operations of DHCP "},{"id":45,"href":"/posts/creating-a-bridged-network-with-netplan-on-ubuntu-22-04/","title":"在 Ubuntu 22.04 使用 netplan 创建桥接网络","section":"Network","content":"本地LAN环境\nLAN网关 192.168.1.1 子网掩码 255.255.255.0 DHCP范围 192.168.1.2-192.168.32 创建一个bridged network # 创建一个网桥br0给虚机使用，使得虚机和其他设备都在一个LAN下\n总配置(netplan get)如下:\nnetwork: version: 2 renderer: NetworkManager ethernets: enp1s0: dhcp4: false dhcp6: false bridges: br0: addresses: - \u0026#34;192.168.1.100/24\u0026#34; nameservers: addresses: - 192.168.1.1 dhcp4: false dhcp6: false interfaces: - enp1s0 parameters: stp: false routes: - to: \u0026#34;default\u0026#34; via: \u0026#34;192.168.1.1\u0026#34; 由三个文件组成:\n/etc/netplan/01-network-manager-all.yaml # Let NetworkManager manage all devices on this system network: version: 2 renderer: NetworkManager /etc/netplan/10-ethernet-enp1s0.yaml network: ethernets: enp1s0: dhcp4: false dhcp6: false /etc/netplan/99-bridged-network-br0.yaml network: bridges: br0: dhcp4: false dhcp6: false addresses: - 192.168.1.100/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: - 192.168.1.1 - 223.5.5.5 interfaces: - enp1s0 parameters: stp: false 应用网络配置 # 容易失联，如果是ssh远程操作请谨慎操作\nnetplan apply 补充 # 如何没有安装NetworkManager需要先安装(通过systemctl status NetworkManager查看是否安装) apt install network-manager -y 生产环境可以systemd-networkd和NetworkManager共存, 但是在我这里遇到了一些问题 nmstate 依赖NetworkManager服务, NM可以使用10-globally-managed-devices.conf配置不管理哪些接口\n禁用 systemd-networkd # 先关闭 systemd-networkd.socket, 否则每次关闭 systemd-networkd 都会被马上重新激活 systemctl stop systemd-networkd.socket systemctl disable systemd-networkd.socket systemctl stop systemd-networkd systemctl disable systemd-networkd netplan apply之后会发现br0会出现好几个inet6 相关讨论 Why does my ubuntu-server have 4 ipv6 addresses?\n"},{"id":46,"href":"/posts/delete-partition-and-expand-another/","title":"删除分区并扩容另一个分区和根文件系统","section":"Storage","content":" 现在要将 /dev/sda3 分区删掉并扩容到 /dev/sda2, 并且在不重启服务器的情况下扩容根文件系统(跟文件系统 / 挂载在 /dev/sda2 上, 并且 filesystem 是 ext4)\n磁盘初始分区和挂载情况 # ➜ ~ lsblk /dev/sda NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 100G 0 disk ├─sda1 8:1 0 512M 0 part /boot/efi ├─sda2 8:2 0 98.5G 0 part / └─sda3 8:3 0 976M 0 part ➜ ~ fdisk -l /dev/sda Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors Disk model: BlockVolume Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disklabel type: gpt Disk identifier: 40BED670-8B91-4520-9785-DB1F1035C039 Device Start End Sectors Size Type /dev/sda1 2048 1050623 1048576 512M EFI System /dev/sda2 1050624 207714303 206663680 98.5G Linux filesystem /dev/sda3 207714304 209713151 1998848 976M Linux swap ➜ ~ df -hT /dev/sda2 Filesystem Type Size Used Avail Use% Mounted on /dev/sda2 ext4 97G 28G 64G 31% / 删除分区 /dev/sda3 # ➜ ~ fdisk /dev/sda Welcome to fdisk (util-linux 2.36.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): d Partition number (1-3, default 3): 3 Partition 3 has been deleted. Command (m for help): p Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors Disk model: BlockVolume Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disklabel type: gpt Disk identifier: 40BED670-8B91-4520-9785-DB1F1035C039 Device Start End Sectors Size Type /dev/sda1 2048 1050623 1048576 512M EFI System /dev/sda2 1050624 207714303 206663680 98.5G Linux filesystem Command (m for help): w # 保存退出 The partition table has been altered. Syncing disks. 扩容分区 /dev/sda2 和 根文件系统 # 使用 fdisk 扩容 /dev/sda2, 前提是 /dev/sda2 后面没有其他分区了，可以这样扩容(先删除不退出并重建分区, 分区 Start 不变, End 增大)\n➜ ~ fdisk /dev/sda Welcome to fdisk (util-linux 2.36.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors Disk model: BlockVolume Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disklabel type: gpt Disk identifier: 40BED670-8B91-4520-9785-DB1F1035C039 Device Start End Sectors Size Type /dev/sda1 2048 1050623 1048576 512M EFI System /dev/sda2 1050624 207714303 206663680 98.5G Linux filesystem Command (m for help): d # 删除第二个分区, 不要保存退出, 退出就凉了 Partition number (1,2, default 2): 2 Partition 2 has been deleted. Command (m for help): p Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors Disk model: BlockVolume Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disklabel type: gpt Disk identifier: 40BED670-8B91-4520-9785-DB1F1035C039 Device Start End Sectors Size Type /dev/sda1 2048 1050623 1048576 512M EFI System Command (m for help): n # 紧接着重新创建 Partition number (2-128, default 2): 2 # 因为使用的是GPT分区表, 所以最多可以有128个分区, MBR的只能有4个分区 First sector (1050624-209715166, default 1050624): Last sector, +/-sectors or +/-size{K,M,G,T,P} (1050624-209715166, default 209715166): Created a new partition 2 of type \u0026#39;Linux filesystem\u0026#39; and of size 99.5 GiB. Partition #2 contains a ext4 signature. Do you want to remove the signature? [Y]es/[N]o: N Command (m for help): p Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors Disk model: BlockVolume Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disklabel type: gpt Disk identifier: 40BED670-8B91-4520-9785-DB1F1035C039 Device Start End Sectors Size Type /dev/sda1 2048 1050623 1048576 512M EFI System /dev/sda2 1050624 209715166 208664543 99.5G Linux filesystem Command (m for help): w # 保存退出 The partition table has been altered. Syncing disks. reload partition table\napt install parted -y partprobe /dev/sda resize文件系统\n➜ ~ resize2fs /dev/sda2 resize2fs 1.46.2 (28-Feb-2021) Filesystem at /dev/sda2 is mounted on /; on-line resizing required old_desc_blocks = 13, new_desc_blocks = 13 The filesystem on /dev/sda2 is now 26083067 (4k) blocks long. ➜ ~ df -hT /dev/sda2 Filesystem Type Size Used Avail Use% Mounted on /dev/sda2 ext4 98G 28G 65G 31% / 至此, 在不重启的情况下 / 目录的容量从最初的 97G 变成了 98G 👏\n参考 # How can I resize an ext root partition at runtime? Re-read The Partition Table Without Rebooting Linux System 调整ext4根文件系统大小 "},{"id":47,"href":"/posts/vscode-extensions/","title":"Vscode Extensions","section":"其他","content":" 记录我的vscode使用的扩展插件 # 持续补充中\nvscode-icons-mac GitLens Go Remote - SSH shellman REST Client Todo Tree 一些配置 # \u0026#34;workbench.tree.indent\u0026#34;: 16 "},{"id":48,"href":"/posts/macos-config/","title":"MacOS Config","section":"其他","content":" 安装iproute2mac # 可以和在linux操作系统一样使用ip命令查看和管理网络, 赞!!!\nbrew install iproute2mac ssh配置alive # 配置ServerAliveInterval, 防止长时间没有数据交互后连接断掉\n# cat ~/.ssh/config Host * ServerAliveInterval 30 Host home HostName 192.168.1.100 User root ... "},{"id":49,"href":"/posts/ubuntu-config/","title":"Ubuntu Config","section":"其他","content":" 以下配置都是在 Ubuntu 22.04 系统配置\n配置中文输入法 # Open Settings, go to Region \u0026amp; Language -\u0026gt; Manage Installed Languages -\u0026gt; Install / Remove languages. Select Chinese (Simplified). Make sure Keyboard Input method system has Ibus selected. Apply. Reboot Log back in, reopen Settings, go to Keyboard. Click on the \u0026ldquo;+\u0026rdquo; sign under Input sources. Select Chinese (China) and then Chinese (Intelligent Pinyin). ubuntu-22-04-chinese-simplified-pinyin-input-support\n修正简体中文显示为异体(日文)字形 # ubuntu/linux对中文支持的不太好, 在选择汉字字体时, 优先选择的是日文或者韩文, 需要手动调整优先级后重启操作系统解决\nroot权限编辑 /etc/fonts/conf.avail/64-language-selector-prefer.conf 配置文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE fontconfig SYSTEM \u0026#34;fonts.dtd\u0026#34;\u0026gt; \u0026lt;fontconfig\u0026gt; \u0026lt;alias\u0026gt; \u0026lt;family\u0026gt;sans-serif\u0026lt;/family\u0026gt; \u0026lt;prefer\u0026gt; + \u0026lt;family\u0026gt;Noto Sans CJK SC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans CJK JP\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans CJK KR\u0026lt;/family\u0026gt; - \u0026lt;family\u0026gt;Noto Sans CJK SC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans CJK TC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans CJK HK\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Lohit Devanagari\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans Sinhala\u0026lt;/family\u0026gt; \u0026lt;/prefer\u0026gt; \u0026lt;/alias\u0026gt; \u0026lt;alias\u0026gt; \u0026lt;family\u0026gt;serif\u0026lt;/family\u0026gt; \u0026lt;prefer\u0026gt; + \u0026lt;family\u0026gt;Noto Serif CJK SC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Serif CJK JP\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Serif CJK KR\u0026lt;/family\u0026gt; - \u0026lt;family\u0026gt;Noto Serif CJK SC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Serif CJK TC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Lohit Devanagari\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Serif Sinhala\u0026lt;/family\u0026gt; \u0026lt;/prefer\u0026gt; \u0026lt;/alias\u0026gt; \u0026lt;alias\u0026gt; \u0026lt;family\u0026gt;monospace\u0026lt;/family\u0026gt; \u0026lt;prefer\u0026gt; + \u0026lt;family\u0026gt;Noto Sans Mono CJK SC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans Mono CJK JP\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans Mono CJK KR\u0026lt;/family\u0026gt; - \u0026lt;family\u0026gt;Noto Sans Mono CJK SC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans Mono CJK TC\u0026lt;/family\u0026gt; \u0026lt;family\u0026gt;Noto Sans Mono CJK HK\u0026lt;/family\u0026gt; \u0026lt;/prefer\u0026gt; \u0026lt;/alias\u0026gt; \u0026lt;/fontconfig\u0026gt; 换apt源 # https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\nsudo su - cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/apt/sources.list # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse deb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse # 预发布软件源，不建议启用 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse EOF apt update -y 使用X11替换Wayland # Wayland目前在我的AMD 5700U小机器上还是不太好用, 鼠标指针会有卡顿\n编辑 /etc/gdm3/custom.conf\n- #WaylandEnable=false + WaylandEnable=false 合上盖子不暂停系统(笔记本) # Make Ubuntu Not Go in Suspend When Laptop Lid is Closed\n如果是在笔记本上安装的, 那么这个设置可以防止合上盖子Lid后Suspend系统\n编辑/etc/systemd/logind.conf文件\n- #HandleLidSwitch=suspend + HandleLidSwitch=ignore - #HandleLidSwitchExternalPower=suspend + HandleLidSwitchExternalPower=ignore - #HandleLidSwitchDocked=ignore + HandleLidSwitchDocked=ignore 然后重启 systemd-logind 生效\nsystemctl restart systemd-logind 安装 oh my zsh # sudo apt install zsh curl -y sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; ohmyz.sh\n安装 golang # 下载并解压到 /usr/local/go目录下\nsudo apt install wget -y wget https://go.dev/dl/go1.21.1.linux-amd64.tar.gz sudo rm -rf /usr/local/go \u0026amp;\u0026amp; sudo tar -C /usr/local -xzf go1.21.1.linux-amd64.tar.gz 修改所有user和所有shell的PATH环境变量， 下面文件都添加一行 export PATH=$PATH:/usr/local/go/bin\n/etc/zsh/zshenv /etc/profile /etc/bash.bashrc 新开一个shell会话验证\n➜ ~ go version go version go1.21.1 linux/amd64 Download and install\n安装 hugo # sudo apt install build-essential -y CGO_ENABLED=1 go install -tags extended github.com/gohugoio/hugo@latest 设置当前用户zsh的PATH环境变量\nvim ~/.zshrc\n+ export PATH=${PATH}:`go env GOPATH`/bin 新开一个zsh shell会话验证\n➜ ~ hugo version hugo v0.118.2+extended linux/amd64 BuildDate=unknown 安装 openssh-server # sudo apt install openssh-server sudo vim /etc/ssh/sshd_config\n- #PermitRootLogin prohibit-password + PermitRootLogin yes sudo systemctl restart ssh 安装 gnome 插件 # 需要先安装有GNOME Shell extension\nsudo apt install gnome-shell-extension-manager 划词翻译插件(可选辞典web地址,我选择的bing) Screen work translate 在 top bar 上显示当前网络上下行速度和总流量 Net speed Simplified 允许连接锁着的远程桌面Allow Locked Remote Desktop 默认情况下不允许连接已经锁屏的终端tty0 askubuntu 所有网卡都禁用ipv6 # 修改/etc/sysctl.conf # + net.ipv6.conf.all.disable_ipv6 = 1 然后执行sysctl -p加载新的配置生效\n修改/etc/default/grub # - GRUB_CMDLINE_LINUX=\u0026#34;...\u0026#34; + GRUB_CMDLINE_LINUX=\u0026#34;... ipv6.disable=1\u0026#34; 然后执行update-grub更新GRUB, 这种指定传递给内核命令行的参数的方式需要重启系统之后才能生效\n安装VLC媒体播放器 # https://www.videolan.org/vlc/download-ubuntu.html\nsudo apt install vlc 快捷键 # 快捷键 功能 v 切换字幕 安装 docker # https://docs.docker.com/engine/install/ubuntu/ https://github.com/docker/docker-install\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh ./get-docker.sh 安装virt-manager # sudo apt install virt-manager -y 设置所有sudo组下的用户执行sudo命令不需要密码 # EDITOR=vim visudo\n# Allow members of group sudo to execute any command - %sudo ALL=(ALL:ALL) ALL + %sudo ALL=(ALL:ALL) NOPASSWD:ALL 删除用户密码 # 危险操作, 具有sudo权限的普通用户就更是危险操作了, 不过自己在家用时, 每次解锁或登录不用输入一长串密码真的很方便!!!, 这样就可以把Screen Blank的时间调短一点了😄\npasswd有一个-d参数\n# passwd -h | grep -e \u0026#34;-d\u0026#34; -d, --delete delete the password for the named account 删除某个账户的密码\nsudo passwd -d {username} "},{"id":50,"href":"/posts/solve-timezone-issue-in-go-application-in-container/","title":"Go应用在容器中的时区","section":"其他","content":" 容器中的时区问题 # 应用直接运行在服务器上需要设置服务器时区为东八区，现在很多应用都是部署在容器中了，同样也是要设置容器镜像的时区。\n许多容器镜像默认时区为 UTC (Coordinated Universal Time 协调世界时)，比东八区慢八个小时，当程序涉及数据库写入操作或者日志记录等功能时就会有时间差。\n常规解决方案一般两大类\nbuild docker镜像时就把镜像内的时区设置为 Asia/Shanghai 运行容器时把本地时区正常的主机的时区配置文件挂载到容器。 看一下 Go 是如何读取时区文件并设置 time.Time 的时区的 # Go 源码 src/time/zoneinfo_unix.go 中代码和注释都很清晰👍\npackage time import ( \u0026#34;syscall\u0026#34; ) // Many systems use /usr/share/zoneinfo, Solaris 2 has // /usr/share/lib/zoneinfo, IRIX 6 has /usr/lib/locale/TZ, // NixOS has /etc/zoneinfo. var platformZoneSources = []string{ \u0026#34;/usr/share/zoneinfo/\u0026#34;, \u0026#34;/usr/share/lib/zoneinfo/\u0026#34;, \u0026#34;/usr/lib/locale/TZ/\u0026#34;, \u0026#34;/etc/zoneinfo\u0026#34;, } func initLocal() { // consult $TZ to find the time zone to use. // no $TZ means use the system default /etc/localtime. // $TZ=\u0026#34;\u0026#34; means use UTC. // $TZ=\u0026#34;foo\u0026#34; or $TZ=\u0026#34;:foo\u0026#34; if foo is an absolute path, then the file pointed // by foo will be used to initialize timezone; otherwise, file // /usr/share/zoneinfo/foo will be used. tz, ok := syscall.Getenv(\u0026#34;TZ\u0026#34;) switch { case !ok: z, err := loadLocation(\u0026#34;localtime\u0026#34;, []string{\u0026#34;/etc\u0026#34;}) if err == nil { localLoc = *z localLoc.name = \u0026#34;Local\u0026#34; return } case tz != \u0026#34;\u0026#34;: if tz[0] == \u0026#39;:\u0026#39; { tz = tz[1:] } if tz != \u0026#34;\u0026#34; \u0026amp;\u0026amp; tz[0] == \u0026#39;/\u0026#39; { if z, err := loadLocation(tz, []string{\u0026#34;\u0026#34;}); err == nil { localLoc = *z if tz == \u0026#34;/etc/localtime\u0026#34; { localLoc.name = \u0026#34;Local\u0026#34; } else { localLoc.name = tz } return } } else if tz != \u0026#34;\u0026#34; \u0026amp;\u0026amp; tz != \u0026#34;UTC\u0026#34; { if z, err := loadLocation(tz, platformZoneSources); err == nil { localLoc = *z return } } } // Fall back to UTC. localLoc.name = \u0026#34;UTC\u0026#34; } 首先检查是否设置了 TZ 环境变量\n设置了 TZ TZ 为空 则时区还是 UTC TZ 第一个字符为 : 去掉 : TZ 不为空且第一个字符为 / 从 TZ 设置的路径中加载时区文件并设置时区 如果没加载到时区文件，那么最终还是 UTC 时区。 TZ 不为空且不是 UTC 从 platformZoneSources 中的几个路径下中加载 TZ 指定的时区文件并设置时区 如果没加载到时区文件，那么最终还是 UTC 时区。 没设置 TZ 加载 /etc/localtime 时区文件 如果没加载到时区文件，那么最终还是 UTC 时区。 综上，在 Dockerfile 中可以用下面两种方式之一正确设置时区\n设置 TZ 为 Asia/Shanghai 不设置 TZ，将 /usr/share/zoneinfo/Asia/Shanghai 拷贝或软链到 /etc/localtime 上面两种方式都需要有 /usr/share/zoneinfo/Asia/Shanghai 时区文件。\n"},{"id":51,"href":"/posts/sqlite3/","title":"SQLite3","section":"Database","content":" rollback日志模式下的五种锁状态介绍 # UNLOCKED 没锁状态 SHARED 获取SHARED锁才能执行读操作，一个数据库可同时存在多个SHARED锁 RESERVED 获取RESERVED锁才能在未来写数据库，一个数据库同一时间只能存在一个RESERVED锁 有RESERVED锁时说明还没开始写，所以有RESERVED锁时可以获取新的SHARED锁 PENDING 有PENDING锁意味着要开始写了，但是此时有其他连接拥有SHARED锁在读数据，此时写操作只能等待所有SHARED释放。 PENDING阻塞其他连接获取新的SHARED锁，当SHARED锁释放完时转为EXCLUSIVE锁开始写操作。 EXCLUSIVE 同一时间只能存在一个EXCLUSIVE锁，并且有EXCLUSIVE锁存在时不允许其他任何锁类型存在。 所以总结一下就是读读可并发，读写不可并发，写写不可并发。\n优化篇 # SQLITE_BUSY 问题 # 看到上面这么多锁不能共存的情况应该会想到，冲突会很频繁，如 EXCLUSIVE 锁存在时不允许其他连接获取任何锁，当其他进程需要读写操作时就会获取锁失败，立即报 SQLITE_BUSY 错误。\n设置 busy_timeout 就不会立即返回 SQLITE_BUSY，会定时retry失败的操作，如果在设置的 busy_timeout 时间内还没执行成功，依然会返回 SQLITE_BUSY。\n使用不同sqlite驱动，设置 busy_timeout 的方式不同\nmodernc.org/sqlite database.db?_pragma=busy_timeout%3d50000 github.com/mattn/go-sqlite3 database.db?_busy_timeout=50000 Shared cache mode 支持 table level locks，暂时还没研究。\n针对写操作慢的问题 # 解决方案：将多个写操作放入一个事务里执行。sqlite官方FAQ对其解释如下\n(19) INSERT is really slow - I can only do few dozen INSERTs per second Actually, SQLite will easily do 50,000 or more INSERT statements per second on an average desktop computer. But it will only do a few dozen transactions per second. Transaction speed is limited by the rotational speed of your disk drive. A transaction normally requires two complete rotations of the disk platter, which on a 7200RPM disk drive limits you to about 60 transactions per second. Transaction speed is limited by disk drive speed because (by default) SQLite actually waits until the data really is safely stored on the disk surface before the transaction is complete. That way, if you suddenly lose power or if your OS crashes, your data is still safe. For details, read about atomic commit in SQLite.. By default, each INSERT statement is its own transaction. But if you surround multiple INSERT statements with BEGIN\u0026hellip;COMMIT then all the inserts are grouped into a single transaction. The time needed to commit the transaction is amortized over all the enclosed insert statements and so the time per insert statement is greatly reduced. Another option is to run PRAGMA synchronous=OFF. This command will cause SQLite to not wait on data to reach the disk surface, which will make write operations appear to be much faster. But if you lose power in the middle of a transaction, your database file might go corrupt.\n测试环境\n# 表信息 sqlite\u0026gt; select count(*) from users; 1553471 # 日志模式 sqlite\u0026gt; PRAGMA journal_mode; delete 10次 insert 不在一个事务里\n$ go test -bench=\u0026#34;^Bench\u0026#34; -benchtime=5s . goos: linux goarch: amd64 pkg: gocn/sqlite-test cpu: Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz BenchmarkWrite-8 52 128920972 ns/op BenchmarkRead-8 141531 43400 ns/op BenchmarkWriteCGO-8 63 81576398 ns/op BenchmarkReadCGO-8 644850 8446 ns/op PASS ok gocn/sqlite-test 29.049s # 结果解释 # write 和 read 单次执行内容分别是十条 inster 和一条 select # BenchmarkWrite 是使用 modernc.org/sqlite 驱动的写操作 # BenchmarkWriteCGO 是使用 github.com/mattn/go-sqlite3 驱动的写操作 10次 insert 在一个事务里后\n$ go test -bench=\u0026#34;^Bench\u0026#34; -benchtime=5s . goos: linux goarch: amd64 pkg: gocn/sqlite-test cpu: Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz BenchmarkWrite-8 601 12099375 ns/op BenchmarkRead-8 142848 43089 ns/op BenchmarkWriteCGO-8 912 8050617 ns/op BenchmarkReadCGO-8 720722 8244 ns/op PASS ok gocn/sqlite-test 38.372s 可以看出来，写操作性能提升明显，写的单次操作(十次insert)时间直接下降了一个数量级，如果能将更多写操作放入一个事务里，性能提升也会越多，直至达到sqlite的写操作瓶颈(50,000 or more INSERT statements per second)。\n参考文档 # 官方文档-五种锁状态介绍 官方FAQ Understanding SQLITE_BUSY github.com/mattn/go-sqlite3 modernc.org/sqlite "},{"id":52,"href":"/posts/vim-tricks/","title":"Vim Tricks","section":"其他","content":" 批量替换 # 批量替换 v1.6.1 为 v2.7.0\n:%s/v1.6.1/v2.7.0/g 两行合为一行 # NORMAL 模式下按 shift + j 就会将光标下一行合并到当前行行尾\n"},{"id":53,"href":"/posts/go-app-reduce-size/","title":"减小go程序编译后的体积","section":"其他","content":" 编译经典程序 # 程序代码 # package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World.\u0026#34;) } 编译环境 # $ go version go version go1.16.7 linux/amd64 0. 直接编译 # $ go build -o helloword main.go $ ls -lh helloword -rwxrwxr-x 1 gobai gobai 1.9M Nov 23 09:34 helloword 1. 修改编译选项 # 除去编译时带的符号表和调试信息\n$ go build -ldflags=\u0026#34;-s -w\u0026#34; -o helloword main.go $ ls -lh helloword -rwxrwxr-x 1 gobai gobai 1.3M Nov 23 09:38 helloword 2. 使用 UPX # 对直接编译出的二进制使用 upx 进一步压缩\n$ go build -o helloword main.go $ upx -9 helloword Ultimate Packer for eXecutables Copyright (C) 1996 - 2020 UPX 3.96 Markus Oberhumer, Laszlo Molnar \u0026amp; John Reiser Jan 23rd 2020 File size Ratio Format Name -------------------- ------ ----------- ----------- 1937143 -\u0026gt; 1105452 57.07% linux/amd64 helloword Packed 1 file. $ ls -lh helloword -rwxrwxr-x 1 gobai gobai 1.1M Nov 23 09:40 helloword 1和2组合使用 # $ go build -ldflags=\u0026#34;-s -w\u0026#34; -o helloword main.go \u0026amp;\u0026amp; upx -9 helloword Ultimate Packer for eXecutables Copyright (C) 1996 - 2020 UPX 3.96 Markus Oberhumer, Laszlo Molnar \u0026amp; John Reiser Jan 23rd 2020 File size Ratio Format Name -------------------- ------ ----------- ----------- 1355776 -\u0026gt; 543392 40.08% linux/amd64 helloword Packed 1 file. $ ls -lh helloword -rwxrwxr-x 1 gobai gobai 531K Nov 23 09:42 helloword 可以看出，压缩效果显著！\n参考链接 # How to reduce compiled file size? "},{"id":54,"href":"/posts/systemd-journal/","title":"About Systemd","section":"其他","content":"记录一下查看和操作 systemd 日志的几个常用命令\n常用过滤日志日志的命令 # 根据时间约束过滤日志 # 获取 2023-01-15 00:00:00 之后的日志 # journalctl --since \u0026#39;2023-01-15 00:00:00\u0026#39; 获取 2023-01-15 00:00:00 之后, 2023-01-15 12:00:00 之前的日志 # journalctl --since \u0026#39;2023-01-15 00:00:00\u0026#39; --until \u0026#39;2023-01-15 12:00:00\u0026#39; 只查看一个服务(Unit)的日志 # journalctl -u nginx 自由组合约束条件 # journalctl -u nginx --since \u0026#39;2023-01-15 00:00:00\u0026#39; --until \u0026#39;2023-01-15 12:00:00\u0026#39; 查看日志占用磁盘量 # journalctl --disk-usage Output Archived and active journals take up 3.9G in the file system. 删除旧的日志 # 只保留最近 一个月 的日志 # journalctl --vacuum-time=1month 只保留最近 1G 的日志 # journalctl --vacuum-size=1G 列出所有systemd服务 # systemctl list-units --type=service --all "},{"id":55,"href":"/posts/git-tricks/","title":"Git Tricks","section":"其他","content":" 暂存当前代码改动 # 场景是在一个git branch写了不少代码以后发现写错分支了，总不能删了再重新写吧，可以使用 git stash 命令解决。\n# 将当前改动的代码暂存 git stash # git checkout 到你要工作的分支 git checkout your_workspace_branch # 把暂存的代码从堆栈弹出到当前分支 git stash pop "}]